{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [256, 256]  # 单层LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 1  # 二分类设置为1，多分类设置为类别的数目\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self.labelList = []\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        \n",
    "        if self.config.numClasses == 1:\n",
    "            labels = df[\"sentiment\"].tolist()\n",
    "        elif self.config.numClasses > 1:\n",
    "            labels = df[\"rate\"].tolist()\n",
    "            \n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        \"\"\"\n",
    "        将标签转换成索引表示\n",
    "        \"\"\"\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        return labelIds\n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        \"\"\"\n",
    "        将词转换成索引\n",
    "        \"\"\"\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "        \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        for review in x:\n",
    "            if len(review) >= self._sequenceLength:\n",
    "                reviews.append(review[:self._sequenceLength])\n",
    "            else:\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews, labels):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
    "        \n",
    "        # 将标签和句子数值化\n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000,)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTM(object):\n",
    "    \"\"\"\n",
    "    Bi-LSTM 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords, dtype=tf.float32,\n",
    "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2]\n",
    "                    self.embeddedWords = tf.concat(outputs, 2)\n",
    "        \n",
    "        # 去除最后时间步的输出作为全连接的输入\n",
    "        finalOutput = self.embeddedWords[:, -1, :]\n",
    "        \n",
    "        outputSize = config.model.hiddenSizes[-1] * 2  # 因为是双向LSTM，最终的输出值是fw和bw的拼接，因此要乘以2\n",
    "        output = tf.reshape(finalOutput, [-1, outputSize])  # reshape成全连接层的输入维度\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, config.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(output, outputW, outputB, name=\"logits\")\n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.float32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                                                    dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "                \n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义各类性能指标\n",
    "\"\"\"\n",
    "\n",
    "def mean(item: list) -> float:\n",
    "    \"\"\"\n",
    "    计算列表中元素的平均值\n",
    "    :param item: 列表对象\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    \"\"\"\n",
    "    计算二类和多类的准确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == true_y[i]:\n",
    "            corr += 1\n",
    "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "    return acc\n",
    "\n",
    "\n",
    "def binary_precision(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的精确率计算\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    pred_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == positive:\n",
    "            pred_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "    return prec\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    true_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if true_y[i] == positive:\n",
    "            true_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    rec = corr / true_corr if true_corr > 0 else 0\n",
    "    return rec\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    \"\"\"\n",
    "    二类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param beta: beta值\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    try:\n",
    "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    except:\n",
    "        f_b = 0\n",
    "    return f_b\n",
    "\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的精确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    \"\"\"\n",
    "    多类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :param beta: beta值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到二分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到多分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param labels:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = multi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to E:\\githubProject\\textClassifier\\Bi-LSTM\\summarys\n",
      "\n",
      "start training model\n",
      "train: step: 1, loss: 0.6968840956687927, acc: 0.5, recall: 0.6721311475409836, precision: 0.4823529411764706, f_beta: 0.5616438356164384\n",
      "train: step: 2, loss: 0.7439129948616028, acc: 0.46875, recall: 0.5230769230769231, precision: 0.4788732394366197, f_beta: 0.5\n",
      "train: step: 3, loss: 0.7105320692062378, acc: 0.5625, recall: 1.0, precision: 0.5625, f_beta: 0.72\n",
      "train: step: 4, loss: 0.7646469473838806, acc: 0.421875, recall: 1.0, precision: 0.421875, f_beta: 0.5934065934065934\n",
      "train: step: 5, loss: 0.6895784735679626, acc: 0.5234375, recall: 0.8260869565217391, precision: 0.5377358490566038, f_beta: 0.6514285714285714\n",
      "train: step: 6, loss: 0.7067426443099976, acc: 0.53125, recall: 0.6351351351351351, precision: 0.5875, f_beta: 0.6103896103896105\n",
      "train: step: 7, loss: 0.6901035308837891, acc: 0.53125, recall: 0.6417910447761194, precision: 0.5443037974683544, f_beta: 0.5890410958904109\n",
      "train: step: 8, loss: 0.7051342725753784, acc: 0.46875, recall: 0.6164383561643836, precision: 0.5294117647058824, f_beta: 0.5696202531645569\n",
      "train: step: 9, loss: 0.6971909999847412, acc: 0.5390625, recall: 0.8857142857142857, precision: 0.5486725663716814, f_beta: 0.6775956284153005\n",
      "train: step: 10, loss: 0.7081223726272583, acc: 0.515625, recall: 0.9850746268656716, precision: 0.5196850393700787, f_beta: 0.6804123711340206\n",
      "train: step: 11, loss: 0.7135739326477051, acc: 0.46875, recall: 1.0, precision: 0.4645669291338583, f_beta: 0.6344086021505376\n",
      "train: step: 12, loss: 0.7006754875183105, acc: 0.4609375, recall: 0.9354838709677419, precision: 0.4715447154471545, f_beta: 0.6270270270270271\n",
      "train: step: 13, loss: 0.696506679058075, acc: 0.5546875, recall: 0.9545454545454546, precision: 0.5384615384615384, f_beta: 0.6885245901639344\n",
      "train: step: 14, loss: 0.7166390419006348, acc: 0.3984375, recall: 0.7037037037037037, precision: 0.3838383838383838, f_beta: 0.4967320261437908\n",
      "train: step: 15, loss: 0.6956084966659546, acc: 0.5234375, recall: 0.7272727272727273, precision: 0.5274725274725275, f_beta: 0.6114649681528662\n",
      "train: step: 16, loss: 0.7010701894760132, acc: 0.5390625, recall: 0.6, precision: 0.5070422535211268, f_beta: 0.549618320610687\n",
      "train: step: 17, loss: 0.6867347955703735, acc: 0.5234375, recall: 0.6612903225806451, precision: 0.5061728395061729, f_beta: 0.5734265734265734\n",
      "train: step: 18, loss: 0.7173205614089966, acc: 0.4765625, recall: 0.5344827586206896, precision: 0.43661971830985913, f_beta: 0.48062015503875966\n",
      "train: step: 19, loss: 0.715083122253418, acc: 0.5, recall: 0.5294117647058824, precision: 0.5294117647058824, f_beta: 0.5294117647058824\n",
      "train: step: 20, loss: 0.6820254921913147, acc: 0.515625, recall: 0.5666666666666667, precision: 0.4857142857142857, f_beta: 0.523076923076923\n",
      "train: step: 21, loss: 0.6910282373428345, acc: 0.5234375, recall: 0.639344262295082, precision: 0.5, f_beta: 0.5611510791366907\n",
      "train: step: 22, loss: 0.689601480960846, acc: 0.5234375, recall: 0.6724137931034483, precision: 0.48148148148148145, f_beta: 0.5611510791366906\n",
      "train: step: 23, loss: 0.6948961019515991, acc: 0.5, recall: 0.65, precision: 0.47560975609756095, f_beta: 0.5492957746478874\n",
      "train: step: 24, loss: 0.6991568207740784, acc: 0.453125, recall: 0.4576271186440678, precision: 0.4153846153846154, f_beta: 0.435483870967742\n",
      "train: step: 25, loss: 0.7074265480041504, acc: 0.46875, recall: 0.4166666666666667, precision: 0.43103448275862066, f_beta: 0.423728813559322\n",
      "train: step: 26, loss: 0.7100591659545898, acc: 0.4375, recall: 0.15384615384615385, precision: 0.37037037037037035, f_beta: 0.2173913043478261\n",
      "train: step: 27, loss: 0.6831986904144287, acc: 0.578125, recall: 0.2777777777777778, precision: 0.5, f_beta: 0.35714285714285715\n",
      "train: step: 28, loss: 0.6896428465843201, acc: 0.5390625, recall: 0.1206896551724138, precision: 0.4666666666666667, f_beta: 0.19178082191780824\n",
      "train: step: 29, loss: 0.6973902583122253, acc: 0.5234375, recall: 0.0, precision: 0.0, f_beta: 0\n",
      "train: step: 30, loss: 0.7158687114715576, acc: 0.4765625, recall: 0.0, precision: 0.0, f_beta: 0\n",
      "train: step: 31, loss: 0.6952537894248962, acc: 0.46875, recall: 0.028985507246376812, precision: 0.6666666666666666, f_beta: 0.05555555555555555\n",
      "train: step: 32, loss: 0.7024844884872437, acc: 0.53125, recall: 0.15151515151515152, precision: 0.7142857142857143, f_beta: 0.25\n",
      "train: step: 33, loss: 0.7022609710693359, acc: 0.5, recall: 0.2878787878787879, precision: 0.5277777777777778, f_beta: 0.37254901960784315\n",
      "train: step: 34, loss: 0.6881099939346313, acc: 0.53125, recall: 0.42857142857142855, precision: 0.5294117647058824, f_beta: 0.4736842105263158\n",
      "train: step: 35, loss: 0.7142977714538574, acc: 0.484375, recall: 0.5294117647058824, precision: 0.391304347826087, f_beta: 0.45\n",
      "train: step: 36, loss: 0.7109683156013489, acc: 0.46875, recall: 0.45, precision: 0.43548387096774194, f_beta: 0.4426229508196722\n",
      "train: step: 37, loss: 0.7071656584739685, acc: 0.5078125, recall: 0.45901639344262296, precision: 0.4827586206896552, f_beta: 0.47058823529411764\n",
      "train: step: 38, loss: 0.7031910419464111, acc: 0.4765625, recall: 0.3508771929824561, precision: 0.4, f_beta: 0.37383177570093457\n",
      "train: step: 39, loss: 0.7101223468780518, acc: 0.421875, recall: 0.3, precision: 0.45652173913043476, f_beta: 0.3620689655172414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 40, loss: 0.6962273120880127, acc: 0.53125, recall: 0.23333333333333334, precision: 0.5, f_beta: 0.3181818181818182\n",
      "train: step: 41, loss: 0.694873034954071, acc: 0.5, recall: 0.13846153846153847, precision: 0.5294117647058824, f_beta: 0.21951219512195122\n",
      "train: step: 42, loss: 0.7044191360473633, acc: 0.453125, recall: 0.11267605633802817, precision: 0.5333333333333333, f_beta: 0.18604651162790697\n",
      "train: step: 43, loss: 0.6937211751937866, acc: 0.515625, recall: 0.140625, precision: 0.5625, f_beta: 0.225\n",
      "train: step: 44, loss: 0.6913652420043945, acc: 0.5, recall: 0.2033898305084746, precision: 0.41379310344827586, f_beta: 0.2727272727272727\n",
      "train: step: 45, loss: 0.7005025148391724, acc: 0.4296875, recall: 0.3548387096774194, precision: 0.4, f_beta: 0.37606837606837606\n",
      "train: step: 46, loss: 0.6927592158317566, acc: 0.484375, recall: 0.4583333333333333, precision: 0.55, f_beta: 0.5\n",
      "train: step: 47, loss: 0.68257737159729, acc: 0.640625, recall: 0.704225352112676, precision: 0.6666666666666666, f_beta: 0.6849315068493151\n",
      "train: step: 48, loss: 0.6987336874008179, acc: 0.46875, recall: 0.696969696969697, precision: 0.48936170212765956, f_beta: 0.575\n",
      "train: step: 49, loss: 0.714200496673584, acc: 0.3125, recall: 0.5490196078431373, precision: 0.3010752688172043, f_beta: 0.38888888888888895\n",
      "train: step: 50, loss: 0.6898911595344543, acc: 0.5, recall: 0.8088235294117647, precision: 0.5188679245283019, f_beta: 0.632183908045977\n",
      "train: step: 51, loss: 0.6945134401321411, acc: 0.5390625, recall: 0.8382352941176471, precision: 0.5428571428571428, f_beta: 0.6589595375722543\n",
      "train: step: 52, loss: 0.6767337918281555, acc: 0.546875, recall: 0.8787878787878788, precision: 0.5370370370370371, f_beta: 0.6666666666666667\n",
      "train: step: 53, loss: 0.7008237838745117, acc: 0.5546875, recall: 0.8676470588235294, precision: 0.5514018691588785, f_beta: 0.6742857142857143\n",
      "train: step: 54, loss: 0.6934418678283691, acc: 0.546875, recall: 0.8656716417910447, precision: 0.5420560747663551, f_beta: 0.6666666666666666\n",
      "train: step: 55, loss: 0.6819517016410828, acc: 0.5078125, recall: 0.8909090909090909, precision: 0.46226415094339623, f_beta: 0.6086956521739131\n",
      "train: step: 56, loss: 0.6859868168830872, acc: 0.5546875, recall: 0.8181818181818182, precision: 0.5454545454545454, f_beta: 0.6545454545454545\n",
      "train: step: 57, loss: 0.7061684131622314, acc: 0.5, recall: 0.803030303030303, precision: 0.5096153846153846, f_beta: 0.6235294117647059\n",
      "train: step: 58, loss: 0.702600359916687, acc: 0.4921875, recall: 0.7666666666666667, precision: 0.4742268041237113, f_beta: 0.5859872611464968\n",
      "train: step: 59, loss: 0.6862382292747498, acc: 0.5390625, recall: 0.6666666666666666, precision: 0.5063291139240507, f_beta: 0.5755395683453238\n",
      "train: step: 60, loss: 0.6907082796096802, acc: 0.5078125, recall: 0.5915492957746479, precision: 0.5526315789473685, f_beta: 0.5714285714285715\n",
      "train: step: 61, loss: 0.6989797949790955, acc: 0.4765625, recall: 0.6065573770491803, precision: 0.4625, f_beta: 0.5248226950354611\n",
      "train: step: 62, loss: 0.7086999416351318, acc: 0.4765625, recall: 0.44776119402985076, precision: 0.5, f_beta: 0.4724409448818898\n",
      "train: step: 63, loss: 0.6865725517272949, acc: 0.4921875, recall: 0.5, precision: 0.46153846153846156, f_beta: 0.48000000000000004\n",
      "train: step: 64, loss: 0.6918766498565674, acc: 0.5234375, recall: 0.5147058823529411, precision: 0.5555555555555556, f_beta: 0.5343511450381679\n",
      "train: step: 65, loss: 0.7060684561729431, acc: 0.453125, recall: 0.4520547945205479, precision: 0.5238095238095238, f_beta: 0.4852941176470588\n",
      "train: step: 66, loss: 0.7003531455993652, acc: 0.46875, recall: 0.4375, precision: 0.4666666666666667, f_beta: 0.45161290322580644\n",
      "train: step: 67, loss: 0.6996053457260132, acc: 0.4453125, recall: 0.36666666666666664, precision: 0.4, f_beta: 0.3826086956521739\n",
      "train: step: 68, loss: 0.697128415107727, acc: 0.4765625, recall: 0.543859649122807, precision: 0.4305555555555556, f_beta: 0.4806201550387597\n",
      "train: step: 69, loss: 0.6879249215126038, acc: 0.515625, recall: 0.55, precision: 0.4852941176470588, f_beta: 0.5156249999999999\n",
      "train: step: 70, loss: 0.7055760622024536, acc: 0.515625, recall: 0.4528301886792453, precision: 0.42105263157894735, f_beta: 0.43636363636363634\n",
      "train: step: 71, loss: 0.7029526233673096, acc: 0.453125, recall: 0.4057971014492754, precision: 0.49122807017543857, f_beta: 0.4444444444444445\n",
      "train: step: 72, loss: 0.6982919573783875, acc: 0.515625, recall: 0.3620689655172414, precision: 0.45652173913043476, f_beta: 0.4038461538461538\n",
      "train: step: 73, loss: 0.6864849925041199, acc: 0.5078125, recall: 0.4, precision: 0.52, f_beta: 0.45217391304347826\n",
      "train: step: 74, loss: 0.7026714086532593, acc: 0.4765625, recall: 0.3235294117647059, precision: 0.5116279069767442, f_beta: 0.39639639639639646\n",
      "train: step: 75, loss: 0.698936402797699, acc: 0.46875, recall: 0.3157894736842105, precision: 0.6, f_beta: 0.41379310344827586\n",
      "train: step: 76, loss: 0.6975381374359131, acc: 0.4609375, recall: 0.30303030303030304, precision: 0.46511627906976744, f_beta: 0.36697247706422015\n",
      "train: step: 77, loss: 0.6936280727386475, acc: 0.5, recall: 0.3389830508474576, precision: 0.4444444444444444, f_beta: 0.38461538461538464\n",
      "train: step: 78, loss: 0.699799656867981, acc: 0.40625, recall: 0.2638888888888889, precision: 0.4523809523809524, f_beta: 0.3333333333333333\n",
      "train: step: 79, loss: 0.6900330781936646, acc: 0.5703125, recall: 0.3939393939393939, precision: 0.6341463414634146, f_beta: 0.48598130841121495\n",
      "train: step: 80, loss: 0.6916123628616333, acc: 0.4921875, recall: 0.42857142857142855, precision: 0.48214285714285715, f_beta: 0.45378151260504196\n",
      "train: step: 81, loss: 0.6878699064254761, acc: 0.5546875, recall: 0.6557377049180327, precision: 0.5263157894736842, f_beta: 0.583941605839416\n",
      "train: step: 82, loss: 0.7009695768356323, acc: 0.4140625, recall: 0.5272727272727272, precision: 0.3717948717948718, f_beta: 0.43609022556390975\n",
      "train: step: 83, loss: 0.7004551887512207, acc: 0.5078125, recall: 0.6307692307692307, precision: 0.5125, f_beta: 0.5655172413793104\n",
      "train: step: 84, loss: 0.6958202719688416, acc: 0.5, recall: 0.6721311475409836, precision: 0.4823529411764706, f_beta: 0.5616438356164384\n",
      "train: step: 85, loss: 0.6982109546661377, acc: 0.515625, recall: 0.6666666666666666, precision: 0.5747126436781609, f_beta: 0.617283950617284\n",
      "train: step: 86, loss: 0.6855331659317017, acc: 0.5703125, recall: 0.7301587301587301, precision: 0.5476190476190477, f_beta: 0.6258503401360545\n",
      "train: step: 87, loss: 0.6953637599945068, acc: 0.5859375, recall: 0.8378378378378378, precision: 0.6019417475728155, f_beta: 0.7005649717514123\n",
      "train: step: 88, loss: 0.691785454750061, acc: 0.484375, recall: 0.8, precision: 0.49523809523809526, f_beta: 0.611764705882353\n",
      "train: step: 89, loss: 0.6783479452133179, acc: 0.5390625, recall: 0.9393939393939394, precision: 0.5299145299145299, f_beta: 0.6775956284153006\n",
      "train: step: 90, loss: 0.6942356824874878, acc: 0.5390625, recall: 0.9076923076923077, precision: 0.5267857142857143, f_beta: 0.6666666666666667\n",
      "train: step: 91, loss: 0.6944553852081299, acc: 0.4921875, recall: 0.9180327868852459, precision: 0.4827586206896552, f_beta: 0.632768361581921\n",
      "train: step: 92, loss: 0.6888355016708374, acc: 0.4921875, recall: 0.8571428571428571, precision: 0.4909090909090909, f_beta: 0.6242774566473989\n",
      "train: step: 93, loss: 0.6899491548538208, acc: 0.5234375, recall: 0.835820895522388, precision: 0.5283018867924528, f_beta: 0.6473988439306357\n",
      "train: step: 94, loss: 0.6883218288421631, acc: 0.5078125, recall: 0.8615384615384616, precision: 0.509090909090909, f_beta: 0.64\n",
      "train: step: 95, loss: 0.6948666572570801, acc: 0.5625, recall: 0.8032786885245902, precision: 0.5268817204301075, f_beta: 0.6363636363636364\n",
      "train: step: 96, loss: 0.6850193738937378, acc: 0.5390625, recall: 0.7, precision: 0.5632183908045977, f_beta: 0.6242038216560509\n",
      "train: step: 97, loss: 0.6940439343452454, acc: 0.5, recall: 0.6538461538461539, precision: 0.425, f_beta: 0.5151515151515152\n",
      "train: step: 98, loss: 0.6893036365509033, acc: 0.53125, recall: 0.47619047619047616, precision: 0.5263157894736842, f_beta: 0.5\n",
      "train: step: 99, loss: 0.6951538324356079, acc: 0.578125, recall: 0.32857142857142857, precision: 0.7666666666666667, f_beta: 0.45999999999999996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 100, loss: 0.6996699571609497, acc: 0.59375, recall: 0.41379310344827586, precision: 0.5714285714285714, f_beta: 0.48000000000000004\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:39:23.359381, step: 100, loss: 0.7028129452314132, acc: 0.4937900641025641,precision: 0.4650783476798735, recall: 0.5000177474620617, f_beta: 0.4799631977868215\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-100\n",
      "\n",
      "train: step: 101, loss: 0.6832329034805298, acc: 0.5625, recall: 0.4745762711864407, precision: 0.5283018867924528, f_beta: 0.5\n",
      "train: step: 102, loss: 0.7301159501075745, acc: 0.4609375, recall: 0.34285714285714286, precision: 0.5106382978723404, f_beta: 0.41025641025641024\n",
      "train: step: 103, loss: 0.6940841674804688, acc: 0.5234375, recall: 0.5294117647058824, precision: 0.5538461538461539, f_beta: 0.5413533834586466\n",
      "train: step: 104, loss: 0.6757316589355469, acc: 0.609375, recall: 0.6551724137931034, precision: 0.5588235294117647, f_beta: 0.6031746031746031\n",
      "train: step: 105, loss: 0.68822181224823, acc: 0.5234375, recall: 0.7213114754098361, precision: 0.5, f_beta: 0.5906040268456376\n",
      "train: step: 106, loss: 0.6948356628417969, acc: 0.4921875, recall: 0.6438356164383562, precision: 0.5465116279069767, f_beta: 0.5911949685534591\n",
      "train: step: 107, loss: 0.6834301948547363, acc: 0.4921875, recall: 0.8153846153846154, precision: 0.5, f_beta: 0.6198830409356725\n",
      "train: step: 108, loss: 0.6866978406906128, acc: 0.53125, recall: 0.9701492537313433, precision: 0.5284552845528455, f_beta: 0.6842105263157895\n",
      "train: step: 109, loss: 0.722561240196228, acc: 0.40625, recall: 0.8596491228070176, precision: 0.4188034188034188, f_beta: 0.5632183908045977\n",
      "train: step: 110, loss: 0.7036141753196716, acc: 0.4375, recall: 0.8888888888888888, precision: 0.42105263157894735, f_beta: 0.5714285714285714\n",
      "train: step: 111, loss: 0.6951353549957275, acc: 0.4921875, recall: 0.7543859649122807, precision: 0.4574468085106383, f_beta: 0.5695364238410595\n",
      "train: step: 112, loss: 0.692459225654602, acc: 0.484375, recall: 0.44776119402985076, precision: 0.5084745762711864, f_beta: 0.4761904761904762\n",
      "train: step: 113, loss: 0.6878633499145508, acc: 0.5546875, recall: 0.3333333333333333, precision: 0.5, f_beta: 0.4\n",
      "train: step: 114, loss: 0.6939986348152161, acc: 0.53125, recall: 0.18181818181818182, precision: 0.6666666666666666, f_beta: 0.28571428571428575\n",
      "train: step: 115, loss: 0.6949521899223328, acc: 0.5390625, recall: 0.1, precision: 0.5454545454545454, f_beta: 0.16901408450704225\n",
      "train: step: 116, loss: 0.6842411160469055, acc: 0.546875, recall: 0.05172413793103448, precision: 0.5, f_beta: 0.09375\n",
      "train: step: 117, loss: 0.7142263650894165, acc: 0.5234375, recall: 0.078125, precision: 0.7142857142857143, f_beta: 0.14084507042253522\n",
      "train: step: 118, loss: 0.6915956735610962, acc: 0.546875, recall: 0.09523809523809523, precision: 0.8571428571428571, f_beta: 0.17142857142857143\n",
      "train: step: 119, loss: 0.6739429235458374, acc: 0.59375, recall: 0.09090909090909091, precision: 0.7142857142857143, f_beta: 0.16129032258064518\n",
      "train: step: 120, loss: 0.7052358984947205, acc: 0.4921875, recall: 0.08823529411764706, precision: 0.6666666666666666, f_beta: 0.15584415584415584\n",
      "train: step: 121, loss: 0.6856430172920227, acc: 0.5234375, recall: 0.06451612903225806, precision: 0.5714285714285714, f_beta: 0.11594202898550726\n",
      "train: step: 122, loss: 0.6831884384155273, acc: 0.5625, recall: 0.19047619047619047, precision: 0.7058823529411765, f_beta: 0.3\n",
      "train: step: 123, loss: 0.6855560541152954, acc: 0.5390625, recall: 0.22058823529411764, precision: 0.7142857142857143, f_beta: 0.33707865168539325\n",
      "train: step: 124, loss: 0.684294581413269, acc: 0.53125, recall: 0.25757575757575757, precision: 0.6071428571428571, f_beta: 0.3617021276595745\n",
      "train: step: 125, loss: 0.6813943386077881, acc: 0.5859375, recall: 0.4246575342465753, precision: 0.7380952380952381, f_beta: 0.5391304347826087\n",
      "train: step: 126, loss: 0.6873714923858643, acc: 0.5390625, recall: 0.43333333333333335, precision: 0.5098039215686274, f_beta: 0.46846846846846846\n",
      "train: step: 127, loss: 0.6717000007629395, acc: 0.59375, recall: 0.6290322580645161, precision: 0.5735294117647058, f_beta: 0.5999999999999999\n",
      "train: step: 128, loss: 0.6677618026733398, acc: 0.5390625, recall: 0.7413793103448276, precision: 0.4942528735632184, f_beta: 0.5931034482758621\n",
      "train: step: 129, loss: 0.6839529275894165, acc: 0.59375, recall: 0.6944444444444444, precision: 0.625, f_beta: 0.6578947368421053\n",
      "train: step: 130, loss: 0.6778231263160706, acc: 0.515625, recall: 0.6851851851851852, precision: 0.45121951219512196, f_beta: 0.5441176470588235\n",
      "train: step: 131, loss: 0.667149007320404, acc: 0.5703125, recall: 0.6140350877192983, precision: 0.5147058823529411, f_beta: 0.5599999999999999\n",
      "train: step: 132, loss: 0.6603647470474243, acc: 0.625, recall: 0.45161290322580644, precision: 0.6666666666666666, f_beta: 0.5384615384615384\n",
      "train: step: 133, loss: 0.6336665749549866, acc: 0.6171875, recall: 0.2413793103448276, precision: 0.7368421052631579, f_beta: 0.3636363636363636\n",
      "train: step: 134, loss: 0.6812061071395874, acc: 0.5234375, recall: 0.16901408450704225, precision: 0.8571428571428571, f_beta: 0.2823529411764706\n",
      "train: step: 135, loss: 0.6908078193664551, acc: 0.515625, recall: 0.16666666666666666, precision: 0.8571428571428571, f_beta: 0.27906976744186046\n",
      "train: step: 136, loss: 0.5589151382446289, acc: 0.7578125, recall: 0.8181818181818182, precision: 0.7875, f_beta: 0.8025477707006369\n",
      "train: step: 137, loss: 0.8206809759140015, acc: 0.625, recall: 1.0, precision: 0.5752212389380531, f_beta: 0.7303370786516854\n",
      "train: step: 138, loss: 0.6582754254341125, acc: 0.6796875, recall: 0.8095238095238095, precision: 0.6375, f_beta: 0.7132867132867132\n",
      "train: step: 139, loss: 0.590976893901825, acc: 0.7421875, recall: 0.5409836065573771, precision: 0.868421052631579, f_beta: 0.6666666666666666\n",
      "train: step: 140, loss: 0.563206672668457, acc: 0.75, recall: 0.603448275862069, precision: 0.7954545454545454, f_beta: 0.6862745098039215\n",
      "train: step: 141, loss: 0.5386536121368408, acc: 0.8359375, recall: 0.7580645161290323, precision: 0.8867924528301887, f_beta: 0.8173913043478261\n",
      "train: step: 142, loss: 0.5530375838279724, acc: 0.796875, recall: 0.8840579710144928, precision: 0.7721518987341772, f_beta: 0.8243243243243245\n",
      "train: step: 143, loss: 0.5839393138885498, acc: 0.6796875, recall: 0.9354838709677419, precision: 0.6105263157894737, f_beta: 0.7388535031847133\n",
      "train: step: 144, loss: 0.5729377269744873, acc: 0.7265625, recall: 1.0, precision: 0.6759259259259259, f_beta: 0.8066298342541436\n",
      "train: step: 145, loss: 0.580785870552063, acc: 0.7265625, recall: 0.9285714285714286, precision: 0.6842105263157895, f_beta: 0.7878787878787878\n",
      "train: step: 146, loss: 0.5317946672439575, acc: 0.7265625, recall: 0.8412698412698413, precision: 0.6794871794871795, f_beta: 0.7517730496453902\n",
      "train: step: 147, loss: 0.5994830131530762, acc: 0.734375, recall: 0.7878787878787878, precision: 0.7222222222222222, f_beta: 0.7536231884057971\n",
      "train: step: 148, loss: 0.5768041610717773, acc: 0.75, recall: 0.8636363636363636, precision: 0.7125, f_beta: 0.7808219178082192\n",
      "train: step: 149, loss: 0.5843629837036133, acc: 0.7109375, recall: 0.9032258064516129, precision: 0.6436781609195402, f_beta: 0.7516778523489932\n",
      "train: step: 150, loss: 0.5539172887802124, acc: 0.765625, recall: 0.8714285714285714, precision: 0.7439024390243902, f_beta: 0.8026315789473685\n",
      "train: step: 151, loss: 0.5573244094848633, acc: 0.734375, recall: 0.8064516129032258, precision: 0.6944444444444444, f_beta: 0.746268656716418\n",
      "train: step: 152, loss: 0.563086748123169, acc: 0.734375, recall: 0.7746478873239436, precision: 0.7534246575342466, f_beta: 0.7638888888888888\n",
      "train: step: 153, loss: 0.5437982082366943, acc: 0.734375, recall: 0.7083333333333334, precision: 0.796875, f_beta: 0.7499999999999999\n",
      "train: step: 154, loss: 0.5757172107696533, acc: 0.71875, recall: 0.9558823529411765, precision: 0.6632653061224489, f_beta: 0.783132530120482\n",
      "train: step: 155, loss: 0.7731747031211853, acc: 0.5703125, recall: 0.9838709677419355, precision: 0.5304347826086957, f_beta: 0.6892655367231639\n",
      "train: step: 156, loss: 0.7572803497314453, acc: 0.578125, recall: 1.0, precision: 0.5221238938053098, f_beta: 0.686046511627907\n",
      "start training model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 157, loss: 0.7127944827079773, acc: 0.546875, recall: 1.0, precision: 0.5042735042735043, f_beta: 0.6704545454545454\n",
      "train: step: 158, loss: 0.6591971516609192, acc: 0.59375, recall: 0.9692307692307692, precision: 0.5575221238938053, f_beta: 0.7078651685393259\n",
      "train: step: 159, loss: 0.6405877470970154, acc: 0.6171875, recall: 0.7258064516129032, precision: 0.5844155844155844, f_beta: 0.6474820143884892\n",
      "train: step: 160, loss: 0.701443612575531, acc: 0.4921875, recall: 0.36666666666666664, precision: 0.4489795918367347, f_beta: 0.4036697247706422\n",
      "train: step: 161, loss: 0.6836446523666382, acc: 0.515625, recall: 0.28125, precision: 0.5294117647058824, f_beta: 0.3673469387755102\n",
      "train: step: 162, loss: 0.6668533682823181, acc: 0.546875, recall: 0.23214285714285715, precision: 0.4642857142857143, f_beta: 0.30952380952380953\n",
      "train: step: 163, loss: 0.6845399737358093, acc: 0.46875, recall: 0.208955223880597, precision: 0.4827586206896552, f_beta: 0.29166666666666663\n",
      "train: step: 164, loss: 0.6545705795288086, acc: 0.5625, recall: 0.3387096774193548, precision: 0.5833333333333334, f_beta: 0.4285714285714285\n",
      "train: step: 165, loss: 0.6559743881225586, acc: 0.5546875, recall: 0.3870967741935484, precision: 0.5581395348837209, f_beta: 0.45714285714285713\n",
      "train: step: 166, loss: 0.6623002290725708, acc: 0.53125, recall: 0.6896551724137931, precision: 0.4878048780487805, f_beta: 0.5714285714285714\n",
      "train: step: 167, loss: 0.594728410243988, acc: 0.6328125, recall: 0.7424242424242424, precision: 0.620253164556962, f_beta: 0.6758620689655171\n",
      "train: step: 168, loss: 0.6790868043899536, acc: 0.609375, recall: 0.8571428571428571, precision: 0.5684210526315789, f_beta: 0.6835443037974683\n",
      "train: step: 169, loss: 0.6645252704620361, acc: 0.5859375, recall: 0.7571428571428571, precision: 0.5955056179775281, f_beta: 0.6666666666666667\n",
      "train: step: 170, loss: 0.6224682331085205, acc: 0.6328125, recall: 0.8243243243243243, precision: 0.6421052631578947, f_beta: 0.7218934911242604\n",
      "train: step: 171, loss: 0.6143831610679626, acc: 0.6015625, recall: 0.9516129032258065, precision: 0.5514018691588785, f_beta: 0.6982248520710059\n",
      "train: step: 172, loss: 0.6497647762298584, acc: 0.5859375, recall: 0.8888888888888888, precision: 0.5490196078431373, f_beta: 0.6787878787878788\n",
      "train: step: 173, loss: 0.7342308759689331, acc: 0.59375, recall: 0.859375, precision: 0.5612244897959183, f_beta: 0.6790123456790124\n",
      "train: step: 174, loss: 0.6352298259735107, acc: 0.6015625, recall: 0.9322033898305084, precision: 0.5392156862745098, f_beta: 0.6832298136645962\n",
      "train: step: 175, loss: 0.6673973798751831, acc: 0.6640625, recall: 0.9090909090909091, precision: 0.660377358490566, f_beta: 0.7650273224043715\n",
      "train: step: 176, loss: 0.6737499237060547, acc: 0.5625, recall: 0.8771929824561403, precision: 0.5050505050505051, f_beta: 0.641025641025641\n",
      "train: step: 177, loss: 0.6425428986549377, acc: 0.65625, recall: 0.9193548387096774, precision: 0.59375, f_beta: 0.7215189873417721\n",
      "train: step: 178, loss: 0.6124459505081177, acc: 0.609375, recall: 0.78125, precision: 0.5813953488372093, f_beta: 0.6666666666666666\n",
      "train: step: 179, loss: 0.641124963760376, acc: 0.546875, recall: 0.7678571428571429, precision: 0.48863636363636365, f_beta: 0.5972222222222222\n",
      "train: step: 180, loss: 0.6947990655899048, acc: 0.546875, recall: 0.6461538461538462, precision: 0.5454545454545454, f_beta: 0.5915492957746479\n",
      "train: step: 181, loss: 0.625023365020752, acc: 0.5625, recall: 0.5535714285714286, precision: 0.5, f_beta: 0.5254237288135594\n",
      "train: step: 182, loss: 0.6386303305625916, acc: 0.6484375, recall: 0.5882352941176471, precision: 0.7017543859649122, f_beta: 0.6399999999999999\n",
      "train: step: 183, loss: 0.666566789150238, acc: 0.5546875, recall: 0.5333333333333333, precision: 0.5245901639344263, f_beta: 0.5289256198347108\n",
      "train: step: 184, loss: 0.6712784767150879, acc: 0.5703125, recall: 0.6379310344827587, precision: 0.5211267605633803, f_beta: 0.5736434108527132\n",
      "train: step: 185, loss: 0.6535211801528931, acc: 0.5703125, recall: 0.5625, precision: 0.5714285714285714, f_beta: 0.5669291338582677\n",
      "train: step: 186, loss: 0.6570370197296143, acc: 0.59375, recall: 0.6111111111111112, precision: 0.6470588235294118, f_beta: 0.6285714285714287\n",
      "train: step: 187, loss: 0.6714613437652588, acc: 0.5234375, recall: 0.6515151515151515, precision: 0.5308641975308642, f_beta: 0.5850340136054422\n",
      "train: step: 188, loss: 0.6575368642807007, acc: 0.5546875, recall: 0.819672131147541, precision: 0.5208333333333334, f_beta: 0.6369426751592356\n",
      "train: step: 189, loss: 0.6652353405952454, acc: 0.609375, recall: 0.8524590163934426, precision: 0.5591397849462365, f_beta: 0.6753246753246752\n",
      "train: step: 190, loss: 0.620137095451355, acc: 0.6484375, recall: 0.8378378378378378, precision: 0.6526315789473685, f_beta: 0.7337278106508875\n",
      "train: step: 191, loss: 0.6284117102622986, acc: 0.6640625, recall: 0.9344262295081968, precision: 0.59375, f_beta: 0.7261146496815287\n",
      "train: step: 192, loss: 0.6432932615280151, acc: 0.59375, recall: 0.8888888888888888, precision: 0.5544554455445545, f_beta: 0.6829268292682927\n",
      "train: step: 193, loss: 0.640267550945282, acc: 0.625, recall: 0.890625, precision: 0.5816326530612245, f_beta: 0.7037037037037037\n",
      "train: step: 194, loss: 0.672280490398407, acc: 0.5859375, recall: 0.8666666666666667, precision: 0.5360824742268041, f_beta: 0.6624203821656052\n",
      "train: step: 195, loss: 0.6581594944000244, acc: 0.6015625, recall: 0.9090909090909091, precision: 0.5208333333333334, f_beta: 0.6622516556291391\n",
      "train: step: 196, loss: 0.6278126239776611, acc: 0.6875, recall: 0.863013698630137, precision: 0.6774193548387096, f_beta: 0.7590361445783131\n",
      "train: step: 197, loss: 0.6683616042137146, acc: 0.5703125, recall: 0.819672131147541, precision: 0.5319148936170213, f_beta: 0.6451612903225806\n",
      "train: step: 198, loss: 0.6436899900436401, acc: 0.625, recall: 0.8529411764705882, precision: 0.6041666666666666, f_beta: 0.7073170731707317\n",
      "train: step: 199, loss: 0.6323809623718262, acc: 0.65625, recall: 0.7866666666666666, precision: 0.6781609195402298, f_beta: 0.728395061728395\n",
      "train: step: 200, loss: 0.6202333569526672, acc: 0.671875, recall: 0.875, precision: 0.6222222222222222, f_beta: 0.7272727272727274\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:40:43.006095, step: 200, loss: 0.6217568379182082, acc: 0.6410256410256411,precision: 0.9034497886848206, recall: 0.5956542697859236, f_beta: 0.716660220846989\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-200\n",
      "\n",
      "train: step: 201, loss: 0.6075842380523682, acc: 0.640625, recall: 0.8703703703703703, precision: 0.5465116279069767, f_beta: 0.6714285714285714\n",
      "train: step: 202, loss: 0.5767043828964233, acc: 0.6484375, recall: 0.9433962264150944, precision: 0.5434782608695652, f_beta: 0.6896551724137931\n",
      "train: step: 203, loss: 0.6252410411834717, acc: 0.671875, recall: 0.9130434782608695, precision: 0.6363636363636364, f_beta: 0.75\n",
      "train: step: 204, loss: 0.5868630409240723, acc: 0.7578125, recall: 0.9305555555555556, precision: 0.7204301075268817, f_beta: 0.8121212121212121\n",
      "train: step: 205, loss: 0.575305163860321, acc: 0.7265625, recall: 0.9384615384615385, precision: 0.6630434782608695, f_beta: 0.7770700636942676\n",
      "train: step: 206, loss: 0.6209002733230591, acc: 0.6484375, recall: 0.7971014492753623, precision: 0.6395348837209303, f_beta: 0.7096774193548387\n",
      "train: step: 207, loss: 0.6408113837242126, acc: 0.609375, recall: 0.7333333333333333, precision: 0.5641025641025641, f_beta: 0.6376811594202899\n",
      "train: step: 208, loss: 0.6822491884231567, acc: 0.640625, recall: 0.6417910447761194, precision: 0.6615384615384615, f_beta: 0.6515151515151515\n",
      "train: step: 209, loss: 0.6504781246185303, acc: 0.6875, recall: 0.6417910447761194, precision: 0.7288135593220338, f_beta: 0.6825396825396826\n",
      "train: step: 210, loss: 0.6807850003242493, acc: 0.6875, recall: 0.6031746031746031, precision: 0.7169811320754716, f_beta: 0.6551724137931034\n",
      "train: step: 211, loss: 0.6980224847793579, acc: 0.6953125, recall: 0.4666666666666667, precision: 0.8, f_beta: 0.5894736842105264\n",
      "train: step: 212, loss: 0.6266211867332458, acc: 0.6875, recall: 0.3888888888888889, precision: 0.75, f_beta: 0.5121951219512195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 213, loss: 0.6845263242721558, acc: 0.59375, recall: 0.3582089552238806, precision: 0.7272727272727273, f_beta: 0.48\n",
      "train: step: 214, loss: 0.6283344030380249, acc: 0.6484375, recall: 0.6911764705882353, precision: 0.6619718309859155, f_beta: 0.6762589928057554\n",
      "train: step: 215, loss: 0.6547818779945374, acc: 0.6171875, recall: 0.8032786885245902, precision: 0.5697674418604651, f_beta: 0.6666666666666667\n",
      "train: step: 216, loss: 0.6954511404037476, acc: 0.578125, recall: 0.7611940298507462, precision: 0.5730337078651685, f_beta: 0.6538461538461537\n",
      "train: step: 217, loss: 0.6939074397087097, acc: 0.546875, recall: 0.7313432835820896, precision: 0.550561797752809, f_beta: 0.6282051282051283\n",
      "train: step: 218, loss: 0.6659529209136963, acc: 0.6015625, recall: 0.75, precision: 0.6, f_beta: 0.6666666666666665\n",
      "train: step: 219, loss: 0.6513615250587463, acc: 0.5546875, recall: 0.6197183098591549, precision: 0.5945945945945946, f_beta: 0.6068965517241379\n",
      "train: step: 220, loss: 0.6596766114234924, acc: 0.5859375, recall: 0.6551724137931034, precision: 0.5352112676056338, f_beta: 0.5891472868217055\n",
      "train: step: 221, loss: 0.6199797987937927, acc: 0.671875, recall: 0.4642857142857143, precision: 0.6842105263157895, f_beta: 0.5531914893617021\n",
      "train: step: 222, loss: 0.6505725383758545, acc: 0.59375, recall: 0.3548387096774194, precision: 0.6470588235294118, f_beta: 0.4583333333333333\n",
      "train: step: 223, loss: 0.642571210861206, acc: 0.5859375, recall: 0.36619718309859156, precision: 0.7647058823529411, f_beta: 0.49523809523809526\n",
      "train: step: 224, loss: 0.6248459219932556, acc: 0.6328125, recall: 0.31746031746031744, precision: 0.8333333333333334, f_beta: 0.45977011494252873\n",
      "train: step: 225, loss: 0.6770667433738708, acc: 0.546875, recall: 0.15789473684210525, precision: 0.47368421052631576, f_beta: 0.23684210526315788\n",
      "train: step: 226, loss: 0.6255512833595276, acc: 0.5859375, recall: 0.26666666666666666, precision: 0.64, f_beta: 0.37647058823529406\n",
      "train: step: 227, loss: 0.6648593544960022, acc: 0.5625, recall: 0.38666666666666666, precision: 0.7435897435897436, f_beta: 0.5087719298245614\n",
      "train: step: 228, loss: 0.6825730204582214, acc: 0.5546875, recall: 0.532258064516129, precision: 0.5409836065573771, f_beta: 0.5365853658536586\n",
      "train: step: 229, loss: 0.6324189901351929, acc: 0.6015625, recall: 0.7313432835820896, precision: 0.5975609756097561, f_beta: 0.6577181208053691\n",
      "train: step: 230, loss: 0.6605074405670166, acc: 0.546875, recall: 0.8153846153846154, precision: 0.5353535353535354, f_beta: 0.6463414634146342\n",
      "train: step: 231, loss: 0.6804286241531372, acc: 0.5859375, recall: 0.9090909090909091, precision: 0.5102040816326531, f_beta: 0.6535947712418301\n",
      "train: step: 232, loss: 0.6182407140731812, acc: 0.6484375, recall: 0.5753424657534246, precision: 0.75, f_beta: 0.6511627906976744\n",
      "train: step: 233, loss: 0.6273075342178345, acc: 0.6484375, recall: 0.4666666666666667, precision: 0.6829268292682927, f_beta: 0.5544554455445545\n",
      "train: step: 234, loss: 0.6388168334960938, acc: 0.6171875, recall: 0.3064516129032258, precision: 0.76, f_beta: 0.4367816091954023\n",
      "train: step: 235, loss: 0.6403366923332214, acc: 0.609375, recall: 0.3387096774193548, precision: 0.7, f_beta: 0.45652173913043476\n",
      "train: step: 236, loss: 0.6297754049301147, acc: 0.6015625, recall: 0.42028985507246375, precision: 0.725, f_beta: 0.5321100917431193\n",
      "train: step: 237, loss: 0.6230550408363342, acc: 0.6328125, recall: 0.4727272727272727, precision: 0.5909090909090909, f_beta: 0.5252525252525252\n",
      "train: step: 238, loss: 0.5620109438896179, acc: 0.7109375, recall: 0.578125, precision: 0.7872340425531915, f_beta: 0.6666666666666667\n",
      "train: step: 239, loss: 0.5966914892196655, acc: 0.671875, recall: 0.5087719298245614, precision: 0.6744186046511628, f_beta: 0.58\n",
      "train: step: 240, loss: 0.5994473695755005, acc: 0.6796875, recall: 0.5555555555555556, precision: 0.8163265306122449, f_beta: 0.6611570247933884\n",
      "train: step: 241, loss: 0.5879044532775879, acc: 0.71875, recall: 0.8333333333333334, precision: 0.625, f_beta: 0.7142857142857143\n",
      "train: step: 242, loss: 0.5326297879219055, acc: 0.7578125, recall: 0.65, precision: 0.7959183673469388, f_beta: 0.7155963302752293\n",
      "train: step: 243, loss: 0.5744428634643555, acc: 0.7109375, recall: 0.49230769230769234, precision: 0.8888888888888888, f_beta: 0.6336633663366337\n",
      "train: step: 244, loss: 0.48338598012924194, acc: 0.796875, recall: 0.8064516129032258, precision: 0.78125, f_beta: 0.7936507936507936\n",
      "train: step: 245, loss: 0.5281848907470703, acc: 0.7578125, recall: 0.9152542372881356, precision: 0.675, f_beta: 0.7769784172661871\n",
      "train: step: 246, loss: 0.41608792543411255, acc: 0.859375, recall: 0.8307692307692308, precision: 0.8852459016393442, f_beta: 0.8571428571428572\n",
      "train: step: 247, loss: 0.5260252952575684, acc: 0.734375, recall: 0.8857142857142857, precision: 0.7045454545454546, f_beta: 0.7848101265822784\n",
      "train: step: 248, loss: 0.414283812046051, acc: 0.8359375, recall: 0.8793103448275862, precision: 0.7846153846153846, f_beta: 0.8292682926829268\n",
      "train: step: 249, loss: 0.5303531885147095, acc: 0.7890625, recall: 0.6612903225806451, precision: 0.8723404255319149, f_beta: 0.7522935779816514\n",
      "train: step: 250, loss: 0.6182268857955933, acc: 0.7421875, recall: 0.5797101449275363, precision: 0.9090909090909091, f_beta: 0.7079646017699115\n",
      "train: step: 251, loss: 0.515324056148529, acc: 0.78125, recall: 0.6333333333333333, precision: 0.8636363636363636, f_beta: 0.7307692307692307\n",
      "train: step: 252, loss: 0.5479250550270081, acc: 0.7734375, recall: 0.8275862068965517, precision: 0.7164179104477612, f_beta: 0.768\n",
      "train: step: 253, loss: 0.4757898449897766, acc: 0.8125, recall: 0.8636363636363636, precision: 0.7916666666666666, f_beta: 0.8260869565217391\n",
      "train: step: 254, loss: 0.45256805419921875, acc: 0.828125, recall: 0.8732394366197183, precision: 0.8266666666666667, f_beta: 0.8493150684931506\n",
      "train: step: 255, loss: 0.44226333498954773, acc: 0.8359375, recall: 0.8166666666666667, precision: 0.8305084745762712, f_beta: 0.8235294117647058\n",
      "train: step: 256, loss: 0.4896928071975708, acc: 0.8125, recall: 0.8548387096774194, precision: 0.7794117647058824, f_beta: 0.8153846153846154\n",
      "train: step: 257, loss: 0.4395211935043335, acc: 0.84375, recall: 0.8235294117647058, precision: 0.875, f_beta: 0.8484848484848485\n",
      "train: step: 258, loss: 0.4251275360584259, acc: 0.8359375, recall: 0.8309859154929577, precision: 0.8676470588235294, f_beta: 0.8489208633093526\n",
      "train: step: 259, loss: 0.4382305145263672, acc: 0.84375, recall: 0.8611111111111112, precision: 0.8611111111111112, f_beta: 0.8611111111111112\n",
      "train: step: 260, loss: 0.5488551259040833, acc: 0.78125, recall: 0.9365079365079365, precision: 0.7108433734939759, f_beta: 0.8082191780821918\n",
      "train: step: 261, loss: 0.6708899736404419, acc: 0.6875, recall: 1.0, precision: 0.5876288659793815, f_beta: 0.7402597402597402\n",
      "train: step: 262, loss: 0.5851433277130127, acc: 0.71875, recall: 0.921875, precision: 0.6555555555555556, f_beta: 0.7662337662337663\n",
      "train: step: 263, loss: 0.4216153621673584, acc: 0.8203125, recall: 0.8805970149253731, precision: 0.7972972972972973, f_beta: 0.8368794326241135\n",
      "train: step: 264, loss: 0.4348331093788147, acc: 0.8359375, recall: 0.847457627118644, precision: 0.8064516129032258, f_beta: 0.8264462809917354\n",
      "train: step: 265, loss: 0.550145149230957, acc: 0.7421875, recall: 0.625, precision: 0.7446808510638298, f_beta: 0.6796116504854368\n",
      "train: step: 266, loss: 0.47181516885757446, acc: 0.84375, recall: 0.8333333333333334, precision: 0.859375, f_beta: 0.8461538461538461\n",
      "train: step: 267, loss: 0.48726698756217957, acc: 0.8046875, recall: 0.8412698412698413, precision: 0.7794117647058824, f_beta: 0.8091603053435115\n",
      "train: step: 268, loss: 0.4154320955276489, acc: 0.8203125, recall: 0.9180327868852459, precision: 0.7567567567567568, f_beta: 0.8296296296296297\n",
      "train: step: 269, loss: 0.47479110956192017, acc: 0.796875, recall: 0.927536231884058, precision: 0.7529411764705882, f_beta: 0.8311688311688311\n",
      "train: step: 270, loss: 0.47279560565948486, acc: 0.78125, recall: 0.9420289855072463, precision: 0.7303370786516854, f_beta: 0.8227848101265822\n",
      "train: step: 271, loss: 0.5430238842964172, acc: 0.734375, recall: 0.9354838709677419, precision: 0.6590909090909091, f_beta: 0.7733333333333332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 272, loss: 0.4954427480697632, acc: 0.765625, recall: 0.9473684210526315, precision: 0.7346938775510204, f_beta: 0.8275862068965517\n",
      "train: step: 273, loss: 0.5107247233390808, acc: 0.7890625, recall: 0.8767123287671232, precision: 0.7804878048780488, f_beta: 0.8258064516129032\n",
      "train: step: 274, loss: 0.5216819047927856, acc: 0.7578125, recall: 0.8387096774193549, precision: 0.7123287671232876, f_beta: 0.7703703703703705\n",
      "train: step: 275, loss: 0.5305693745613098, acc: 0.734375, recall: 0.6197183098591549, precision: 0.8627450980392157, f_beta: 0.721311475409836\n",
      "train: step: 276, loss: 0.5046405792236328, acc: 0.75, recall: 0.6981132075471698, precision: 0.6981132075471698, f_beta: 0.6981132075471698\n",
      "train: step: 277, loss: 0.43539631366729736, acc: 0.8515625, recall: 0.8225806451612904, precision: 0.864406779661017, f_beta: 0.8429752066115702\n",
      "train: step: 278, loss: 0.4564831256866455, acc: 0.828125, recall: 0.8194444444444444, precision: 0.8676470588235294, f_beta: 0.8428571428571429\n",
      "train: step: 279, loss: 0.7341333627700806, acc: 0.6875, recall: 0.9253731343283582, precision: 0.6391752577319587, f_beta: 0.7560975609756097\n",
      "train: step: 280, loss: 0.4401254951953888, acc: 0.828125, recall: 0.9193548387096774, precision: 0.7702702702702703, f_beta: 0.8382352941176471\n",
      "train: step: 281, loss: 0.49245405197143555, acc: 0.796875, recall: 0.9436619718309859, precision: 0.7528089887640449, f_beta: 0.8374999999999999\n",
      "train: step: 282, loss: 0.47586196660995483, acc: 0.7890625, recall: 0.9452054794520548, precision: 0.75, f_beta: 0.8363636363636364\n",
      "train: step: 283, loss: 0.5827900767326355, acc: 0.6875, recall: 1.0, precision: 0.550561797752809, f_beta: 0.7101449275362319\n",
      "train: step: 284, loss: 0.4704199433326721, acc: 0.8203125, recall: 0.95, precision: 0.7402597402597403, f_beta: 0.8321167883211679\n",
      "train: step: 285, loss: 0.5570210218429565, acc: 0.703125, recall: 0.5245901639344263, precision: 0.7804878048780488, f_beta: 0.627450980392157\n",
      "train: step: 286, loss: 0.7221308350563049, acc: 0.6171875, recall: 0.31343283582089554, precision: 0.875, f_beta: 0.4615384615384615\n",
      "train: step: 287, loss: 0.6124569177627563, acc: 0.6953125, recall: 0.4032258064516129, precision: 0.9259259259259259, f_beta: 0.5617977528089888\n",
      "train: step: 288, loss: 0.5458859801292419, acc: 0.765625, recall: 0.578125, precision: 0.925, f_beta: 0.7115384615384616\n",
      "train: step: 289, loss: 0.5217469930648804, acc: 0.7890625, recall: 0.6307692307692307, precision: 0.9318181818181818, f_beta: 0.7522935779816513\n",
      "train: step: 290, loss: 0.4546116292476654, acc: 0.828125, recall: 0.7241379310344828, precision: 0.875, f_beta: 0.7924528301886793\n",
      "train: step: 291, loss: 0.5489440560340881, acc: 0.7734375, recall: 0.7183098591549296, precision: 0.85, f_beta: 0.7786259541984734\n",
      "train: step: 292, loss: 0.45386919379234314, acc: 0.828125, recall: 0.7833333333333333, precision: 0.8392857142857143, f_beta: 0.8103448275862069\n",
      "train: step: 293, loss: 0.5339736938476562, acc: 0.7734375, recall: 0.7619047619047619, precision: 0.7741935483870968, f_beta: 0.768\n",
      "train: step: 294, loss: 0.6002157926559448, acc: 0.734375, recall: 0.7076923076923077, precision: 0.7540983606557377, f_beta: 0.7301587301587301\n",
      "train: step: 295, loss: 0.5048959255218506, acc: 0.7734375, recall: 0.821917808219178, precision: 0.7894736842105263, f_beta: 0.8053691275167785\n",
      "train: step: 296, loss: 0.5328333973884583, acc: 0.765625, recall: 0.8103448275862069, precision: 0.7121212121212122, f_beta: 0.7580645161290323\n",
      "train: step: 297, loss: 0.5101478099822998, acc: 0.7734375, recall: 0.7936507936507936, precision: 0.7575757575757576, f_beta: 0.7751937984496123\n",
      "train: step: 298, loss: 0.46321186423301697, acc: 0.8515625, recall: 0.9206349206349206, precision: 0.8055555555555556, f_beta: 0.8592592592592592\n",
      "train: step: 299, loss: 0.56778484582901, acc: 0.75, recall: 0.75, precision: 0.7258064516129032, f_beta: 0.7377049180327869\n",
      "train: step: 300, loss: 0.5235236287117004, acc: 0.7734375, recall: 0.7702702702702703, precision: 0.8260869565217391, f_beta: 0.7972027972027972\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:42:02.718669, step: 300, loss: 0.48507890563744765, acc: 0.8108974358974359,precision: 0.7979581807856925, recall: 0.8227644293856647, f_beta: 0.8090428905141925\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-300\n",
      "\n",
      "train: step: 301, loss: 0.4516781270503998, acc: 0.828125, recall: 0.782608695652174, precision: 0.8852459016393442, f_beta: 0.8307692307692308\n",
      "train: step: 302, loss: 0.44904428720474243, acc: 0.8515625, recall: 0.8305084745762712, precision: 0.8448275862068966, f_beta: 0.8376068376068375\n",
      "train: step: 303, loss: 0.46108096837997437, acc: 0.8359375, recall: 0.847457627118644, precision: 0.8064516129032258, f_beta: 0.8264462809917354\n",
      "train: step: 304, loss: 0.5585845708847046, acc: 0.7109375, recall: 0.4461538461538462, precision: 0.9666666666666667, f_beta: 0.6105263157894737\n",
      "train: step: 305, loss: 0.6813904047012329, acc: 0.6484375, recall: 0.328125, precision: 0.9130434782608695, f_beta: 0.4827586206896551\n",
      "train: step: 306, loss: 0.7223286032676697, acc: 0.6171875, recall: 0.2898550724637681, precision: 1.0, f_beta: 0.44943820224719105\n",
      "train: step: 307, loss: 0.7012765407562256, acc: 0.59375, recall: 0.1746031746031746, precision: 1.0, f_beta: 0.29729729729729726\n",
      "train: step: 308, loss: 0.7260739803314209, acc: 0.6015625, recall: 0.1206896551724138, precision: 1.0, f_beta: 0.21538461538461542\n",
      "train: step: 309, loss: 0.7759794592857361, acc: 0.53125, recall: 0.03225806451612903, precision: 1.0, f_beta: 0.0625\n",
      "train: step: 310, loss: 0.7402794361114502, acc: 0.5078125, recall: 0.07352941176470588, precision: 1.0, f_beta: 0.13698630136986303\n",
      "train: step: 311, loss: 0.6991147994995117, acc: 0.515625, recall: 0.16666666666666666, precision: 0.45454545454545453, f_beta: 0.2439024390243902\n",
      "train: step: 312, loss: 0.6984313130378723, acc: 0.515625, recall: 0.4918032786885246, precision: 0.4918032786885246, f_beta: 0.4918032786885246\n",
      "start training model\n",
      "train: step: 313, loss: 0.6875700950622559, acc: 0.5078125, recall: 0.8076923076923077, precision: 0.4421052631578947, f_beta: 0.5714285714285714\n",
      "train: step: 314, loss: 0.7479094862937927, acc: 0.4609375, recall: 0.8703703703703703, precision: 0.43119266055045874, f_beta: 0.5766871165644172\n",
      "train: step: 315, loss: 0.7111752033233643, acc: 0.4921875, recall: 0.7910447761194029, precision: 0.5096153846153846, f_beta: 0.6198830409356725\n",
      "train: step: 316, loss: 0.6555545330047607, acc: 0.578125, recall: 0.9027777777777778, precision: 0.5803571428571429, f_beta: 0.7065217391304347\n",
      "train: step: 317, loss: 0.7233881950378418, acc: 0.5546875, recall: 0.9206349206349206, precision: 0.5272727272727272, f_beta: 0.6705202312138728\n",
      "train: step: 318, loss: 0.726413369178772, acc: 0.5390625, recall: 0.8867924528301887, precision: 0.47, f_beta: 0.6143790849673203\n",
      "train: step: 319, loss: 0.682714581489563, acc: 0.578125, recall: 0.8225806451612904, precision: 0.5425531914893617, f_beta: 0.6538461538461539\n",
      "train: step: 320, loss: 0.7071489095687866, acc: 0.4921875, recall: 0.6811594202898551, precision: 0.5222222222222223, f_beta: 0.5911949685534592\n",
      "train: step: 321, loss: 0.6852835416793823, acc: 0.546875, recall: 0.5797101449275363, precision: 0.5797101449275363, f_beta: 0.5797101449275363\n",
      "train: step: 322, loss: 0.6961991786956787, acc: 0.5546875, recall: 0.4461538461538462, precision: 0.58, f_beta: 0.5043478260869565\n",
      "train: step: 323, loss: 0.7011719942092896, acc: 0.5, recall: 0.3333333333333333, precision: 0.5609756097560976, f_beta: 0.4181818181818182\n",
      "train: step: 324, loss: 0.6853512525558472, acc: 0.4921875, recall: 0.3, precision: 0.43902439024390244, f_beta: 0.3564356435643565\n",
      "train: step: 325, loss: 0.6890697479248047, acc: 0.5078125, recall: 0.2878787878787879, precision: 0.5428571428571428, f_beta: 0.3762376237623762\n",
      "train: step: 326, loss: 0.6635961532592773, acc: 0.6171875, recall: 0.4262295081967213, precision: 0.65, f_beta: 0.5148514851485149\n",
      "train: step: 327, loss: 0.6671466827392578, acc: 0.5859375, recall: 0.47692307692307695, precision: 0.62, f_beta: 0.5391304347826087\n",
      "train: step: 328, loss: 0.6913775205612183, acc: 0.578125, recall: 0.37254901960784315, precision: 0.4634146341463415, f_beta: 0.4130434782608696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 329, loss: 0.7023169994354248, acc: 0.5, recall: 0.4, precision: 0.56, f_beta: 0.4666666666666667\n",
      "train: step: 330, loss: 0.6642295122146606, acc: 0.5546875, recall: 0.43333333333333335, precision: 0.5306122448979592, f_beta: 0.47706422018348627\n",
      "train: step: 331, loss: 0.6804866790771484, acc: 0.578125, recall: 0.43548387096774194, precision: 0.5869565217391305, f_beta: 0.5\n",
      "train: step: 332, loss: 0.6796805262565613, acc: 0.578125, recall: 0.5, precision: 0.5740740740740741, f_beta: 0.5344827586206897\n",
      "train: step: 333, loss: 0.6803600192070007, acc: 0.5546875, recall: 0.546875, precision: 0.5555555555555556, f_beta: 0.5511811023622047\n",
      "train: step: 334, loss: 0.6650419235229492, acc: 0.609375, recall: 0.5846153846153846, precision: 0.6229508196721312, f_beta: 0.6031746031746033\n",
      "train: step: 335, loss: 0.656647264957428, acc: 0.6015625, recall: 0.48484848484848486, precision: 0.6530612244897959, f_beta: 0.5565217391304348\n",
      "train: step: 336, loss: 0.6812736392021179, acc: 0.484375, recall: 0.423728813559322, precision: 0.43859649122807015, f_beta: 0.43103448275862066\n",
      "train: step: 337, loss: 0.6506367325782776, acc: 0.6328125, recall: 0.6333333333333333, precision: 0.6031746031746031, f_beta: 0.6178861788617885\n",
      "train: step: 338, loss: 0.6573092937469482, acc: 0.6171875, recall: 0.6714285714285714, precision: 0.6438356164383562, f_beta: 0.6573426573426574\n",
      "train: step: 339, loss: 0.678317666053772, acc: 0.625, recall: 0.746268656716418, precision: 0.6172839506172839, f_beta: 0.6756756756756757\n",
      "train: step: 340, loss: 0.6649861931800842, acc: 0.5625, recall: 0.7142857142857143, precision: 0.5421686746987951, f_beta: 0.6164383561643836\n",
      "train: step: 341, loss: 0.656004786491394, acc: 0.59375, recall: 0.7285714285714285, precision: 0.6071428571428571, f_beta: 0.6623376623376622\n",
      "train: step: 342, loss: 0.6325652003288269, acc: 0.65625, recall: 0.6307692307692307, precision: 0.6721311475409836, f_beta: 0.6507936507936507\n",
      "train: step: 343, loss: 0.6393579244613647, acc: 0.65625, recall: 0.746268656716418, precision: 0.6493506493506493, f_beta: 0.6944444444444445\n",
      "train: step: 344, loss: 0.6721709966659546, acc: 0.5703125, recall: 0.696969696969697, precision: 0.5679012345679012, f_beta: 0.6258503401360545\n",
      "train: step: 345, loss: 0.6391527652740479, acc: 0.65625, recall: 0.8615384615384616, precision: 0.6153846153846154, f_beta: 0.717948717948718\n",
      "train: step: 346, loss: 0.6240792274475098, acc: 0.6796875, recall: 0.8939393939393939, precision: 0.6344086021505376, f_beta: 0.7421383647798742\n",
      "train: step: 347, loss: 0.651252806186676, acc: 0.671875, recall: 0.7647058823529411, precision: 0.6666666666666666, f_beta: 0.7123287671232877\n",
      "train: step: 348, loss: 0.621589183807373, acc: 0.703125, recall: 0.7746478873239436, precision: 0.7142857142857143, f_beta: 0.7432432432432432\n",
      "train: step: 349, loss: 0.6394578814506531, acc: 0.6484375, recall: 0.7049180327868853, precision: 0.6142857142857143, f_beta: 0.6564885496183206\n",
      "train: step: 350, loss: 0.6032794713973999, acc: 0.6796875, recall: 0.6857142857142857, precision: 0.7164179104477612, f_beta: 0.7007299270072992\n",
      "train: step: 351, loss: 0.6574637293815613, acc: 0.5703125, recall: 0.7068965517241379, precision: 0.5189873417721519, f_beta: 0.5985401459854014\n",
      "train: step: 352, loss: 0.6162258386611938, acc: 0.671875, recall: 0.7903225806451613, precision: 0.6282051282051282, f_beta: 0.7000000000000001\n",
      "train: step: 353, loss: 0.5903809070587158, acc: 0.6875, recall: 0.8032786885245902, precision: 0.6363636363636364, f_beta: 0.7101449275362318\n",
      "train: step: 354, loss: 0.598711371421814, acc: 0.6953125, recall: 0.8405797101449275, precision: 0.6744186046511628, f_beta: 0.7483870967741935\n",
      "train: step: 355, loss: 0.5753300189971924, acc: 0.7421875, recall: 0.7678571428571429, precision: 0.6825396825396826, f_beta: 0.7226890756302521\n",
      "train: step: 356, loss: 0.595253586769104, acc: 0.7265625, recall: 0.5757575757575758, precision: 0.8444444444444444, f_beta: 0.6846846846846847\n",
      "train: step: 357, loss: 0.5601635575294495, acc: 0.8125, recall: 0.7704918032786885, precision: 0.8245614035087719, f_beta: 0.7966101694915254\n",
      "train: step: 358, loss: 0.6136034727096558, acc: 0.7578125, recall: 0.704225352112676, precision: 0.8333333333333334, f_beta: 0.7633587786259541\n",
      "train: step: 359, loss: 0.5366154909133911, acc: 0.828125, recall: 0.7931034482758621, precision: 0.8214285714285714, f_beta: 0.8070175438596492\n",
      "train: step: 360, loss: 0.5014004707336426, acc: 0.859375, recall: 0.8064516129032258, precision: 0.8928571428571429, f_beta: 0.8474576271186439\n",
      "train: step: 361, loss: 0.5594348907470703, acc: 0.734375, recall: 0.6911764705882353, precision: 0.7833333333333333, f_beta: 0.7343749999999999\n",
      "train: step: 362, loss: 0.5593308806419373, acc: 0.734375, recall: 0.6, precision: 0.8297872340425532, f_beta: 0.6964285714285714\n",
      "train: step: 363, loss: 0.5755085349082947, acc: 0.734375, recall: 0.6451612903225806, precision: 0.7692307692307693, f_beta: 0.7017543859649122\n",
      "train: step: 364, loss: 0.5370556116104126, acc: 0.765625, recall: 0.6417910447761194, precision: 0.8775510204081632, f_beta: 0.7413793103448276\n",
      "train: step: 365, loss: 0.5374852418899536, acc: 0.8125, recall: 0.8571428571428571, precision: 0.782608695652174, f_beta: 0.8181818181818182\n",
      "train: step: 366, loss: 0.5231250524520874, acc: 0.8203125, recall: 0.8524590163934426, precision: 0.7878787878787878, f_beta: 0.8188976377952757\n",
      "train: step: 367, loss: 0.451188325881958, acc: 0.875, recall: 0.9090909090909091, precision: 0.819672131147541, f_beta: 0.8620689655172413\n",
      "train: step: 368, loss: 0.4775874614715576, acc: 0.8203125, recall: 0.8028169014084507, precision: 0.8636363636363636, f_beta: 0.832116788321168\n",
      "train: step: 369, loss: 0.5399091243743896, acc: 0.78125, recall: 0.8939393939393939, precision: 0.7375, f_beta: 0.8082191780821918\n",
      "train: step: 370, loss: 0.5042250752449036, acc: 0.7578125, recall: 0.95, precision: 0.6705882352941176, f_beta: 0.786206896551724\n",
      "train: step: 371, loss: 0.4298853278160095, acc: 0.8515625, recall: 0.7777777777777778, precision: 0.9074074074074074, f_beta: 0.8376068376068377\n",
      "train: step: 372, loss: 0.4858759045600891, acc: 0.796875, recall: 0.7058823529411765, precision: 0.8888888888888888, f_beta: 0.7868852459016393\n",
      "train: step: 373, loss: 0.4688163697719574, acc: 0.8125, recall: 0.8, precision: 0.8484848484848485, f_beta: 0.823529411764706\n",
      "train: step: 374, loss: 0.5340538024902344, acc: 0.7734375, recall: 0.9344262295081968, precision: 0.6951219512195121, f_beta: 0.7972027972027972\n",
      "train: step: 375, loss: 0.5342625379562378, acc: 0.734375, recall: 0.875, precision: 0.6447368421052632, f_beta: 0.7424242424242424\n",
      "train: step: 376, loss: 0.4215165674686432, acc: 0.84375, recall: 0.8181818181818182, precision: 0.9130434782608695, f_beta: 0.863013698630137\n",
      "train: step: 377, loss: 0.4233342409133911, acc: 0.8203125, recall: 0.7794117647058824, precision: 0.8688524590163934, f_beta: 0.8217054263565892\n",
      "train: step: 378, loss: 0.4308083653450012, acc: 0.84375, recall: 0.9206349206349206, precision: 0.7945205479452054, f_beta: 0.8529411764705882\n",
      "train: step: 379, loss: 0.4879039227962494, acc: 0.796875, recall: 0.8412698412698413, precision: 0.7681159420289855, f_beta: 0.8030303030303031\n",
      "train: step: 380, loss: 0.46123266220092773, acc: 0.8203125, recall: 0.9016393442622951, precision: 0.7638888888888888, f_beta: 0.8270676691729323\n",
      "train: step: 381, loss: 0.4024185538291931, acc: 0.859375, recall: 0.8405797101449275, precision: 0.8923076923076924, f_beta: 0.8656716417910447\n",
      "train: step: 382, loss: 0.34420180320739746, acc: 0.890625, recall: 0.8701298701298701, precision: 0.9436619718309859, f_beta: 0.9054054054054054\n",
      "train: step: 383, loss: 0.4456218481063843, acc: 0.8046875, recall: 0.7704918032786885, precision: 0.8103448275862069, f_beta: 0.7899159663865546\n",
      "train: step: 384, loss: 0.4248168468475342, acc: 0.8125, recall: 0.96875, precision: 0.7380952380952381, f_beta: 0.8378378378378379\n",
      "train: step: 385, loss: 0.4718036651611328, acc: 0.78125, recall: 0.9655172413793104, precision: 0.6829268292682927, f_beta: 0.8000000000000002\n",
      "train: step: 386, loss: 0.43526357412338257, acc: 0.8203125, recall: 0.6911764705882353, precision: 0.9591836734693877, f_beta: 0.8034188034188035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 387, loss: 0.4368945360183716, acc: 0.8203125, recall: 0.7164179104477612, precision: 0.9230769230769231, f_beta: 0.8067226890756303\n",
      "train: step: 388, loss: 0.37226441502571106, acc: 0.890625, recall: 0.835820895522388, precision: 0.9491525423728814, f_beta: 0.888888888888889\n",
      "train: step: 389, loss: 0.39590081572532654, acc: 0.8515625, recall: 0.8571428571428571, precision: 0.8695652173913043, f_beta: 0.8633093525179856\n",
      "train: step: 390, loss: 0.3460612893104553, acc: 0.875, recall: 0.9827586206896551, precision: 0.7916666666666666, f_beta: 0.8769230769230769\n",
      "train: step: 391, loss: 0.45273923873901367, acc: 0.8125, recall: 0.9444444444444444, precision: 0.7727272727272727, f_beta: 0.85\n",
      "train: step: 392, loss: 0.4011567234992981, acc: 0.8359375, recall: 0.8857142857142857, precision: 0.8266666666666667, f_beta: 0.8551724137931035\n",
      "train: step: 393, loss: 0.3445301055908203, acc: 0.875, recall: 0.9019607843137255, precision: 0.8070175438596491, f_beta: 0.8518518518518519\n",
      "train: step: 394, loss: 0.3953455686569214, acc: 0.828125, recall: 0.7681159420289855, precision: 0.8983050847457628, f_beta: 0.828125\n",
      "train: step: 395, loss: 0.4660404920578003, acc: 0.8203125, recall: 0.7575757575757576, precision: 0.8771929824561403, f_beta: 0.8130081300813008\n",
      "train: step: 396, loss: 0.36461859941482544, acc: 0.8203125, recall: 0.7272727272727273, precision: 0.9056603773584906, f_beta: 0.8067226890756303\n",
      "train: step: 397, loss: 0.3942239284515381, acc: 0.8515625, recall: 0.9402985074626866, precision: 0.8076923076923077, f_beta: 0.8689655172413794\n",
      "train: step: 398, loss: 0.3999665379524231, acc: 0.8515625, recall: 0.9701492537313433, precision: 0.7926829268292683, f_beta: 0.8724832214765101\n",
      "train: step: 399, loss: 0.5206699967384338, acc: 0.7890625, recall: 0.9344262295081968, precision: 0.7125, f_beta: 0.8085106382978724\n",
      "train: step: 400, loss: 0.3482734262943268, acc: 0.875, recall: 0.8356164383561644, precision: 0.9384615384615385, f_beta: 0.8840579710144928\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:43:22.412052, step: 400, loss: 0.404829319470968, acc: 0.8323317307692307,precision: 0.7673263519253228, recall: 0.885941796270827, f_beta: 0.821466118480239\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-400\n",
      "\n",
      "train: step: 401, loss: 0.38858136534690857, acc: 0.828125, recall: 0.7833333333333333, precision: 0.8392857142857143, f_beta: 0.8103448275862069\n",
      "train: step: 402, loss: 0.38942182064056396, acc: 0.8515625, recall: 0.8636363636363636, precision: 0.8507462686567164, f_beta: 0.8571428571428571\n",
      "train: step: 403, loss: 0.352024644613266, acc: 0.8828125, recall: 0.85, precision: 0.8947368421052632, f_beta: 0.8717948717948718\n",
      "train: step: 404, loss: 0.359921395778656, acc: 0.875, recall: 0.9523809523809523, precision: 0.821917808219178, f_beta: 0.8823529411764706\n",
      "train: step: 405, loss: 0.3602706789970398, acc: 0.8828125, recall: 0.8840579710144928, precision: 0.8970588235294118, f_beta: 0.8905109489051095\n",
      "train: step: 406, loss: 0.3944992423057556, acc: 0.8359375, recall: 0.8703703703703703, precision: 0.7704918032786885, f_beta: 0.817391304347826\n",
      "train: step: 407, loss: 0.38329458236694336, acc: 0.84375, recall: 0.8852459016393442, precision: 0.8059701492537313, f_beta: 0.84375\n",
      "train: step: 408, loss: 0.45882943272590637, acc: 0.7890625, recall: 0.9032258064516129, precision: 0.7272727272727273, f_beta: 0.8057553956834532\n",
      "train: step: 409, loss: 0.424210786819458, acc: 0.84375, recall: 0.8714285714285714, precision: 0.8472222222222222, f_beta: 0.8591549295774648\n",
      "train: step: 410, loss: 0.38277995586395264, acc: 0.8515625, recall: 0.90625, precision: 0.8169014084507042, f_beta: 0.8592592592592593\n",
      "train: step: 411, loss: 0.3566622734069824, acc: 0.875, recall: 0.9264705882352942, precision: 0.8513513513513513, f_beta: 0.8873239436619719\n",
      "train: step: 412, loss: 0.3665151596069336, acc: 0.859375, recall: 0.8732394366197183, precision: 0.8732394366197183, f_beta: 0.8732394366197183\n",
      "train: step: 413, loss: 0.3303109109401703, acc: 0.890625, recall: 0.8983050847457628, precision: 0.8688524590163934, f_beta: 0.8833333333333333\n",
      "train: step: 414, loss: 0.3655466139316559, acc: 0.828125, recall: 0.7611940298507462, precision: 0.8947368421052632, f_beta: 0.8225806451612904\n",
      "train: step: 415, loss: 0.3968639373779297, acc: 0.8203125, recall: 0.6885245901639344, precision: 0.9130434782608695, f_beta: 0.7850467289719626\n",
      "train: step: 416, loss: 0.465030312538147, acc: 0.828125, recall: 0.8311688311688312, precision: 0.8767123287671232, f_beta: 0.8533333333333334\n",
      "train: step: 417, loss: 0.41722214221954346, acc: 0.8203125, recall: 0.7971014492753623, precision: 0.859375, f_beta: 0.8270676691729324\n",
      "train: step: 418, loss: 0.3850918710231781, acc: 0.875, recall: 0.8947368421052632, precision: 0.8360655737704918, f_beta: 0.864406779661017\n",
      "train: step: 419, loss: 0.43308454751968384, acc: 0.84375, recall: 0.8421052631578947, precision: 0.8135593220338984, f_beta: 0.8275862068965518\n",
      "train: step: 420, loss: 0.33040329813957214, acc: 0.859375, recall: 0.8888888888888888, precision: 0.8648648648648649, f_beta: 0.8767123287671232\n",
      "train: step: 421, loss: 0.3588103652000427, acc: 0.875, recall: 0.8333333333333334, precision: 0.8653846153846154, f_beta: 0.8490566037735849\n",
      "train: step: 422, loss: 0.5315898656845093, acc: 0.796875, recall: 0.6875, precision: 0.88, f_beta: 0.7719298245614036\n",
      "train: step: 423, loss: 0.4181004464626312, acc: 0.8125, recall: 0.7761194029850746, precision: 0.8524590163934426, f_beta: 0.8124999999999999\n",
      "train: step: 424, loss: 0.4454750716686249, acc: 0.8125, recall: 0.8194444444444444, precision: 0.8428571428571429, f_beta: 0.8309859154929577\n",
      "train: step: 425, loss: 0.38303500413894653, acc: 0.828125, recall: 0.9305555555555556, precision: 0.7976190476190477, f_beta: 0.858974358974359\n",
      "train: step: 426, loss: 0.41877278685569763, acc: 0.8125, recall: 0.8888888888888888, precision: 0.7671232876712328, f_beta: 0.8235294117647058\n",
      "train: step: 427, loss: 0.3244686424732208, acc: 0.8671875, recall: 0.8125, precision: 0.9122807017543859, f_beta: 0.859504132231405\n",
      "train: step: 428, loss: 0.37088990211486816, acc: 0.8671875, recall: 0.864406779661017, precision: 0.85, f_beta: 0.8571428571428572\n",
      "train: step: 429, loss: 0.34953218698501587, acc: 0.8515625, recall: 0.7910447761194029, precision: 0.9137931034482759, f_beta: 0.848\n",
      "train: step: 430, loss: 0.4030326306819916, acc: 0.84375, recall: 0.7903225806451613, precision: 0.875, f_beta: 0.8305084745762712\n",
      "train: step: 431, loss: 0.3628655672073364, acc: 0.8671875, recall: 0.8867924528301887, precision: 0.8103448275862069, f_beta: 0.8468468468468469\n",
      "train: step: 432, loss: 0.3314472436904907, acc: 0.875, recall: 0.9333333333333333, precision: 0.8235294117647058, f_beta: 0.8749999999999999\n",
      "train: step: 433, loss: 0.3319041132926941, acc: 0.8828125, recall: 0.9180327868852459, precision: 0.8484848484848485, f_beta: 0.8818897637795275\n",
      "train: step: 434, loss: 0.5018429756164551, acc: 0.8203125, recall: 0.9180327868852459, precision: 0.7567567567567568, f_beta: 0.8296296296296297\n",
      "train: step: 435, loss: 0.3096306324005127, acc: 0.8671875, recall: 0.9642857142857143, precision: 0.782608695652174, f_beta: 0.864\n",
      "train: step: 436, loss: 0.34469884634017944, acc: 0.875, recall: 0.9076923076923077, precision: 0.855072463768116, f_beta: 0.8805970149253731\n",
      "train: step: 437, loss: 0.47105926275253296, acc: 0.8125, recall: 0.7096774193548387, precision: 0.88, f_beta: 0.7857142857142856\n",
      "train: step: 438, loss: 0.41141048073768616, acc: 0.84375, recall: 0.726027397260274, precision: 1.0, f_beta: 0.8412698412698413\n",
      "train: step: 439, loss: 0.5133031606674194, acc: 0.8125, recall: 0.625, precision: 1.0, f_beta: 0.7692307692307693\n",
      "train: step: 440, loss: 0.3323798179626465, acc: 0.8984375, recall: 0.8615384615384616, precision: 0.9333333333333333, f_beta: 0.8960000000000001\n",
      "train: step: 441, loss: 0.2877653241157532, acc: 0.890625, recall: 0.9523809523809523, precision: 0.8450704225352113, f_beta: 0.8955223880597014\n",
      "train: step: 442, loss: 0.3280685544013977, acc: 0.859375, recall: 0.9464285714285714, precision: 0.7794117647058824, f_beta: 0.8548387096774193\n",
      "train: step: 443, loss: 0.4553001821041107, acc: 0.7890625, recall: 0.9666666666666667, precision: 0.6987951807228916, f_beta: 0.8111888111888113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 444, loss: 0.3236098885536194, acc: 0.875, recall: 0.9692307692307692, precision: 0.8181818181818182, f_beta: 0.8873239436619719\n",
      "train: step: 445, loss: 0.4205116629600525, acc: 0.859375, recall: 0.9152542372881356, precision: 0.8059701492537313, f_beta: 0.8571428571428572\n",
      "train: step: 446, loss: 0.2577555179595947, acc: 0.921875, recall: 0.9047619047619048, precision: 0.9344262295081968, f_beta: 0.9193548387096775\n",
      "train: step: 447, loss: 0.38193657994270325, acc: 0.8203125, recall: 0.7866666666666666, precision: 0.8939393939393939, f_beta: 0.8368794326241135\n",
      "train: step: 448, loss: 0.3454376459121704, acc: 0.8671875, recall: 0.7972972972972973, precision: 0.9672131147540983, f_beta: 0.874074074074074\n",
      "train: step: 449, loss: 0.3804624080657959, acc: 0.8671875, recall: 0.7833333333333333, precision: 0.9215686274509803, f_beta: 0.8468468468468469\n",
      "train: step: 450, loss: 0.41704457998275757, acc: 0.828125, recall: 0.7971014492753623, precision: 0.873015873015873, f_beta: 0.8333333333333334\n",
      "train: step: 451, loss: 0.39899176359176636, acc: 0.828125, recall: 0.8545454545454545, precision: 0.7704918032786885, f_beta: 0.810344827586207\n",
      "train: step: 452, loss: 0.4205567538738251, acc: 0.8125, recall: 0.7457627118644068, precision: 0.8301886792452831, f_beta: 0.7857142857142858\n",
      "train: step: 453, loss: 0.3896257281303406, acc: 0.859375, recall: 0.847457627118644, precision: 0.847457627118644, f_beta: 0.847457627118644\n",
      "train: step: 454, loss: 0.3517302870750427, acc: 0.84375, recall: 0.8507462686567164, precision: 0.8507462686567164, f_beta: 0.8507462686567164\n",
      "train: step: 455, loss: 0.4117172062397003, acc: 0.8203125, recall: 0.8571428571428571, precision: 0.7241379310344828, f_beta: 0.7850467289719627\n",
      "train: step: 456, loss: 0.3065611720085144, acc: 0.8984375, recall: 0.9130434782608695, precision: 0.9, f_beta: 0.9064748201438848\n",
      "train: step: 457, loss: 0.28288546204566956, acc: 0.8671875, recall: 0.9056603773584906, precision: 0.8, f_beta: 0.8495575221238938\n",
      "train: step: 458, loss: 0.4282905161380768, acc: 0.8046875, recall: 0.796875, precision: 0.8095238095238095, f_beta: 0.8031496062992126\n",
      "train: step: 459, loss: 0.3134434223175049, acc: 0.875, recall: 0.7936507936507936, precision: 0.9433962264150944, f_beta: 0.8620689655172413\n",
      "train: step: 460, loss: 0.45610278844833374, acc: 0.8203125, recall: 0.734375, precision: 0.8867924528301887, f_beta: 0.8034188034188035\n",
      "train: step: 461, loss: 0.4643183946609497, acc: 0.8203125, recall: 0.6949152542372882, precision: 0.8913043478260869, f_beta: 0.780952380952381\n",
      "train: step: 462, loss: 0.3988693654537201, acc: 0.7734375, recall: 0.6621621621621622, precision: 0.9245283018867925, f_beta: 0.7716535433070866\n",
      "train: step: 463, loss: 0.443469375371933, acc: 0.8203125, recall: 0.7230769230769231, precision: 0.9038461538461539, f_beta: 0.8034188034188035\n",
      "train: step: 464, loss: 0.3787349462509155, acc: 0.859375, recall: 0.8727272727272727, precision: 0.8135593220338984, f_beta: 0.8421052631578948\n",
      "train: step: 465, loss: 0.3588700592517853, acc: 0.8671875, recall: 0.875, precision: 0.8305084745762712, f_beta: 0.8521739130434782\n",
      "train: step: 466, loss: 0.29836827516555786, acc: 0.90625, recall: 0.9, precision: 0.9, f_beta: 0.9\n",
      "train: step: 467, loss: 0.425963431596756, acc: 0.8203125, recall: 0.8235294117647058, precision: 0.835820895522388, f_beta: 0.8296296296296296\n",
      "train: step: 468, loss: 0.41363221406936646, acc: 0.8515625, recall: 0.8888888888888888, precision: 0.7868852459016393, f_beta: 0.8347826086956522\n",
      "start training model\n",
      "train: step: 469, loss: 0.2885129451751709, acc: 0.9140625, recall: 0.9848484848484849, precision: 0.8666666666666667, f_beta: 0.9219858156028369\n",
      "train: step: 470, loss: 0.28766757249832153, acc: 0.8984375, recall: 0.9838709677419355, precision: 0.8356164383561644, f_beta: 0.9037037037037038\n",
      "train: step: 471, loss: 0.34938088059425354, acc: 0.8671875, recall: 0.9166666666666666, precision: 0.8208955223880597, f_beta: 0.8661417322834646\n",
      "train: step: 472, loss: 0.25727567076683044, acc: 0.921875, recall: 0.9384615384615385, precision: 0.9104477611940298, f_beta: 0.9242424242424243\n",
      "train: step: 473, loss: 0.31459254026412964, acc: 0.875, recall: 0.9242424242424242, precision: 0.8472222222222222, f_beta: 0.8840579710144927\n",
      "train: step: 474, loss: 0.29182708263397217, acc: 0.921875, recall: 0.9464285714285714, precision: 0.8833333333333333, f_beta: 0.9137931034482758\n",
      "train: step: 475, loss: 0.31984198093414307, acc: 0.8828125, recall: 0.8620689655172413, precision: 0.8771929824561403, f_beta: 0.8695652173913043\n",
      "train: step: 476, loss: 0.3515856862068176, acc: 0.8671875, recall: 0.85, precision: 0.864406779661017, f_beta: 0.8571428571428572\n",
      "train: step: 477, loss: 0.30343639850616455, acc: 0.9140625, recall: 0.8939393939393939, precision: 0.9365079365079365, f_beta: 0.9147286821705426\n",
      "train: step: 478, loss: 0.252689391374588, acc: 0.9140625, recall: 0.9230769230769231, precision: 0.9090909090909091, f_beta: 0.9160305343511451\n",
      "train: step: 479, loss: 0.33502861857414246, acc: 0.8828125, recall: 0.8571428571428571, precision: 0.9230769230769231, f_beta: 0.888888888888889\n",
      "train: step: 480, loss: 0.25943514704704285, acc: 0.8984375, recall: 0.8857142857142857, precision: 0.9253731343283582, f_beta: 0.9051094890510949\n",
      "train: step: 481, loss: 0.36092114448547363, acc: 0.859375, recall: 0.8405797101449275, precision: 0.8923076923076924, f_beta: 0.8656716417910447\n",
      "train: step: 482, loss: 0.42624229192733765, acc: 0.84375, recall: 0.8461538461538461, precision: 0.8461538461538461, f_beta: 0.8461538461538461\n",
      "train: step: 483, loss: 0.3154616057872772, acc: 0.890625, recall: 0.8125, precision: 0.9629629629629629, f_beta: 0.8813559322033898\n",
      "train: step: 484, loss: 0.2402714043855667, acc: 0.90625, recall: 0.8387096774193549, precision: 0.9629629629629629, f_beta: 0.896551724137931\n",
      "train: step: 485, loss: 0.27809444069862366, acc: 0.90625, recall: 0.9491525423728814, precision: 0.8615384615384616, f_beta: 0.903225806451613\n",
      "train: step: 486, loss: 0.41713643074035645, acc: 0.8359375, recall: 0.9076923076923077, precision: 0.7972972972972973, f_beta: 0.8489208633093526\n",
      "train: step: 487, loss: 0.31257450580596924, acc: 0.859375, recall: 0.9538461538461539, precision: 0.8051948051948052, f_beta: 0.8732394366197184\n",
      "train: step: 488, loss: 0.3280048072338104, acc: 0.8671875, recall: 0.9487179487179487, precision: 0.8505747126436781, f_beta: 0.896969696969697\n",
      "train: step: 489, loss: 0.2935284674167633, acc: 0.8984375, recall: 1.0, precision: 0.8395061728395061, f_beta: 0.912751677852349\n",
      "train: step: 490, loss: 0.4044319987297058, acc: 0.828125, recall: 0.9558823529411765, precision: 0.7738095238095238, f_beta: 0.8552631578947368\n",
      "train: step: 491, loss: 0.4205859303474426, acc: 0.796875, recall: 0.8913043478260869, precision: 0.6612903225806451, f_beta: 0.7592592592592593\n",
      "train: step: 492, loss: 0.2451682686805725, acc: 0.9140625, recall: 0.9166666666666666, precision: 0.9295774647887324, f_beta: 0.9230769230769231\n",
      "train: step: 493, loss: 0.29695188999176025, acc: 0.8828125, recall: 0.8769230769230769, precision: 0.890625, f_beta: 0.883720930232558\n",
      "train: step: 494, loss: 0.31755003333091736, acc: 0.8671875, recall: 0.7692307692307693, precision: 0.9615384615384616, f_beta: 0.8547008547008548\n",
      "train: step: 495, loss: 0.3659467101097107, acc: 0.84375, recall: 0.711864406779661, precision: 0.9333333333333333, f_beta: 0.8076923076923076\n",
      "train: step: 496, loss: 0.3235537111759186, acc: 0.875, recall: 0.7592592592592593, precision: 0.9318181818181818, f_beta: 0.836734693877551\n",
      "train: step: 497, loss: 0.3068353831768036, acc: 0.9140625, recall: 0.8873239436619719, precision: 0.9545454545454546, f_beta: 0.9197080291970803\n",
      "train: step: 498, loss: 0.3654322624206543, acc: 0.859375, recall: 0.9354838709677419, precision: 0.8055555555555556, f_beta: 0.8656716417910448\n",
      "train: step: 499, loss: 0.2878803610801697, acc: 0.90625, recall: 0.9206349206349206, precision: 0.8923076923076924, f_beta: 0.90625\n",
      "train: step: 500, loss: 0.2925664186477661, acc: 0.8671875, recall: 0.9545454545454546, precision: 0.8181818181818182, f_beta: 0.881118881118881\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:44:41.319201, step: 500, loss: 0.40608985531024444, acc: 0.8359375,precision: 0.9201645153520861, recall: 0.7898583859164966, f_beta: 0.8488781748951114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-500\n",
      "\n",
      "train: step: 501, loss: 0.2972283959388733, acc: 0.90625, recall: 0.9701492537313433, precision: 0.8666666666666667, f_beta: 0.915492957746479\n",
      "train: step: 502, loss: 0.3679940700531006, acc: 0.8359375, recall: 0.9264705882352942, precision: 0.7974683544303798, f_beta: 0.8571428571428572\n",
      "train: step: 503, loss: 0.3454097509384155, acc: 0.875, recall: 0.8939393939393939, precision: 0.8676470588235294, f_beta: 0.8805970149253731\n",
      "train: step: 504, loss: 0.26794111728668213, acc: 0.8828125, recall: 0.9285714285714286, precision: 0.8253968253968254, f_beta: 0.8739495798319328\n",
      "train: step: 505, loss: 0.3388272225856781, acc: 0.890625, recall: 0.9830508474576272, precision: 0.8169014084507042, f_beta: 0.8923076923076924\n",
      "train: step: 506, loss: 0.27562856674194336, acc: 0.8984375, recall: 0.9242424242424242, precision: 0.8840579710144928, f_beta: 0.9037037037037037\n",
      "train: step: 507, loss: 0.269278347492218, acc: 0.8984375, recall: 0.9393939393939394, precision: 0.8732394366197183, f_beta: 0.9051094890510948\n",
      "train: step: 508, loss: 0.2930137813091278, acc: 0.9140625, recall: 0.9264705882352942, precision: 0.9130434782608695, f_beta: 0.9197080291970804\n",
      "train: step: 509, loss: 0.27733033895492554, acc: 0.921875, recall: 0.90625, precision: 0.9354838709677419, f_beta: 0.9206349206349206\n",
      "train: step: 510, loss: 0.29470765590667725, acc: 0.9140625, recall: 0.8823529411764706, precision: 0.9523809523809523, f_beta: 0.916030534351145\n",
      "train: step: 511, loss: 0.2552787959575653, acc: 0.90625, recall: 0.9180327868852459, precision: 0.8888888888888888, f_beta: 0.9032258064516128\n",
      "train: step: 512, loss: 0.28972476720809937, acc: 0.890625, recall: 0.9016393442622951, precision: 0.873015873015873, f_beta: 0.8870967741935485\n",
      "train: step: 513, loss: 0.28620243072509766, acc: 0.890625, recall: 0.9649122807017544, precision: 0.8208955223880597, f_beta: 0.8870967741935484\n",
      "train: step: 514, loss: 0.21152722835540771, acc: 0.921875, recall: 0.9354838709677419, precision: 0.90625, f_beta: 0.9206349206349206\n",
      "train: step: 515, loss: 0.2736552059650421, acc: 0.875, recall: 0.8125, precision: 0.9285714285714286, f_beta: 0.8666666666666666\n",
      "train: step: 516, loss: 0.3203830122947693, acc: 0.859375, recall: 0.8387096774193549, precision: 0.8666666666666667, f_beta: 0.8524590163934426\n",
      "train: step: 517, loss: 0.23611438274383545, acc: 0.9296875, recall: 0.8923076923076924, precision: 0.9666666666666667, f_beta: 0.928\n",
      "train: step: 518, loss: 0.28767409920692444, acc: 0.8828125, recall: 0.828125, precision: 0.9298245614035088, f_beta: 0.8760330578512396\n",
      "train: step: 519, loss: 0.3184766173362732, acc: 0.875, recall: 0.8225806451612904, precision: 0.9107142857142857, f_beta: 0.864406779661017\n",
      "train: step: 520, loss: 0.3417896032333374, acc: 0.8828125, recall: 0.90625, precision: 0.8656716417910447, f_beta: 0.8854961832061069\n",
      "train: step: 521, loss: 0.32005149126052856, acc: 0.890625, recall: 0.9032258064516129, precision: 0.875, f_beta: 0.8888888888888888\n",
      "train: step: 522, loss: 0.3027690351009369, acc: 0.8828125, recall: 0.9230769230769231, precision: 0.8571428571428571, f_beta: 0.888888888888889\n",
      "train: step: 523, loss: 0.32767608761787415, acc: 0.8671875, recall: 0.8947368421052632, precision: 0.8225806451612904, f_beta: 0.8571428571428571\n",
      "train: step: 524, loss: 0.308157742023468, acc: 0.890625, recall: 0.8852459016393442, precision: 0.8852459016393442, f_beta: 0.8852459016393442\n",
      "train: step: 525, loss: 0.22433072328567505, acc: 0.921875, recall: 0.9253731343283582, precision: 0.9253731343283582, f_beta: 0.9253731343283582\n",
      "train: step: 526, loss: 0.24017266929149628, acc: 0.921875, recall: 0.8709677419354839, precision: 0.9642857142857143, f_beta: 0.9152542372881356\n",
      "train: step: 527, loss: 0.3586917519569397, acc: 0.8828125, recall: 0.7966101694915254, precision: 0.94, f_beta: 0.8623853211009174\n",
      "train: step: 528, loss: 0.2339000105857849, acc: 0.921875, recall: 0.9322033898305084, precision: 0.9016393442622951, f_beta: 0.9166666666666666\n",
      "train: step: 529, loss: 0.31689995527267456, acc: 0.8671875, recall: 0.8285714285714286, precision: 0.9206349206349206, f_beta: 0.8721804511278196\n",
      "train: step: 530, loss: 0.2617869973182678, acc: 0.921875, recall: 0.9436619718309859, precision: 0.9178082191780822, f_beta: 0.9305555555555556\n",
      "train: step: 531, loss: 0.3230286240577698, acc: 0.859375, recall: 0.8059701492537313, precision: 0.9152542372881356, f_beta: 0.8571428571428572\n",
      "train: step: 532, loss: 0.3160463273525238, acc: 0.8671875, recall: 0.796875, precision: 0.9272727272727272, f_beta: 0.8571428571428571\n",
      "train: step: 533, loss: 0.23823782801628113, acc: 0.90625, recall: 0.8275862068965517, precision: 0.96, f_beta: 0.888888888888889\n",
      "train: step: 534, loss: 0.3662755489349365, acc: 0.828125, recall: 0.7540983606557377, precision: 0.8679245283018868, f_beta: 0.8070175438596491\n",
      "train: step: 535, loss: 0.34772634506225586, acc: 0.8671875, recall: 0.7627118644067796, precision: 0.9375, f_beta: 0.8411214953271028\n",
      "train: step: 536, loss: 0.1811826527118683, acc: 0.953125, recall: 0.9375, precision: 0.967741935483871, f_beta: 0.9523809523809523\n",
      "train: step: 537, loss: 0.22506771981716156, acc: 0.8984375, recall: 0.8787878787878788, precision: 0.9206349206349206, f_beta: 0.8992248062015504\n",
      "train: step: 538, loss: 0.32605621218681335, acc: 0.8828125, recall: 1.0, precision: 0.7945205479452054, f_beta: 0.8854961832061068\n",
      "train: step: 539, loss: 0.20667588710784912, acc: 0.9453125, recall: 0.9818181818181818, precision: 0.9, f_beta: 0.9391304347826087\n",
      "train: step: 540, loss: 0.3204275965690613, acc: 0.890625, recall: 0.875, precision: 0.875, f_beta: 0.875\n",
      "train: step: 541, loss: 0.24353881180286407, acc: 0.8984375, recall: 0.8507462686567164, precision: 0.95, f_beta: 0.8976377952755905\n",
      "train: step: 542, loss: 0.20399072766304016, acc: 0.9296875, recall: 0.9354838709677419, precision: 0.9206349206349206, f_beta: 0.9279999999999999\n",
      "train: step: 543, loss: 0.23988530039787292, acc: 0.875, recall: 0.8225806451612904, precision: 0.9107142857142857, f_beta: 0.864406779661017\n",
      "train: step: 544, loss: 0.26745355129241943, acc: 0.8828125, recall: 0.8356164383561644, precision: 0.953125, f_beta: 0.8905109489051095\n",
      "train: step: 545, loss: 0.37725239992141724, acc: 0.8828125, recall: 0.8823529411764706, precision: 0.8955223880597015, f_beta: 0.888888888888889\n",
      "train: step: 546, loss: 0.24375557899475098, acc: 0.9296875, recall: 0.9090909090909091, precision: 0.9259259259259259, f_beta: 0.9174311926605504\n",
      "train: step: 547, loss: 0.3073155879974365, acc: 0.8515625, recall: 0.8607594936708861, precision: 0.8947368421052632, f_beta: 0.8774193548387097\n",
      "train: step: 548, loss: 0.2196555733680725, acc: 0.9453125, recall: 0.9298245614035088, precision: 0.9464285714285714, f_beta: 0.9380530973451328\n",
      "train: step: 549, loss: 0.26661571860313416, acc: 0.890625, recall: 0.8805970149253731, precision: 0.9076923076923077, f_beta: 0.8939393939393939\n",
      "train: step: 550, loss: 0.35976025462150574, acc: 0.8828125, recall: 0.8307692307692308, precision: 0.9310344827586207, f_beta: 0.8780487804878049\n",
      "train: step: 551, loss: 0.3416658341884613, acc: 0.890625, recall: 0.84375, precision: 0.9310344827586207, f_beta: 0.8852459016393444\n",
      "train: step: 552, loss: 0.21976077556610107, acc: 0.9375, recall: 0.9538461538461539, precision: 0.9253731343283582, f_beta: 0.9393939393939394\n",
      "train: step: 553, loss: 0.20729342103004456, acc: 0.9296875, recall: 0.9322033898305084, precision: 0.9166666666666666, f_beta: 0.9243697478991596\n",
      "train: step: 554, loss: 0.14596426486968994, acc: 0.953125, recall: 0.9846153846153847, precision: 0.927536231884058, f_beta: 0.9552238805970149\n",
      "train: step: 555, loss: 0.273010790348053, acc: 0.8984375, recall: 0.9848484848484849, precision: 0.8441558441558441, f_beta: 0.9090909090909091\n",
      "train: step: 556, loss: 0.2872270345687866, acc: 0.90625, recall: 0.9696969696969697, precision: 0.8648648648648649, f_beta: 0.9142857142857143\n",
      "train: step: 557, loss: 0.2955172061920166, acc: 0.859375, recall: 0.9, precision: 0.8181818181818182, f_beta: 0.8571428571428572\n",
      "train: step: 558, loss: 0.268200159072876, acc: 0.9140625, recall: 0.9, precision: 0.9152542372881356, f_beta: 0.9075630252100839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 559, loss: 0.31286776065826416, acc: 0.890625, recall: 0.890625, precision: 0.890625, f_beta: 0.890625\n",
      "train: step: 560, loss: 0.2591824233531952, acc: 0.890625, recall: 0.8888888888888888, precision: 0.8571428571428571, f_beta: 0.8727272727272727\n",
      "train: step: 561, loss: 0.422817200422287, acc: 0.8671875, recall: 0.9041095890410958, precision: 0.868421052631579, f_beta: 0.8859060402684564\n",
      "train: step: 562, loss: 0.4261975884437561, acc: 0.8046875, recall: 0.9016393442622951, precision: 0.7432432432432432, f_beta: 0.8148148148148148\n",
      "train: step: 563, loss: 0.34023359417915344, acc: 0.8671875, recall: 0.9726027397260274, precision: 0.8255813953488372, f_beta: 0.8930817610062893\n",
      "train: step: 564, loss: 0.3459819555282593, acc: 0.8515625, recall: 0.9726027397260274, precision: 0.8068181818181818, f_beta: 0.8819875776397516\n",
      "train: step: 565, loss: 0.37465864419937134, acc: 0.8125, recall: 0.9242424242424242, precision: 0.7625, f_beta: 0.8356164383561645\n",
      "train: step: 566, loss: 0.3501812517642975, acc: 0.859375, recall: 0.9210526315789473, precision: 0.8536585365853658, f_beta: 0.8860759493670887\n",
      "train: step: 567, loss: 0.30132195353507996, acc: 0.8828125, recall: 0.9710144927536232, precision: 0.8375, f_beta: 0.8993288590604026\n",
      "train: step: 568, loss: 0.3905786871910095, acc: 0.828125, recall: 0.9117647058823529, precision: 0.7948717948717948, f_beta: 0.8493150684931507\n",
      "train: step: 569, loss: 0.37189027667045593, acc: 0.828125, recall: 0.8103448275862069, precision: 0.8103448275862069, f_beta: 0.8103448275862069\n",
      "train: step: 570, loss: 0.35777363181114197, acc: 0.8515625, recall: 0.8253968253968254, precision: 0.8666666666666667, f_beta: 0.8455284552845528\n",
      "train: step: 571, loss: 0.32202214002609253, acc: 0.8515625, recall: 0.8507462686567164, precision: 0.8636363636363636, f_beta: 0.8571428571428571\n",
      "train: step: 572, loss: 0.31691503524780273, acc: 0.875, recall: 0.95, precision: 0.8142857142857143, f_beta: 0.8769230769230769\n",
      "train: step: 573, loss: 0.2928931415081024, acc: 0.890625, recall: 0.8771929824561403, precision: 0.8771929824561403, f_beta: 0.8771929824561403\n",
      "train: step: 574, loss: 0.4371013343334198, acc: 0.8125, recall: 0.7323943661971831, precision: 0.9122807017543859, f_beta: 0.8124999999999999\n",
      "train: step: 575, loss: 0.4653969407081604, acc: 0.8203125, recall: 0.746268656716418, precision: 0.8928571428571429, f_beta: 0.8130081300813009\n",
      "train: step: 576, loss: 0.2772618532180786, acc: 0.9140625, recall: 0.819672131147541, precision: 1.0, f_beta: 0.9009009009009009\n",
      "train: step: 577, loss: 0.22884878516197205, acc: 0.90625, recall: 0.8518518518518519, precision: 0.92, f_beta: 0.8846153846153846\n",
      "train: step: 578, loss: 0.2842293679714203, acc: 0.8671875, recall: 0.7636363636363637, precision: 0.9130434782608695, f_beta: 0.8316831683168316\n",
      "train: step: 579, loss: 0.37604567408561707, acc: 0.8671875, recall: 0.8656716417910447, precision: 0.8787878787878788, f_beta: 0.8721804511278195\n",
      "train: step: 580, loss: 0.3142067790031433, acc: 0.90625, recall: 0.9545454545454546, precision: 0.875, f_beta: 0.9130434782608695\n",
      "train: step: 581, loss: 0.24541139602661133, acc: 0.921875, recall: 0.9104477611940298, precision: 0.9384615384615385, f_beta: 0.9242424242424243\n",
      "train: step: 582, loss: 0.3014903962612152, acc: 0.890625, recall: 0.9402985074626866, precision: 0.863013698630137, f_beta: 0.9\n",
      "train: step: 583, loss: 0.302773654460907, acc: 0.875, recall: 0.9402985074626866, precision: 0.84, f_beta: 0.8873239436619718\n",
      "train: step: 584, loss: 0.23517566919326782, acc: 0.9140625, recall: 0.9206349206349206, precision: 0.90625, f_beta: 0.9133858267716536\n",
      "train: step: 585, loss: 0.30578675866127014, acc: 0.8828125, recall: 0.9154929577464789, precision: 0.8783783783783784, f_beta: 0.896551724137931\n",
      "train: step: 586, loss: 0.2959398627281189, acc: 0.90625, recall: 0.9459459459459459, precision: 0.8974358974358975, f_beta: 0.9210526315789475\n",
      "train: step: 587, loss: 0.3124714195728302, acc: 0.8671875, recall: 0.9322033898305084, precision: 0.8088235294117647, f_beta: 0.8661417322834646\n",
      "train: step: 588, loss: 0.3549540638923645, acc: 0.875, recall: 0.835820895522388, precision: 0.9180327868852459, f_beta: 0.875\n",
      "train: step: 589, loss: 0.22287192940711975, acc: 0.921875, recall: 0.859375, precision: 0.9821428571428571, f_beta: 0.9166666666666665\n",
      "train: step: 590, loss: 0.292235404253006, acc: 0.9140625, recall: 0.8918918918918919, precision: 0.9565217391304348, f_beta: 0.9230769230769231\n",
      "train: step: 591, loss: 0.274641752243042, acc: 0.90625, recall: 0.8709677419354839, precision: 0.9310344827586207, f_beta: 0.9\n",
      "train: step: 592, loss: 0.34253713488578796, acc: 0.8671875, recall: 0.8947368421052632, precision: 0.8831168831168831, f_beta: 0.8888888888888888\n",
      "train: step: 593, loss: 0.23674660921096802, acc: 0.921875, recall: 0.9402985074626866, precision: 0.9130434782608695, f_beta: 0.9264705882352942\n",
      "train: step: 594, loss: 0.2563118040561676, acc: 0.9140625, recall: 0.921875, precision: 0.9076923076923077, f_beta: 0.9147286821705427\n",
      "train: step: 595, loss: 0.2701955735683441, acc: 0.9140625, recall: 0.9166666666666666, precision: 0.9016393442622951, f_beta: 0.9090909090909091\n",
      "train: step: 596, loss: 0.22863173484802246, acc: 0.9140625, recall: 0.9661016949152542, precision: 0.8636363636363636, f_beta: 0.912\n",
      "train: step: 597, loss: 0.2676772475242615, acc: 0.890625, recall: 0.9523809523809523, precision: 0.8450704225352113, f_beta: 0.8955223880597014\n",
      "train: step: 598, loss: 0.2683919370174408, acc: 0.8828125, recall: 0.9833333333333333, precision: 0.8082191780821918, f_beta: 0.8872180451127819\n",
      "train: step: 599, loss: 0.2551778256893158, acc: 0.9140625, recall: 0.9344262295081968, precision: 0.890625, f_beta: 0.9120000000000001\n",
      "train: step: 600, loss: 0.31416937708854675, acc: 0.8515625, recall: 0.9705882352941176, precision: 0.7951807228915663, f_beta: 0.8741721854304635\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:46:00.614885, step: 600, loss: 0.35852961509655684, acc: 0.8517628205128205,precision: 0.8291911707584444, recall: 0.8714253597128937, f_beta: 0.8487596887344188\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-600\n",
      "\n",
      "train: step: 601, loss: 0.2771036922931671, acc: 0.90625, recall: 0.9523809523809523, precision: 0.8695652173913043, f_beta: 0.909090909090909\n",
      "train: step: 602, loss: 0.2842259109020233, acc: 0.875, recall: 0.7936507936507936, precision: 0.9433962264150944, f_beta: 0.8620689655172413\n",
      "train: step: 603, loss: 0.3056241571903229, acc: 0.8671875, recall: 0.75, precision: 0.9574468085106383, f_beta: 0.8411214953271028\n",
      "train: step: 604, loss: 0.31241095066070557, acc: 0.875, recall: 0.7796610169491526, precision: 0.9387755102040817, f_beta: 0.8518518518518519\n",
      "train: step: 605, loss: 0.24702753126621246, acc: 0.8984375, recall: 0.8636363636363636, precision: 0.9344262295081968, f_beta: 0.8976377952755905\n",
      "train: step: 606, loss: 0.3247241973876953, acc: 0.875, recall: 0.8833333333333333, precision: 0.8548387096774194, f_beta: 0.8688524590163934\n",
      "train: step: 607, loss: 0.26426535844802856, acc: 0.890625, recall: 0.8939393939393939, precision: 0.8939393939393939, f_beta: 0.8939393939393939\n",
      "train: step: 608, loss: 0.28016674518585205, acc: 0.875, recall: 0.9384615384615385, precision: 0.8356164383561644, f_beta: 0.8840579710144928\n",
      "train: step: 609, loss: 0.2826544940471649, acc: 0.9140625, recall: 0.8852459016393442, precision: 0.9310344827586207, f_beta: 0.9075630252100839\n",
      "train: step: 610, loss: 0.3195757269859314, acc: 0.859375, recall: 0.8305084745762712, precision: 0.8596491228070176, f_beta: 0.8448275862068966\n",
      "train: step: 611, loss: 0.266834557056427, acc: 0.890625, recall: 0.8676470588235294, precision: 0.921875, f_beta: 0.893939393939394\n",
      "train: step: 612, loss: 0.23512667417526245, acc: 0.9453125, recall: 0.9242424242424242, precision: 0.9682539682539683, f_beta: 0.9457364341085271\n",
      "train: step: 613, loss: 0.32917511463165283, acc: 0.8671875, recall: 0.875, precision: 0.8615384615384616, f_beta: 0.8682170542635659\n",
      "train: step: 614, loss: 0.22233468294143677, acc: 0.9375, recall: 0.9193548387096774, precision: 0.95, f_beta: 0.9344262295081968\n",
      "train: step: 615, loss: 0.3671266436576843, acc: 0.875, recall: 0.8805970149253731, precision: 0.8805970149253731, f_beta: 0.8805970149253731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 616, loss: 0.2743605077266693, acc: 0.9140625, recall: 0.9285714285714286, precision: 0.8813559322033898, f_beta: 0.9043478260869564\n",
      "train: step: 617, loss: 0.23029395937919617, acc: 0.9140625, recall: 0.8805970149253731, precision: 0.9516129032258065, f_beta: 0.9147286821705426\n",
      "train: step: 618, loss: 0.24312880635261536, acc: 0.9140625, recall: 0.8571428571428571, precision: 0.9642857142857143, f_beta: 0.9075630252100839\n",
      "train: step: 619, loss: 0.3019275963306427, acc: 0.875, recall: 0.8636363636363636, precision: 0.890625, f_beta: 0.8769230769230768\n",
      "train: step: 620, loss: 0.3110373914241791, acc: 0.8984375, recall: 0.8412698412698413, precision: 0.9464285714285714, f_beta: 0.8907563025210083\n",
      "train: step: 621, loss: 0.38305458426475525, acc: 0.8515625, recall: 0.7936507936507936, precision: 0.8928571428571429, f_beta: 0.8403361344537815\n",
      "train: step: 622, loss: 0.30732858180999756, acc: 0.890625, recall: 0.8823529411764706, precision: 0.9090909090909091, f_beta: 0.8955223880597014\n",
      "train: step: 623, loss: 0.20550556480884552, acc: 0.9140625, recall: 0.8596491228070176, precision: 0.9423076923076923, f_beta: 0.8990825688073394\n",
      "train: step: 624, loss: 0.2554853558540344, acc: 0.890625, recall: 0.8235294117647058, precision: 0.8936170212765957, f_beta: 0.8571428571428571\n",
      "start training model\n",
      "train: step: 625, loss: 0.18664194643497467, acc: 0.9140625, recall: 0.9215686274509803, precision: 0.8703703703703703, f_beta: 0.8952380952380952\n",
      "train: step: 626, loss: 0.18476063013076782, acc: 0.9296875, recall: 0.9166666666666666, precision: 0.9565217391304348, f_beta: 0.9361702127659574\n",
      "train: step: 627, loss: 0.18407182395458221, acc: 0.9375, recall: 1.0, precision: 0.9, f_beta: 0.9473684210526316\n",
      "train: step: 628, loss: 0.30283570289611816, acc: 0.875, recall: 0.96, precision: 0.7741935483870968, f_beta: 0.8571428571428571\n",
      "train: step: 629, loss: 0.24101467430591583, acc: 0.9140625, recall: 0.9864864864864865, precision: 0.8795180722891566, f_beta: 0.9299363057324841\n",
      "train: step: 630, loss: 0.21629685163497925, acc: 0.9140625, recall: 0.9571428571428572, precision: 0.8933333333333333, f_beta: 0.9241379310344828\n",
      "train: step: 631, loss: 0.2870791256427765, acc: 0.890625, recall: 0.9393939393939394, precision: 0.8611111111111112, f_beta: 0.8985507246376813\n",
      "train: step: 632, loss: 0.13694116473197937, acc: 0.953125, recall: 0.9855072463768116, precision: 0.9315068493150684, f_beta: 0.9577464788732394\n",
      "train: step: 633, loss: 0.31993529200553894, acc: 0.890625, recall: 0.864406779661017, precision: 0.8947368421052632, f_beta: 0.8793103448275862\n",
      "train: step: 634, loss: 0.21260574460029602, acc: 0.9375, recall: 0.890625, precision: 0.9827586206896551, f_beta: 0.9344262295081966\n",
      "train: step: 635, loss: 0.3161279559135437, acc: 0.890625, recall: 0.7818181818181819, precision: 0.9555555555555556, f_beta: 0.86\n",
      "train: step: 636, loss: 0.26694077253341675, acc: 0.890625, recall: 0.835820895522388, precision: 0.9491525423728814, f_beta: 0.888888888888889\n",
      "train: step: 637, loss: 0.2141052633523941, acc: 0.9296875, recall: 0.8653846153846154, precision: 0.9574468085106383, f_beta: 0.9090909090909091\n",
      "train: step: 638, loss: 0.28765058517456055, acc: 0.8984375, recall: 0.8235294117647058, precision: 0.9824561403508771, f_beta: 0.896\n",
      "train: step: 639, loss: 0.262905478477478, acc: 0.890625, recall: 0.8620689655172413, precision: 0.8928571428571429, f_beta: 0.8771929824561403\n",
      "train: step: 640, loss: 0.20203089714050293, acc: 0.921875, recall: 0.9464285714285714, precision: 0.8833333333333333, f_beta: 0.9137931034482758\n",
      "train: step: 641, loss: 0.31972837448120117, acc: 0.890625, recall: 0.9384615384615385, precision: 0.8591549295774648, f_beta: 0.8970588235294118\n",
      "train: step: 642, loss: 0.13594457507133484, acc: 0.953125, recall: 0.9863013698630136, precision: 0.935064935064935, f_beta: 0.9600000000000001\n",
      "train: step: 643, loss: 0.2571997344493866, acc: 0.921875, recall: 0.9622641509433962, precision: 0.864406779661017, f_beta: 0.9107142857142857\n",
      "train: step: 644, loss: 0.2318163514137268, acc: 0.9140625, recall: 0.9655172413793104, precision: 0.8615384615384616, f_beta: 0.9105691056910569\n",
      "train: step: 645, loss: 0.25180909037590027, acc: 0.9296875, recall: 0.8596491228070176, precision: 0.98, f_beta: 0.9158878504672898\n",
      "train: step: 646, loss: 0.3583632707595825, acc: 0.890625, recall: 0.8309859154929577, precision: 0.9672131147540983, f_beta: 0.8939393939393939\n",
      "train: step: 647, loss: 0.15686142444610596, acc: 0.953125, recall: 0.9180327868852459, precision: 0.9824561403508771, f_beta: 0.9491525423728813\n",
      "train: step: 648, loss: 0.23049435019493103, acc: 0.921875, recall: 0.8888888888888888, precision: 0.9491525423728814, f_beta: 0.9180327868852458\n",
      "train: step: 649, loss: 0.2057921588420868, acc: 0.9296875, recall: 0.9154929577464789, precision: 0.9558823529411765, f_beta: 0.9352517985611511\n",
      "train: step: 650, loss: 0.1966850310564041, acc: 0.9375, recall: 0.9253731343283582, precision: 0.9538461538461539, f_beta: 0.9393939393939394\n",
      "train: step: 651, loss: 0.30552923679351807, acc: 0.8828125, recall: 0.8615384615384616, precision: 0.9032258064516129, f_beta: 0.8818897637795274\n",
      "train: step: 652, loss: 0.22414901852607727, acc: 0.9140625, recall: 0.935064935064935, precision: 0.9230769230769231, f_beta: 0.9290322580645162\n",
      "train: step: 653, loss: 0.15362238883972168, acc: 0.953125, recall: 0.953125, precision: 0.953125, f_beta: 0.953125\n",
      "train: step: 654, loss: 0.27126234769821167, acc: 0.9140625, recall: 0.922077922077922, precision: 0.9342105263157895, f_beta: 0.9281045751633986\n",
      "train: step: 655, loss: 0.2123013436794281, acc: 0.890625, recall: 0.9090909090909091, precision: 0.8823529411764706, f_beta: 0.8955223880597014\n",
      "train: step: 656, loss: 0.24073244631290436, acc: 0.8984375, recall: 0.9253731343283582, precision: 0.8857142857142857, f_beta: 0.9051094890510949\n",
      "train: step: 657, loss: 0.18818002939224243, acc: 0.9375, recall: 0.967741935483871, precision: 0.9090909090909091, f_beta: 0.9374999999999999\n",
      "train: step: 658, loss: 0.23470720648765564, acc: 0.9375, recall: 0.9365079365079365, precision: 0.9365079365079365, f_beta: 0.9365079365079365\n",
      "train: step: 659, loss: 0.18533103168010712, acc: 0.921875, recall: 0.8923076923076924, precision: 0.9508196721311475, f_beta: 0.9206349206349206\n",
      "train: step: 660, loss: 0.2578323185443878, acc: 0.921875, recall: 0.8987341772151899, precision: 0.9726027397260274, f_beta: 0.9342105263157895\n",
      "train: step: 661, loss: 0.26379406452178955, acc: 0.90625, recall: 0.8450704225352113, precision: 0.9836065573770492, f_beta: 0.9090909090909091\n",
      "train: step: 662, loss: 0.21872705221176147, acc: 0.9140625, recall: 0.8767123287671232, precision: 0.9696969696969697, f_beta: 0.920863309352518\n",
      "train: step: 663, loss: 0.30782294273376465, acc: 0.890625, recall: 0.8857142857142857, precision: 0.9117647058823529, f_beta: 0.8985507246376812\n",
      "train: step: 664, loss: 0.2601248323917389, acc: 0.8828125, recall: 0.819672131147541, precision: 0.9259259259259259, f_beta: 0.8695652173913043\n",
      "train: step: 665, loss: 0.28456825017929077, acc: 0.875, recall: 0.8253968253968254, precision: 0.9122807017543859, f_beta: 0.8666666666666667\n",
      "train: step: 666, loss: 0.16208148002624512, acc: 0.9609375, recall: 0.9393939393939394, precision: 0.9841269841269841, f_beta: 0.9612403100775193\n",
      "train: step: 667, loss: 0.23179951310157776, acc: 0.90625, recall: 0.90625, precision: 0.90625, f_beta: 0.90625\n",
      "train: step: 668, loss: 0.2652646005153656, acc: 0.90625, recall: 0.8983050847457628, precision: 0.8983050847457628, f_beta: 0.8983050847457628\n",
      "train: step: 669, loss: 0.25978752970695496, acc: 0.90625, recall: 0.9242424242424242, precision: 0.8970588235294118, f_beta: 0.9104477611940298\n",
      "train: step: 670, loss: 0.16399697959423065, acc: 0.921875, recall: 0.9491525423728814, precision: 0.8888888888888888, f_beta: 0.9180327868852458\n",
      "train: step: 671, loss: 0.18692980706691742, acc: 0.9296875, recall: 0.8852459016393442, precision: 0.9642857142857143, f_beta: 0.923076923076923\n",
      "train: step: 672, loss: 0.20816883444786072, acc: 0.921875, recall: 0.8939393939393939, precision: 0.9516129032258065, f_beta: 0.921875\n",
      "train: step: 673, loss: 0.26016199588775635, acc: 0.8984375, recall: 0.875, precision: 0.8909090909090909, f_beta: 0.8828828828828829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 674, loss: 0.22256219387054443, acc: 0.9296875, recall: 0.9, precision: 0.9473684210526315, f_beta: 0.9230769230769231\n",
      "train: step: 675, loss: 0.16832450032234192, acc: 0.9296875, recall: 0.9466666666666667, precision: 0.9342105263157895, f_beta: 0.9403973509933775\n",
      "train: step: 676, loss: 0.2372378408908844, acc: 0.9140625, recall: 0.8852459016393442, precision: 0.9310344827586207, f_beta: 0.9075630252100839\n",
      "train: step: 677, loss: 0.20497533679008484, acc: 0.921875, recall: 0.9365079365079365, precision: 0.9076923076923077, f_beta: 0.9218749999999999\n",
      "train: step: 678, loss: 0.27666422724723816, acc: 0.9140625, recall: 0.9375, precision: 0.8955223880597015, f_beta: 0.9160305343511451\n",
      "train: step: 679, loss: 0.17724472284317017, acc: 0.9375, recall: 0.90625, precision: 0.9666666666666667, f_beta: 0.9354838709677419\n",
      "train: step: 680, loss: 0.20207223296165466, acc: 0.9453125, recall: 0.9402985074626866, precision: 0.9545454545454546, f_beta: 0.9473684210526316\n",
      "train: step: 681, loss: 0.20721830427646637, acc: 0.9375, recall: 0.9672131147540983, precision: 0.9076923076923077, f_beta: 0.9365079365079365\n",
      "train: step: 682, loss: 0.19360528886318207, acc: 0.9453125, recall: 0.9230769230769231, precision: 0.967741935483871, f_beta: 0.9448818897637796\n",
      "train: step: 683, loss: 0.21432799100875854, acc: 0.9140625, recall: 0.9206349206349206, precision: 0.90625, f_beta: 0.9133858267716536\n",
      "train: step: 684, loss: 0.12678466737270355, acc: 0.9453125, recall: 0.9137931034482759, precision: 0.9636363636363636, f_beta: 0.9380530973451328\n",
      "train: step: 685, loss: 0.1285126507282257, acc: 0.953125, recall: 0.9333333333333333, precision: 0.9655172413793104, f_beta: 0.9491525423728815\n",
      "train: step: 686, loss: 0.2632775902748108, acc: 0.8984375, recall: 0.9076923076923077, precision: 0.8939393939393939, f_beta: 0.900763358778626\n",
      "train: step: 687, loss: 0.2602050304412842, acc: 0.921875, recall: 0.953125, precision: 0.8970588235294118, f_beta: 0.9242424242424244\n",
      "train: step: 688, loss: 0.22760459780693054, acc: 0.9296875, recall: 1.0, precision: 0.8676470588235294, f_beta: 0.9291338582677166\n",
      "train: step: 689, loss: 0.2777695953845978, acc: 0.859375, recall: 0.9508196721311475, precision: 0.7945205479452054, f_beta: 0.8656716417910448\n",
      "train: step: 690, loss: 0.19740428030490875, acc: 0.9296875, recall: 0.9180327868852459, precision: 0.9333333333333333, f_beta: 0.9256198347107439\n",
      "train: step: 691, loss: 0.277351438999176, acc: 0.8828125, recall: 0.875, precision: 0.8888888888888888, f_beta: 0.8818897637795274\n",
      "train: step: 692, loss: 0.1680973768234253, acc: 0.953125, recall: 0.9402985074626866, precision: 0.9692307692307692, f_beta: 0.9545454545454547\n",
      "train: step: 693, loss: 0.23550675809383392, acc: 0.90625, recall: 0.8888888888888888, precision: 0.8888888888888888, f_beta: 0.8888888888888888\n",
      "train: step: 694, loss: 0.29921963810920715, acc: 0.8515625, recall: 0.7681159420289855, precision: 0.9464285714285714, f_beta: 0.848\n",
      "train: step: 695, loss: 0.15507426857948303, acc: 0.953125, recall: 0.9154929577464789, precision: 1.0, f_beta: 0.9558823529411764\n",
      "train: step: 696, loss: 0.27189087867736816, acc: 0.9140625, recall: 0.8852459016393442, precision: 0.9310344827586207, f_beta: 0.9075630252100839\n",
      "train: step: 697, loss: 0.21181263029575348, acc: 0.9296875, recall: 0.9516129032258065, precision: 0.9076923076923077, f_beta: 0.9291338582677167\n",
      "train: step: 698, loss: 0.24571070075035095, acc: 0.90625, recall: 0.9661016949152542, precision: 0.8507462686567164, f_beta: 0.9047619047619047\n",
      "train: step: 699, loss: 0.31686505675315857, acc: 0.90625, recall: 0.9402985074626866, precision: 0.8873239436619719, f_beta: 0.9130434782608696\n",
      "train: step: 700, loss: 0.16200032830238342, acc: 0.9296875, recall: 0.9615384615384616, precision: 0.9259259259259259, f_beta: 0.9433962264150944\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:47:20.654733, step: 700, loss: 0.37720146698829454, acc: 0.8561698717948718,precision: 0.904384864998263, recall: 0.8265890548057885, f_beta: 0.8631566164148148\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-700\n",
      "\n",
      "train: step: 701, loss: 0.3421584367752075, acc: 0.8828125, recall: 0.90625, precision: 0.8656716417910447, f_beta: 0.8854961832061069\n",
      "train: step: 702, loss: 0.15880514681339264, acc: 0.9453125, recall: 0.9838709677419355, precision: 0.9104477611940298, f_beta: 0.9457364341085271\n",
      "train: step: 703, loss: 0.18810057640075684, acc: 0.953125, recall: 0.9642857142857143, precision: 0.9310344827586207, f_beta: 0.9473684210526316\n",
      "train: step: 704, loss: 0.27365899085998535, acc: 0.90625, recall: 0.8852459016393442, precision: 0.9152542372881356, f_beta: 0.9\n",
      "train: step: 705, loss: 0.24616068601608276, acc: 0.921875, recall: 0.8985507246376812, precision: 0.9538461538461539, f_beta: 0.9253731343283582\n",
      "train: step: 706, loss: 0.22222229838371277, acc: 0.90625, recall: 0.8363636363636363, precision: 0.9387755102040817, f_beta: 0.8846153846153846\n",
      "train: step: 707, loss: 0.22492018342018127, acc: 0.9140625, recall: 0.8548387096774194, precision: 0.9636363636363636, f_beta: 0.905982905982906\n",
      "train: step: 708, loss: 0.18931066989898682, acc: 0.921875, recall: 0.9122807017543859, precision: 0.9122807017543859, f_beta: 0.9122807017543859\n",
      "train: step: 709, loss: 0.22314617037773132, acc: 0.9140625, recall: 0.8857142857142857, precision: 0.9538461538461539, f_beta: 0.9185185185185185\n",
      "train: step: 710, loss: 0.2978346347808838, acc: 0.8984375, recall: 0.8461538461538461, precision: 0.9482758620689655, f_beta: 0.894308943089431\n",
      "train: step: 711, loss: 0.11779605597257614, acc: 0.9453125, recall: 0.8928571428571429, precision: 0.9803921568627451, f_beta: 0.9345794392523364\n",
      "train: step: 712, loss: 0.22063395380973816, acc: 0.921875, recall: 0.8833333333333333, precision: 0.9464285714285714, f_beta: 0.9137931034482758\n",
      "train: step: 713, loss: 0.22588151693344116, acc: 0.9140625, recall: 0.8928571428571429, precision: 0.9090909090909091, f_beta: 0.9009009009009009\n",
      "train: step: 714, loss: 0.20181956887245178, acc: 0.9296875, recall: 0.9305555555555556, precision: 0.9436619718309859, f_beta: 0.9370629370629372\n",
      "train: step: 715, loss: 0.27982044219970703, acc: 0.8984375, recall: 0.8360655737704918, precision: 0.9444444444444444, f_beta: 0.8869565217391304\n",
      "train: step: 716, loss: 0.15673011541366577, acc: 0.9453125, recall: 0.9538461538461539, precision: 0.9393939393939394, f_beta: 0.9465648854961831\n",
      "train: step: 717, loss: 0.279058575630188, acc: 0.8984375, recall: 0.8857142857142857, precision: 0.9253731343283582, f_beta: 0.9051094890510949\n",
      "train: step: 718, loss: 0.23761756718158722, acc: 0.9375, recall: 0.9193548387096774, precision: 0.95, f_beta: 0.9344262295081968\n",
      "train: step: 719, loss: 0.25746822357177734, acc: 0.921875, recall: 0.8870967741935484, precision: 0.9482758620689655, f_beta: 0.9166666666666667\n",
      "train: step: 720, loss: 0.25286588072776794, acc: 0.9140625, recall: 0.9090909090909091, precision: 0.9230769230769231, f_beta: 0.9160305343511451\n",
      "train: step: 721, loss: 0.18080668151378632, acc: 0.90625, recall: 0.9154929577464789, precision: 0.9154929577464789, f_beta: 0.9154929577464789\n",
      "train: step: 722, loss: 0.2727014720439911, acc: 0.8984375, recall: 0.8983050847457628, precision: 0.8833333333333333, f_beta: 0.8907563025210085\n",
      "train: step: 723, loss: 0.2360113263130188, acc: 0.9140625, recall: 0.9310344827586207, precision: 0.8852459016393442, f_beta: 0.9075630252100839\n",
      "train: step: 724, loss: 0.25078293681144714, acc: 0.9140625, recall: 0.9402985074626866, precision: 0.9, f_beta: 0.9197080291970803\n",
      "train: step: 725, loss: 0.16874074935913086, acc: 0.9375, recall: 0.9701492537313433, precision: 0.9154929577464789, f_beta: 0.9420289855072463\n",
      "train: step: 726, loss: 0.1928849071264267, acc: 0.9453125, recall: 0.9591836734693877, precision: 0.9038461538461539, f_beta: 0.9306930693069307\n",
      "train: step: 727, loss: 0.26718831062316895, acc: 0.8984375, recall: 1.0, precision: 0.8289473684210527, f_beta: 0.9064748201438849\n",
      "train: step: 728, loss: 0.2207537293434143, acc: 0.90625, recall: 0.9344262295081968, precision: 0.8769230769230769, f_beta: 0.9047619047619049\n",
      "train: step: 729, loss: 0.3061522841453552, acc: 0.8671875, recall: 0.8153846153846154, precision: 0.9137931034482759, f_beta: 0.8617886178861788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 730, loss: 0.22765135765075684, acc: 0.9296875, recall: 0.967741935483871, precision: 0.8955223880597015, f_beta: 0.930232558139535\n",
      "train: step: 731, loss: 0.23801571130752563, acc: 0.8984375, recall: 0.8666666666666667, precision: 0.9558823529411765, f_beta: 0.9090909090909091\n",
      "train: step: 732, loss: 0.19376735389232635, acc: 0.9453125, recall: 0.9285714285714286, precision: 0.9701492537313433, f_beta: 0.948905109489051\n",
      "train: step: 733, loss: 0.20129142701625824, acc: 0.921875, recall: 0.890625, precision: 0.95, f_beta: 0.9193548387096774\n",
      "train: step: 734, loss: 0.23271316289901733, acc: 0.90625, recall: 0.84375, precision: 0.9642857142857143, f_beta: 0.8999999999999999\n",
      "train: step: 735, loss: 0.16548700630664825, acc: 0.9140625, recall: 0.8833333333333333, precision: 0.9298245614035088, f_beta: 0.905982905982906\n",
      "train: step: 736, loss: 0.20414678752422333, acc: 0.921875, recall: 0.953125, precision: 0.8970588235294118, f_beta: 0.9242424242424244\n",
      "train: step: 737, loss: 0.23575349152088165, acc: 0.921875, recall: 0.90625, precision: 0.9354838709677419, f_beta: 0.9206349206349206\n",
      "train: step: 738, loss: 0.34242555499076843, acc: 0.890625, recall: 0.9230769230769231, precision: 0.8695652173913043, f_beta: 0.8955223880597014\n",
      "train: step: 739, loss: 0.29237815737724304, acc: 0.921875, recall: 0.984375, precision: 0.875, f_beta: 0.9264705882352942\n",
      "train: step: 740, loss: 0.23166650533676147, acc: 0.921875, recall: 0.9852941176470589, precision: 0.881578947368421, f_beta: 0.9305555555555556\n",
      "train: step: 741, loss: 0.22556579113006592, acc: 0.9375, recall: 0.9253731343283582, precision: 0.9538461538461539, f_beta: 0.9393939393939394\n",
      "train: step: 742, loss: 0.1548658013343811, acc: 0.953125, recall: 0.9642857142857143, precision: 0.9310344827586207, f_beta: 0.9473684210526316\n",
      "train: step: 743, loss: 0.1605745106935501, acc: 0.953125, recall: 0.9583333333333334, precision: 0.9583333333333334, f_beta: 0.9583333333333334\n",
      "train: step: 744, loss: 0.17842765152454376, acc: 0.9375, recall: 0.921875, precision: 0.9516129032258065, f_beta: 0.9365079365079365\n",
      "train: step: 745, loss: 0.2237977385520935, acc: 0.9140625, recall: 0.9230769230769231, precision: 0.9090909090909091, f_beta: 0.9160305343511451\n",
      "train: step: 746, loss: 0.20507009327411652, acc: 0.9140625, recall: 0.8947368421052632, precision: 0.9107142857142857, f_beta: 0.9026548672566371\n",
      "train: step: 747, loss: 0.18086405098438263, acc: 0.9296875, recall: 0.8947368421052632, precision: 0.9444444444444444, f_beta: 0.918918918918919\n",
      "train: step: 748, loss: 0.3038550615310669, acc: 0.8828125, recall: 0.7894736842105263, precision: 0.9375, f_beta: 0.8571428571428572\n",
      "train: step: 749, loss: 0.22918596863746643, acc: 0.90625, recall: 0.819672131147541, precision: 0.9803921568627451, f_beta: 0.8928571428571428\n",
      "train: step: 750, loss: 0.20789973437786102, acc: 0.9140625, recall: 0.864406779661017, precision: 0.9444444444444444, f_beta: 0.9026548672566371\n",
      "train: step: 751, loss: 0.20846109092235565, acc: 0.921875, recall: 0.9104477611940298, precision: 0.9384615384615385, f_beta: 0.9242424242424243\n",
      "train: step: 752, loss: 0.24741655588150024, acc: 0.9296875, recall: 0.8970588235294118, precision: 0.9682539682539683, f_beta: 0.9312977099236641\n",
      "train: step: 753, loss: 0.27325642108917236, acc: 0.890625, recall: 0.8615384615384616, precision: 0.9180327868852459, f_beta: 0.8888888888888888\n",
      "train: step: 754, loss: 0.34357452392578125, acc: 0.859375, recall: 0.9152542372881356, precision: 0.8059701492537313, f_beta: 0.8571428571428572\n",
      "train: step: 755, loss: 0.2949223220348358, acc: 0.890625, recall: 0.9830508474576272, precision: 0.8169014084507042, f_beta: 0.8923076923076924\n",
      "train: step: 756, loss: 0.2804470658302307, acc: 0.90625, recall: 0.9852941176470589, precision: 0.8589743589743589, f_beta: 0.9178082191780821\n",
      "train: step: 757, loss: 0.12845051288604736, acc: 0.96875, recall: 0.9714285714285714, precision: 0.9714285714285714, f_beta: 0.9714285714285714\n",
      "train: step: 758, loss: 0.19046278297901154, acc: 0.9375, recall: 0.9583333333333334, precision: 0.9324324324324325, f_beta: 0.9452054794520548\n",
      "train: step: 759, loss: 0.19304490089416504, acc: 0.9375, recall: 0.971830985915493, precision: 0.92, f_beta: 0.9452054794520549\n",
      "train: step: 760, loss: 0.24278147518634796, acc: 0.90625, recall: 0.90625, precision: 0.90625, f_beta: 0.90625\n",
      "train: step: 761, loss: 0.2076914757490158, acc: 0.8828125, recall: 0.8656716417910447, precision: 0.90625, f_beta: 0.8854961832061069\n",
      "train: step: 762, loss: 0.19629205763339996, acc: 0.9296875, recall: 0.9047619047619048, precision: 0.95, f_beta: 0.9268292682926829\n",
      "train: step: 763, loss: 0.19643594324588776, acc: 0.9375, recall: 0.8904109589041096, precision: 1.0, f_beta: 0.9420289855072463\n",
      "train: step: 764, loss: 0.2612665295600891, acc: 0.90625, recall: 0.8636363636363636, precision: 0.95, f_beta: 0.9047619047619048\n",
      "train: step: 765, loss: 0.2297670543193817, acc: 0.921875, recall: 0.8958333333333334, precision: 0.8958333333333334, f_beta: 0.8958333333333334\n",
      "train: step: 766, loss: 0.32471519708633423, acc: 0.8984375, recall: 0.890625, precision: 0.9047619047619048, f_beta: 0.8976377952755906\n",
      "train: step: 767, loss: 0.14839112758636475, acc: 0.9609375, recall: 0.9655172413793104, precision: 0.9491525423728814, f_beta: 0.9572649572649573\n",
      "train: step: 768, loss: 0.21308961510658264, acc: 0.90625, recall: 0.9571428571428572, precision: 0.881578947368421, f_beta: 0.9178082191780822\n",
      "train: step: 769, loss: 0.18610930442810059, acc: 0.9296875, recall: 0.9230769230769231, precision: 0.9056603773584906, f_beta: 0.9142857142857143\n",
      "train: step: 770, loss: 0.23883718252182007, acc: 0.90625, recall: 0.9393939393939394, precision: 0.8857142857142857, f_beta: 0.9117647058823529\n",
      "train: step: 771, loss: 0.15451760590076447, acc: 0.953125, recall: 0.9206349206349206, precision: 0.9830508474576272, f_beta: 0.9508196721311476\n",
      "train: step: 772, loss: 0.21357282996177673, acc: 0.921875, recall: 0.9848484848484849, precision: 0.8783783783783784, f_beta: 0.9285714285714285\n",
      "train: step: 773, loss: 0.212688148021698, acc: 0.9296875, recall: 0.9491525423728814, precision: 0.9032258064516129, f_beta: 0.9256198347107438\n",
      "train: step: 774, loss: 0.22914022207260132, acc: 0.9140625, recall: 0.8840579710144928, precision: 0.953125, f_beta: 0.9172932330827068\n",
      "train: step: 775, loss: 0.15747609734535217, acc: 0.953125, recall: 0.953125, precision: 0.953125, f_beta: 0.953125\n",
      "train: step: 776, loss: 0.16495728492736816, acc: 0.9296875, recall: 0.9078947368421053, precision: 0.971830985915493, f_beta: 0.9387755102040817\n",
      "train: step: 777, loss: 0.12283240258693695, acc: 0.96875, recall: 0.9814814814814815, precision: 0.9464285714285714, f_beta: 0.9636363636363636\n",
      "train: step: 778, loss: 0.23042716085910797, acc: 0.9140625, recall: 0.8909090909090909, precision: 0.9074074074074074, f_beta: 0.8990825688073394\n",
      "train: step: 779, loss: 0.19926199316978455, acc: 0.9375, recall: 0.96875, precision: 0.9117647058823529, f_beta: 0.9393939393939394\n",
      "train: step: 780, loss: 0.30385854840278625, acc: 0.8828125, recall: 0.7846153846153846, precision: 0.9807692307692307, f_beta: 0.8717948717948717\n",
      "start training model\n",
      "train: step: 781, loss: 0.18685252964496613, acc: 0.9453125, recall: 0.9375, precision: 0.9523809523809523, f_beta: 0.9448818897637795\n",
      "train: step: 782, loss: 0.101862333714962, acc: 0.9765625, recall: 0.96875, precision: 0.9841269841269841, f_beta: 0.9763779527559054\n",
      "train: step: 783, loss: 0.11303244531154633, acc: 0.96875, recall: 0.9571428571428572, precision: 0.9852941176470589, f_beta: 0.9710144927536232\n",
      "train: step: 784, loss: 0.1419292688369751, acc: 0.96875, recall: 1.0, precision: 0.9393939393939394, f_beta: 0.96875\n",
      "train: step: 785, loss: 0.19211812317371368, acc: 0.9453125, recall: 0.9322033898305084, precision: 0.9482758620689655, f_beta: 0.94017094017094\n",
      "train: step: 786, loss: 0.2606406807899475, acc: 0.9140625, recall: 0.9253731343283582, precision: 0.9117647058823529, f_beta: 0.9185185185185185\n",
      "train: step: 787, loss: 0.10345271229743958, acc: 0.9765625, recall: 0.9558823529411765, precision: 1.0, f_beta: 0.9774436090225563\n",
      "train: step: 788, loss: 0.16711273789405823, acc: 0.953125, recall: 0.9552238805970149, precision: 0.9552238805970149, f_beta: 0.9552238805970149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 789, loss: 0.1317373812198639, acc: 0.953125, recall: 0.9850746268656716, precision: 0.9295774647887324, f_beta: 0.9565217391304348\n",
      "train: step: 790, loss: 0.18564210832118988, acc: 0.953125, recall: 0.9508196721311475, precision: 0.9508196721311475, f_beta: 0.9508196721311475\n",
      "train: step: 791, loss: 0.211307555437088, acc: 0.921875, recall: 0.9142857142857143, precision: 0.9411764705882353, f_beta: 0.9275362318840579\n",
      "train: step: 792, loss: 0.15456892549991608, acc: 0.9296875, recall: 0.9032258064516129, precision: 0.9491525423728814, f_beta: 0.9256198347107438\n",
      "train: step: 793, loss: 0.2428428679704666, acc: 0.9296875, recall: 0.9310344827586207, precision: 0.9152542372881356, f_beta: 0.923076923076923\n",
      "train: step: 794, loss: 0.1256030648946762, acc: 0.96875, recall: 0.95, precision: 0.9827586206896551, f_beta: 0.9661016949152542\n",
      "train: step: 795, loss: 0.15261617302894592, acc: 0.9296875, recall: 0.8974358974358975, precision: 0.9859154929577465, f_beta: 0.9395973154362417\n",
      "train: step: 796, loss: 0.1507035493850708, acc: 0.953125, recall: 0.9393939393939394, precision: 0.96875, f_beta: 0.9538461538461539\n",
      "train: step: 797, loss: 0.18101327121257782, acc: 0.9375, recall: 0.9054054054054054, precision: 0.9852941176470589, f_beta: 0.943661971830986\n",
      "train: step: 798, loss: 0.19788077473640442, acc: 0.9375, recall: 0.9137931034482759, precision: 0.9464285714285714, f_beta: 0.9298245614035087\n",
      "train: step: 799, loss: 0.11911585181951523, acc: 0.9609375, recall: 0.9402985074626866, precision: 0.984375, f_beta: 0.9618320610687023\n",
      "train: step: 800, loss: 0.10842646658420563, acc: 0.96875, recall: 0.9821428571428571, precision: 0.9482758620689655, f_beta: 0.9649122807017544\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:48:40.881036, step: 800, loss: 0.3921862271351692, acc: 0.8655849358974359,precision: 0.8970319128928389, recall: 0.8443960794267585, f_beta: 0.8690993267902095\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-800\n",
      "\n",
      "train: step: 801, loss: 0.16281342506408691, acc: 0.9453125, recall: 0.9402985074626866, precision: 0.9545454545454546, f_beta: 0.9473684210526316\n",
      "train: step: 802, loss: 0.203457772731781, acc: 0.9453125, recall: 0.9285714285714286, precision: 0.9701492537313433, f_beta: 0.948905109489051\n",
      "train: step: 803, loss: 0.12410402297973633, acc: 0.953125, recall: 0.9852941176470589, precision: 0.9305555555555556, f_beta: 0.9571428571428572\n",
      "train: step: 804, loss: 0.15656936168670654, acc: 0.953125, recall: 0.9846153846153847, precision: 0.927536231884058, f_beta: 0.9552238805970149\n",
      "train: step: 805, loss: 0.2218075394630432, acc: 0.921875, recall: 0.9682539682539683, precision: 0.8840579710144928, f_beta: 0.9242424242424243\n",
      "train: step: 806, loss: 0.14346051216125488, acc: 0.96875, recall: 0.9705882352941176, precision: 0.9705882352941176, f_beta: 0.9705882352941176\n",
      "train: step: 807, loss: 0.13973353803157806, acc: 0.9296875, recall: 0.9375, precision: 0.9230769230769231, f_beta: 0.9302325581395349\n",
      "train: step: 808, loss: 0.22534267604351044, acc: 0.9375, recall: 0.9016393442622951, precision: 0.9649122807017544, f_beta: 0.9322033898305084\n",
      "train: step: 809, loss: 0.0579853281378746, acc: 0.984375, recall: 0.9692307692307692, precision: 1.0, f_beta: 0.9843749999999999\n",
      "train: step: 810, loss: 0.12652753293514252, acc: 0.96875, recall: 0.9692307692307692, precision: 0.9692307692307692, f_beta: 0.9692307692307692\n",
      "train: step: 811, loss: 0.14452768862247467, acc: 0.953125, recall: 0.9310344827586207, precision: 0.9642857142857143, f_beta: 0.9473684210526316\n",
      "train: step: 812, loss: 0.21071164309978485, acc: 0.9296875, recall: 0.8775510204081632, precision: 0.9347826086956522, f_beta: 0.9052631578947369\n",
      "train: step: 813, loss: 0.1862431764602661, acc: 0.9296875, recall: 0.921875, precision: 0.9365079365079365, f_beta: 0.9291338582677166\n",
      "train: step: 814, loss: 0.20824383199214935, acc: 0.9296875, recall: 0.9180327868852459, precision: 0.9333333333333333, f_beta: 0.9256198347107439\n",
      "train: step: 815, loss: 0.22384849190711975, acc: 0.921875, recall: 0.927536231884058, precision: 0.927536231884058, f_beta: 0.927536231884058\n",
      "train: step: 816, loss: 0.15286101400852203, acc: 0.953125, recall: 0.9482758620689655, precision: 0.9482758620689655, f_beta: 0.9482758620689655\n",
      "train: step: 817, loss: 0.13101917505264282, acc: 0.9765625, recall: 0.9838709677419355, precision: 0.9682539682539683, f_beta: 0.976\n",
      "train: step: 818, loss: 0.14158311486244202, acc: 0.96875, recall: 0.9692307692307692, precision: 0.9692307692307692, f_beta: 0.9692307692307692\n",
      "train: step: 819, loss: 0.30582982301712036, acc: 0.875, recall: 0.9491525423728814, precision: 0.8115942028985508, f_beta: 0.8749999999999999\n",
      "train: step: 820, loss: 0.25214946269989014, acc: 0.9296875, recall: 0.9122807017543859, precision: 0.9285714285714286, f_beta: 0.9203539823008849\n",
      "train: step: 821, loss: 0.16566216945648193, acc: 0.953125, recall: 0.9384615384615385, precision: 0.9682539682539683, f_beta: 0.953125\n",
      "train: step: 822, loss: 0.2566629648208618, acc: 0.90625, recall: 0.8787878787878788, precision: 0.9354838709677419, f_beta: 0.90625\n",
      "train: step: 823, loss: 0.15705552697181702, acc: 0.9296875, recall: 0.890625, precision: 0.9661016949152542, f_beta: 0.9268292682926829\n",
      "train: step: 824, loss: 0.1765168011188507, acc: 0.9453125, recall: 0.8867924528301887, precision: 0.9791666666666666, f_beta: 0.9306930693069307\n",
      "train: step: 825, loss: 0.2628474831581116, acc: 0.9375, recall: 0.8923076923076924, precision: 0.9830508474576272, f_beta: 0.9354838709677421\n",
      "train: step: 826, loss: 0.22911056876182556, acc: 0.9140625, recall: 0.8571428571428571, precision: 0.9411764705882353, f_beta: 0.897196261682243\n",
      "train: step: 827, loss: 0.24989193677902222, acc: 0.921875, recall: 0.8615384615384616, precision: 0.9824561403508771, f_beta: 0.9180327868852458\n",
      "train: step: 828, loss: 0.13943319022655487, acc: 0.953125, recall: 0.90625, precision: 1.0, f_beta: 0.9508196721311475\n",
      "train: step: 829, loss: 0.14361144602298737, acc: 0.9609375, recall: 0.9661016949152542, precision: 0.95, f_beta: 0.957983193277311\n",
      "train: step: 830, loss: 0.13238075375556946, acc: 0.9609375, recall: 0.9423076923076923, precision: 0.9607843137254902, f_beta: 0.9514563106796117\n",
      "train: step: 831, loss: 0.23363429307937622, acc: 0.9375, recall: 0.95, precision: 0.9193548387096774, f_beta: 0.9344262295081968\n",
      "train: step: 832, loss: 0.2023105025291443, acc: 0.9375, recall: 0.9117647058823529, precision: 0.96875, f_beta: 0.9393939393939394\n",
      "train: step: 833, loss: 0.18497170507907867, acc: 0.953125, recall: 0.927536231884058, precision: 0.9846153846153847, f_beta: 0.9552238805970149\n",
      "train: step: 834, loss: 0.13487526774406433, acc: 0.96875, recall: 0.9696969696969697, precision: 0.9696969696969697, f_beta: 0.9696969696969697\n",
      "train: step: 835, loss: 0.10393775999546051, acc: 0.96875, recall: 0.9705882352941176, precision: 0.9705882352941176, f_beta: 0.9705882352941176\n",
      "train: step: 836, loss: 0.11998974531888962, acc: 0.96875, recall: 0.9629629629629629, precision: 0.9873417721518988, f_beta: 0.975\n",
      "train: step: 837, loss: 0.1612236499786377, acc: 0.9296875, recall: 0.9180327868852459, precision: 0.9333333333333333, f_beta: 0.9256198347107439\n",
      "train: step: 838, loss: 0.09886482357978821, acc: 0.96875, recall: 0.9736842105263158, precision: 0.9736842105263158, f_beta: 0.9736842105263158\n",
      "train: step: 839, loss: 0.15559248626232147, acc: 0.953125, recall: 0.9076923076923077, precision: 1.0, f_beta: 0.9516129032258065\n",
      "train: step: 840, loss: 0.2190377116203308, acc: 0.921875, recall: 0.9178082191780822, precision: 0.9436619718309859, f_beta: 0.9305555555555556\n",
      "train: step: 841, loss: 0.22907474637031555, acc: 0.921875, recall: 0.9464285714285714, precision: 0.8833333333333333, f_beta: 0.9137931034482758\n",
      "train: step: 842, loss: 0.10413273423910141, acc: 0.953125, recall: 0.9245283018867925, precision: 0.9607843137254902, f_beta: 0.9423076923076923\n",
      "train: step: 843, loss: 0.1865776926279068, acc: 0.9375, recall: 0.9857142857142858, precision: 0.9078947368421053, f_beta: 0.9452054794520548\n",
      "train: step: 844, loss: 0.08691367506980896, acc: 0.984375, recall: 0.9701492537313433, precision: 1.0, f_beta: 0.9848484848484849\n",
      "train: step: 845, loss: 0.16491271555423737, acc: 0.9609375, recall: 0.9508196721311475, precision: 0.9666666666666667, f_beta: 0.9586776859504132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 846, loss: 0.08886668086051941, acc: 0.96875, recall: 0.9838709677419355, precision: 0.953125, f_beta: 0.9682539682539683\n",
      "train: step: 847, loss: 0.14562401175498962, acc: 0.953125, recall: 0.9322033898305084, precision: 0.9649122807017544, f_beta: 0.9482758620689654\n",
      "train: step: 848, loss: 0.18217065930366516, acc: 0.9375, recall: 0.8970588235294118, precision: 0.9838709677419355, f_beta: 0.9384615384615386\n",
      "train: step: 849, loss: 0.1050020307302475, acc: 0.9765625, recall: 0.9692307692307692, precision: 0.984375, f_beta: 0.9767441860465116\n",
      "train: step: 850, loss: 0.20520807802677155, acc: 0.921875, recall: 0.8852459016393442, precision: 0.9473684210526315, f_beta: 0.9152542372881356\n",
      "train: step: 851, loss: 0.1601390838623047, acc: 0.9375, recall: 0.8939393939393939, precision: 0.9833333333333333, f_beta: 0.9365079365079364\n",
      "train: step: 852, loss: 0.16953109204769135, acc: 0.9296875, recall: 0.92, precision: 0.9583333333333334, f_beta: 0.9387755102040817\n",
      "train: step: 853, loss: 0.11387638002634048, acc: 0.96875, recall: 0.9838709677419355, precision: 0.953125, f_beta: 0.9682539682539683\n",
      "train: step: 854, loss: 0.09799562394618988, acc: 0.9765625, recall: 0.9848484848484849, precision: 0.9701492537313433, f_beta: 0.9774436090225564\n",
      "train: step: 855, loss: 0.1154031753540039, acc: 0.9765625, recall: 0.9722222222222222, precision: 0.9859154929577465, f_beta: 0.979020979020979\n",
      "train: step: 856, loss: 0.1259162276983261, acc: 0.9609375, recall: 0.9444444444444444, precision: 0.9855072463768116, f_beta: 0.9645390070921985\n",
      "train: step: 857, loss: 0.2827523350715637, acc: 0.921875, recall: 0.9090909090909091, precision: 0.9375, f_beta: 0.923076923076923\n",
      "train: step: 858, loss: 0.10432678461074829, acc: 0.984375, recall: 0.9866666666666667, precision: 0.9866666666666667, f_beta: 0.9866666666666668\n",
      "train: step: 859, loss: 0.10293661803007126, acc: 0.9609375, recall: 0.9701492537313433, precision: 0.9558823529411765, f_beta: 0.962962962962963\n",
      "train: step: 860, loss: 0.16143321990966797, acc: 0.9609375, recall: 0.9701492537313433, precision: 0.9558823529411765, f_beta: 0.962962962962963\n",
      "train: step: 861, loss: 0.1415766477584839, acc: 0.9375, recall: 0.9154929577464789, precision: 0.9701492537313433, f_beta: 0.9420289855072463\n",
      "train: step: 862, loss: 0.12109842896461487, acc: 0.9375, recall: 0.9491525423728814, precision: 0.9180327868852459, f_beta: 0.9333333333333333\n",
      "train: step: 863, loss: 0.17239999771118164, acc: 0.9296875, recall: 0.9032258064516129, precision: 0.9491525423728814, f_beta: 0.9256198347107438\n",
      "train: step: 864, loss: 0.1167188212275505, acc: 0.96875, recall: 0.9565217391304348, precision: 0.9850746268656716, f_beta: 0.9705882352941176\n",
      "train: step: 865, loss: 0.19019217789173126, acc: 0.9453125, recall: 0.9122807017543859, precision: 0.9629629629629629, f_beta: 0.9369369369369369\n",
      "train: step: 866, loss: 0.14849862456321716, acc: 0.953125, recall: 0.9649122807017544, precision: 0.9322033898305084, f_beta: 0.9482758620689654\n",
      "train: step: 867, loss: 0.20599102973937988, acc: 0.9375, recall: 0.95, precision: 0.9193548387096774, f_beta: 0.9344262295081968\n",
      "train: step: 868, loss: 0.13925866782665253, acc: 0.9609375, recall: 0.9344262295081968, precision: 0.9827586206896551, f_beta: 0.9579831932773109\n",
      "train: step: 869, loss: 0.1343304067850113, acc: 0.96875, recall: 0.9714285714285714, precision: 0.9714285714285714, f_beta: 0.9714285714285714\n",
      "train: step: 870, loss: 0.1602538824081421, acc: 0.9609375, recall: 0.9122807017543859, precision: 1.0, f_beta: 0.9541284403669724\n",
      "train: step: 871, loss: 0.15117260813713074, acc: 0.9453125, recall: 0.8888888888888888, precision: 1.0, f_beta: 0.9411764705882353\n",
      "train: step: 872, loss: 0.12898141145706177, acc: 0.9296875, recall: 0.9137931034482759, precision: 0.9298245614035088, f_beta: 0.9217391304347825\n",
      "train: step: 873, loss: 0.12211261689662933, acc: 0.96875, recall: 0.9615384615384616, precision: 0.9615384615384616, f_beta: 0.9615384615384616\n",
      "train: step: 874, loss: 0.12363415211439133, acc: 0.9609375, recall: 0.9577464788732394, precision: 0.9714285714285714, f_beta: 0.9645390070921985\n",
      "train: step: 875, loss: 0.2575099766254425, acc: 0.9140625, recall: 0.9428571428571428, precision: 0.9041095890410958, f_beta: 0.923076923076923\n",
      "train: step: 876, loss: 0.11578460037708282, acc: 0.96875, recall: 0.9852941176470589, precision: 0.9571428571428572, f_beta: 0.9710144927536232\n",
      "train: step: 877, loss: 0.19104617834091187, acc: 0.9609375, recall: 0.9864864864864865, precision: 0.948051948051948, f_beta: 0.9668874172185431\n",
      "train: step: 878, loss: 0.2444753646850586, acc: 0.890625, recall: 1.0, precision: 0.8108108108108109, f_beta: 0.8955223880597014\n",
      "train: step: 879, loss: 0.1710907518863678, acc: 0.9296875, recall: 0.9857142857142858, precision: 0.8961038961038961, f_beta: 0.9387755102040817\n",
      "train: step: 880, loss: 0.19733501970767975, acc: 0.9375, recall: 0.9818181818181818, precision: 0.8852459016393442, f_beta: 0.9310344827586207\n",
      "train: step: 881, loss: 0.07408569753170013, acc: 0.9765625, recall: 0.984375, precision: 0.9692307692307692, f_beta: 0.9767441860465116\n",
      "train: step: 882, loss: 0.2195042222738266, acc: 0.9453125, recall: 0.9056603773584906, precision: 0.96, f_beta: 0.9320388349514563\n",
      "train: step: 883, loss: 0.19346913695335388, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9696969696969697, f_beta: 0.955223880597015\n",
      "train: step: 884, loss: 0.18582211434841156, acc: 0.9375, recall: 0.921875, precision: 0.9516129032258065, f_beta: 0.9365079365079365\n",
      "train: step: 885, loss: 0.12581497430801392, acc: 0.96875, recall: 0.9436619718309859, precision: 1.0, f_beta: 0.9710144927536231\n",
      "train: step: 886, loss: 0.1696014106273651, acc: 0.9453125, recall: 0.8923076923076924, precision: 1.0, f_beta: 0.9430894308943091\n",
      "train: step: 887, loss: 0.17043353617191315, acc: 0.9609375, recall: 0.9295774647887324, precision: 1.0, f_beta: 0.9635036496350364\n",
      "train: step: 888, loss: 0.1685856580734253, acc: 0.953125, recall: 0.967741935483871, precision: 0.9375, f_beta: 0.9523809523809523\n",
      "train: step: 889, loss: 0.21393254399299622, acc: 0.921875, recall: 0.9230769230769231, precision: 0.9230769230769231, f_beta: 0.9230769230769231\n",
      "train: step: 890, loss: 0.21810394525527954, acc: 0.921875, recall: 0.9661016949152542, precision: 0.8769230769230769, f_beta: 0.9193548387096773\n",
      "train: step: 891, loss: 0.09366711974143982, acc: 0.984375, recall: 1.0, precision: 0.9672131147540983, f_beta: 0.9833333333333333\n",
      "train: step: 892, loss: 0.16562160849571228, acc: 0.9375, recall: 0.9850746268656716, precision: 0.9041095890410958, f_beta: 0.9428571428571428\n",
      "train: step: 893, loss: 0.12385490536689758, acc: 0.9609375, recall: 0.96, precision: 0.972972972972973, f_beta: 0.9664429530201343\n",
      "train: step: 894, loss: 0.15778478980064392, acc: 0.9453125, recall: 0.875, precision: 1.0, f_beta: 0.9333333333333333\n",
      "train: step: 895, loss: 0.08918686211109161, acc: 0.96875, recall: 0.953125, precision: 0.9838709677419355, f_beta: 0.9682539682539683\n",
      "train: step: 896, loss: 0.08078573644161224, acc: 0.9765625, recall: 0.9565217391304348, precision: 0.9777777777777777, f_beta: 0.967032967032967\n",
      "train: step: 897, loss: 0.20435309410095215, acc: 0.9296875, recall: 1.0, precision: 0.8571428571428571, f_beta: 0.923076923076923\n",
      "train: step: 898, loss: 0.28573524951934814, acc: 0.9296875, recall: 0.9615384615384616, precision: 0.9259259259259259, f_beta: 0.9433962264150944\n",
      "train: step: 899, loss: 0.16503387689590454, acc: 0.9296875, recall: 0.9384615384615385, precision: 0.9242424242424242, f_beta: 0.9312977099236641\n",
      "train: step: 900, loss: 0.19459567964076996, acc: 0.9296875, recall: 0.9027777777777778, precision: 0.9701492537313433, f_beta: 0.9352517985611511\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:50:00.669793, step: 900, loss: 0.4097688205731221, acc: 0.8511618589743589,precision: 0.9169801594486169, recall: 0.8132958564008991, f_beta: 0.8611815284507535\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-900\n",
      "\n",
      "train: step: 901, loss: 0.24524718523025513, acc: 0.8984375, recall: 0.9571428571428572, precision: 0.8701298701298701, f_beta: 0.9115646258503401\n",
      "train: step: 902, loss: 0.1635047048330307, acc: 0.9296875, recall: 0.9672131147540983, precision: 0.8939393939393939, f_beta: 0.9291338582677166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 903, loss: 0.15115010738372803, acc: 0.9453125, recall: 0.9558823529411765, precision: 0.9420289855072463, f_beta: 0.9489051094890512\n",
      "train: step: 904, loss: 0.18714408576488495, acc: 0.9453125, recall: 0.9122807017543859, precision: 0.9629629629629629, f_beta: 0.9369369369369369\n",
      "train: step: 905, loss: 0.20363594591617584, acc: 0.9296875, recall: 0.921875, precision: 0.9365079365079365, f_beta: 0.9291338582677166\n",
      "train: step: 906, loss: 0.16113299131393433, acc: 0.953125, recall: 0.9242424242424242, precision: 0.9838709677419355, f_beta: 0.9531249999999999\n",
      "train: step: 907, loss: 0.1555977761745453, acc: 0.9375, recall: 0.92, precision: 0.92, f_beta: 0.92\n",
      "train: step: 908, loss: 0.2067357301712036, acc: 0.9296875, recall: 0.9090909090909091, precision: 0.9259259259259259, f_beta: 0.9174311926605504\n",
      "train: step: 909, loss: 0.17674238979816437, acc: 0.9375, recall: 0.8970588235294118, precision: 0.9838709677419355, f_beta: 0.9384615384615386\n",
      "train: step: 910, loss: 0.10420919209718704, acc: 0.96875, recall: 0.9696969696969697, precision: 0.9696969696969697, f_beta: 0.9696969696969697\n",
      "train: step: 911, loss: 0.2034430354833603, acc: 0.9375, recall: 0.9705882352941176, precision: 0.9166666666666666, f_beta: 0.9428571428571428\n",
      "train: step: 912, loss: 0.17412486672401428, acc: 0.9609375, recall: 0.9848484848484849, precision: 0.9420289855072463, f_beta: 0.962962962962963\n",
      "train: step: 913, loss: 0.17184759676456451, acc: 0.9453125, recall: 0.9365079365079365, precision: 0.9516129032258065, f_beta: 0.944\n",
      "train: step: 914, loss: 0.12168676406145096, acc: 0.9453125, recall: 0.9516129032258065, precision: 0.9365079365079365, f_beta: 0.944\n",
      "train: step: 915, loss: 0.1038624495267868, acc: 0.9609375, recall: 0.967741935483871, precision: 0.9523809523809523, f_beta: 0.96\n",
      "train: step: 916, loss: 0.1692541241645813, acc: 0.921875, recall: 0.9076923076923077, precision: 0.9365079365079365, f_beta: 0.9218749999999999\n",
      "train: step: 917, loss: 0.17179284989833832, acc: 0.9375, recall: 0.9152542372881356, precision: 0.9473684210526315, f_beta: 0.9310344827586206\n",
      "train: step: 918, loss: 0.08001455664634705, acc: 0.9765625, recall: 0.9473684210526315, precision: 1.0, f_beta: 0.972972972972973\n",
      "train: step: 919, loss: 0.11964824795722961, acc: 0.96875, recall: 0.9841269841269841, precision: 0.9538461538461539, f_beta: 0.96875\n",
      "train: step: 920, loss: 0.18647360801696777, acc: 0.9296875, recall: 0.9076923076923077, precision: 0.9516129032258065, f_beta: 0.9291338582677167\n",
      "train: step: 921, loss: 0.2025817334651947, acc: 0.9375, recall: 0.9206349206349206, precision: 0.9508196721311475, f_beta: 0.9354838709677418\n",
      "train: step: 922, loss: 0.11599848419427872, acc: 0.953125, recall: 0.95, precision: 0.95, f_beta: 0.9500000000000001\n",
      "train: step: 923, loss: 0.18811193108558655, acc: 0.953125, recall: 0.9701492537313433, precision: 0.9420289855072463, f_beta: 0.9558823529411764\n",
      "train: step: 924, loss: 0.2402133345603943, acc: 0.90625, recall: 0.9193548387096774, precision: 0.890625, f_beta: 0.9047619047619047\n",
      "train: step: 925, loss: 0.22593358159065247, acc: 0.9140625, recall: 0.921875, precision: 0.9076923076923077, f_beta: 0.9147286821705427\n",
      "train: step: 926, loss: 0.12659099698066711, acc: 0.96875, recall: 0.9411764705882353, precision: 1.0, f_beta: 0.9696969696969697\n",
      "train: step: 927, loss: 0.12933409214019775, acc: 0.953125, recall: 0.9508196721311475, precision: 0.9508196721311475, f_beta: 0.9508196721311475\n",
      "train: step: 928, loss: 0.21481479704380035, acc: 0.921875, recall: 0.8923076923076924, precision: 0.9508196721311475, f_beta: 0.9206349206349206\n",
      "train: step: 929, loss: 0.10197526216506958, acc: 0.9609375, recall: 0.9672131147540983, precision: 0.9516129032258065, f_beta: 0.959349593495935\n",
      "train: step: 930, loss: 0.11784720420837402, acc: 0.9609375, recall: 0.9545454545454546, precision: 0.9692307692307692, f_beta: 0.9618320610687022\n",
      "train: step: 931, loss: 0.12659567594528198, acc: 0.953125, recall: 0.9692307692307692, precision: 0.9402985074626866, f_beta: 0.9545454545454547\n",
      "train: step: 932, loss: 0.20232778787612915, acc: 0.9375, recall: 0.9056603773584906, precision: 0.9411764705882353, f_beta: 0.923076923076923\n",
      "train: step: 933, loss: 0.2427939474582672, acc: 0.90625, recall: 0.9230769230769231, precision: 0.8571428571428571, f_beta: 0.888888888888889\n",
      "train: step: 934, loss: 0.11223262548446655, acc: 0.9765625, recall: 0.9682539682539683, precision: 0.9838709677419355, f_beta: 0.976\n",
      "train: step: 935, loss: 0.11639802157878876, acc: 0.96875, recall: 0.9852941176470589, precision: 0.9571428571428572, f_beta: 0.9710144927536232\n",
      "train: step: 936, loss: 0.15730145573616028, acc: 0.953125, recall: 0.9354838709677419, precision: 0.9666666666666667, f_beta: 0.9508196721311476\n",
      "start training model\n",
      "train: step: 937, loss: 0.17812705039978027, acc: 0.9453125, recall: 0.9625, precision: 0.9506172839506173, f_beta: 0.9565217391304348\n",
      "train: step: 938, loss: 0.09163236618041992, acc: 0.96875, recall: 0.9552238805970149, precision: 0.9846153846153847, f_beta: 0.9696969696969696\n",
      "train: step: 939, loss: 0.06323376297950745, acc: 0.984375, recall: 0.9836065573770492, precision: 0.9836065573770492, f_beta: 0.9836065573770492\n",
      "train: step: 940, loss: 0.12526853382587433, acc: 0.9609375, recall: 0.9365079365079365, precision: 0.9833333333333333, f_beta: 0.9593495934959351\n",
      "train: step: 941, loss: 0.19693703949451447, acc: 0.9375, recall: 0.9393939393939394, precision: 0.9393939393939394, f_beta: 0.9393939393939394\n",
      "train: step: 942, loss: 0.1941247433423996, acc: 0.9375, recall: 0.9310344827586207, precision: 0.9310344827586207, f_beta: 0.9310344827586207\n",
      "train: step: 943, loss: 0.07532281428575516, acc: 0.9765625, recall: 0.9538461538461539, precision: 1.0, f_beta: 0.9763779527559054\n",
      "train: step: 944, loss: 0.15863776206970215, acc: 0.9609375, recall: 0.9692307692307692, precision: 0.9545454545454546, f_beta: 0.9618320610687022\n",
      "train: step: 945, loss: 0.07384233921766281, acc: 0.9765625, recall: 0.9692307692307692, precision: 0.984375, f_beta: 0.9767441860465116\n",
      "train: step: 946, loss: 0.21138668060302734, acc: 0.9375, recall: 0.8909090909090909, precision: 0.9607843137254902, f_beta: 0.9245283018867925\n",
      "train: step: 947, loss: 0.025541212409734726, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 948, loss: 0.06577523797750473, acc: 0.9921875, recall: 1.0, precision: 0.9848484848484849, f_beta: 0.9923664122137404\n",
      "train: step: 949, loss: 0.15458089113235474, acc: 0.9296875, recall: 0.896551724137931, precision: 0.9454545454545454, f_beta: 0.920353982300885\n",
      "train: step: 950, loss: 0.136474609375, acc: 0.953125, recall: 0.9402985074626866, precision: 0.9692307692307692, f_beta: 0.9545454545454547\n",
      "train: step: 951, loss: 0.04827052727341652, acc: 0.984375, recall: 0.9672131147540983, precision: 1.0, f_beta: 0.9833333333333333\n",
      "train: step: 952, loss: 0.07710488140583038, acc: 0.96875, recall: 0.9402985074626866, precision: 1.0, f_beta: 0.9692307692307692\n",
      "train: step: 953, loss: 0.02702224627137184, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 954, loss: 0.07589324563741684, acc: 0.984375, recall: 0.9818181818181818, precision: 0.9818181818181818, f_beta: 0.9818181818181818\n",
      "train: step: 955, loss: 0.0980796217918396, acc: 0.96875, recall: 0.9852941176470589, precision: 0.9571428571428572, f_beta: 0.9710144927536232\n",
      "train: step: 956, loss: 0.13232003152370453, acc: 0.96875, recall: 0.9571428571428572, precision: 0.9852941176470589, f_beta: 0.9710144927536232\n",
      "train: step: 957, loss: 0.14842022955417633, acc: 0.9453125, recall: 0.9375, precision: 0.9523809523809523, f_beta: 0.9448818897637795\n",
      "train: step: 958, loss: 0.049232885241508484, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 959, loss: 0.10280482470989227, acc: 0.96875, recall: 0.9705882352941176, precision: 0.9705882352941176, f_beta: 0.9705882352941176\n",
      "train: step: 960, loss: 0.03452190011739731, acc: 0.9921875, recall: 0.9836065573770492, precision: 1.0, f_beta: 0.9917355371900827\n",
      "train: step: 961, loss: 0.17529605329036713, acc: 0.9453125, recall: 0.9444444444444444, precision: 0.9577464788732394, f_beta: 0.951048951048951\n",
      "train: step: 962, loss: 0.07353466749191284, acc: 0.9765625, recall: 0.9848484848484849, precision: 0.9701492537313433, f_beta: 0.9774436090225564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 963, loss: 0.20852187275886536, acc: 0.953125, recall: 0.9402985074626866, precision: 0.9692307692307692, f_beta: 0.9545454545454547\n",
      "train: step: 964, loss: 0.053001247346401215, acc: 0.9921875, recall: 0.9841269841269841, precision: 1.0, f_beta: 0.9919999999999999\n",
      "train: step: 965, loss: 0.10054639726877213, acc: 0.9765625, recall: 0.9710144927536232, precision: 0.9852941176470589, f_beta: 0.9781021897810219\n",
      "train: step: 966, loss: 0.03771469369530678, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 967, loss: 0.10632183402776718, acc: 0.96875, recall: 0.9655172413793104, precision: 0.9655172413793104, f_beta: 0.9655172413793104\n",
      "train: step: 968, loss: 0.09043075144290924, acc: 0.9765625, recall: 0.9714285714285714, precision: 0.9855072463768116, f_beta: 0.9784172661870504\n",
      "train: step: 969, loss: 0.05763203650712967, acc: 0.9765625, recall: 0.9672131147540983, precision: 0.9833333333333333, f_beta: 0.9752066115702478\n",
      "train: step: 970, loss: 0.14268198609352112, acc: 0.953125, recall: 0.9310344827586207, precision: 0.9642857142857143, f_beta: 0.9473684210526316\n",
      "train: step: 971, loss: 0.17161275446414948, acc: 0.9375, recall: 0.9285714285714286, precision: 0.9558823529411765, f_beta: 0.9420289855072465\n",
      "train: step: 972, loss: 0.08963966369628906, acc: 0.9765625, recall: 0.9714285714285714, precision: 0.9855072463768116, f_beta: 0.9784172661870504\n",
      "train: step: 973, loss: 0.07483404874801636, acc: 0.984375, recall: 0.975, precision: 1.0, f_beta: 0.9873417721518987\n",
      "train: step: 974, loss: 0.07668642699718475, acc: 0.9765625, recall: 0.9818181818181818, precision: 0.9642857142857143, f_beta: 0.972972972972973\n",
      "train: step: 975, loss: 0.04750766232609749, acc: 0.984375, recall: 0.9857142857142858, precision: 0.9857142857142858, f_beta: 0.9857142857142858\n",
      "train: step: 976, loss: 0.09388887137174606, acc: 0.984375, recall: 0.9824561403508771, precision: 0.9824561403508771, f_beta: 0.9824561403508771\n",
      "train: step: 977, loss: 0.07725325226783752, acc: 0.9765625, recall: 0.9714285714285714, precision: 0.9855072463768116, f_beta: 0.9784172661870504\n",
      "train: step: 978, loss: 0.04370029270648956, acc: 0.9765625, recall: 0.9821428571428571, precision: 0.9649122807017544, f_beta: 0.9734513274336283\n",
      "train: step: 979, loss: 0.10557620972394943, acc: 0.9609375, recall: 0.9523809523809523, precision: 0.967741935483871, f_beta: 0.96\n",
      "train: step: 980, loss: 0.08261135965585709, acc: 0.96875, recall: 0.9696969696969697, precision: 0.9696969696969697, f_beta: 0.9696969696969697\n",
      "train: step: 981, loss: 0.1886640042066574, acc: 0.953125, recall: 0.9622641509433962, precision: 0.9272727272727272, f_beta: 0.9444444444444444\n",
      "train: step: 982, loss: 0.1813526302576065, acc: 0.9453125, recall: 0.9402985074626866, precision: 0.9545454545454546, f_beta: 0.9473684210526316\n",
      "train: step: 983, loss: 0.23234258592128754, acc: 0.9453125, recall: 0.971830985915493, precision: 0.9324324324324325, f_beta: 0.9517241379310345\n",
      "train: step: 984, loss: 0.0615653470158577, acc: 0.9609375, recall: 0.9836065573770492, precision: 0.9375, f_beta: 0.96\n",
      "train: step: 985, loss: 0.13501223921775818, acc: 0.953125, recall: 0.9830508474576272, precision: 0.9206349206349206, f_beta: 0.9508196721311476\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8b491f92c490>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"start training model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatchTrain\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnextBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainReviews\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_beta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainStep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatchTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[0mcurrentStep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobalStep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-8b491f92c490>\u001b[0m in \u001b[0;36mtrainStep\u001b[1;34m(batchX, batchY)\u001b[0m\n\u001b[0;32m     73\u001b[0m             _, summary, step, loss, predictions = sess.run(\n\u001b[0;32m     74\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mtrainOp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummaryOp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobalStep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 feed_dict)\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mtimeStr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "labelList = data.labelList\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTM(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        savedModelPath = \"../model/Bi-LSTM/savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "            os.rmdir(savedModelPath)\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
    "                feed_dict)\n",
    "            \n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "                \n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
    "                                                              labels=labelList)\n",
    "                \n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, prec, recall, f_beta\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
    "                feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "            \n",
    "                acc, precision, recall, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, precision, recall, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, precision, recall, f_beta\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "                \n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                    currentStep, loss, acc, recall, prec, f_beta))\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    f_betas = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        f_betas.append(f_beta)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(precisions),\n",
    "                                                                                                       mean(recalls), mean(f_betas)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(lstm.predictions)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../model/Bi-LSTM/model/my-model-900\n",
      "['1']\n"
     ]
    }
   ],
   "source": [
    "x = \"this movie is full of references like mad max ii the wild one and many others the ladybug´s face it´s a clear reference or tribute to peter lorre this movie is a masterpiece we´ll talk much more about in the future\"\n",
    "\n",
    "# 注：下面两个词典要保证和当前加载的模型对应的词典是一致的\n",
    "with open(\"../data/wordJson/word2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx = json.load(f)\n",
    "        \n",
    "with open(\"../data/wordJson/label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label2idx = json.load(f)\n",
    "idx2label = {value: key for key, value in label2idx.items()}\n",
    "    \n",
    "xIds = [word2idx.get(item, word2idx[\"UNK\"]) for item in x.split(\" \")]\n",
    "if len(xIds) >= config.sequenceLength:\n",
    "    xIds = xIds[:config.sequenceLength]\n",
    "else:\n",
    "    xIds = xIds + [word2idx[\"PAD\"]] * (config.sequenceLength - len(xIds))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, gpu_options=gpu_options)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        checkpoint_file = tf.train.latest_checkpoint(\"../model/Bi-LSTM/model/\")\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # 获得需要喂给模型的参数，输出的结果依赖的输入值\n",
    "        inputX = graph.get_operation_by_name(\"inputX\").outputs[0]\n",
    "        dropoutKeepProb = graph.get_operation_by_name(\"dropoutKeepProb\").outputs[0]\n",
    "\n",
    "        # 获得输出的结果\n",
    "        predictions = graph.get_tensor_by_name(\"output/predictions:0\")\n",
    "\n",
    "        pred = sess.run(predictions, feed_dict={inputX: [xIds], dropoutKeepProb: 1.0})[0]\n",
    "        \n",
    "pred = [idx2label[item] for item in pred]     \n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

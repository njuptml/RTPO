{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 4\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [256, 128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 1  # 二分类设置为1，多分类设置为类别的数目\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self.labelList = []\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        \n",
    "        if self.config.numClasses == 1:\n",
    "            labels = df[\"sentiment\"].tolist()\n",
    "        elif self.config.numClasses > 1:\n",
    "            labels = df[\"rate\"].tolist()\n",
    "            \n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        \"\"\"\n",
    "        将标签转换成索引表示\n",
    "        \"\"\"\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        return labelIds\n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        \"\"\"\n",
    "        将词转换成索引\n",
    "        \"\"\"\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "        \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        for review in x:\n",
    "            if len(review) >= self._sequenceLength:\n",
    "                reviews.append(review[:self._sequenceLength])\n",
    "            else:\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews, labels):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
    "        \n",
    "        # 将标签和句子数值化\n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000,)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTMAttention(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs_, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords, dtype=tf.float32,\n",
    "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2], 传入到下一层Bi-LSTM中\n",
    "                    self.embeddedWords = tf.concat(outputs_, 2)\n",
    "                \n",
    "        # 将最后一层Bi-LSTM输出的结果分割成前向和后向的输出\n",
    "        outputs = tf.split(self.embeddedWords, 2, -1)\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self.attention(H)\n",
    "            outputSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, config.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(output, outputW, outputB, name=\"logits\")\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.float32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                                                    dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "                \n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss\n",
    "    \n",
    "    def attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes[-1]\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.reshape(r, [-1, hiddenSize])\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义各类性能指标\n",
    "\"\"\"\n",
    "\n",
    "def mean(item: list) -> float:\n",
    "    \"\"\"\n",
    "    计算列表中元素的平均值\n",
    "    :param item: 列表对象\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    \"\"\"\n",
    "    计算二类和多类的准确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == true_y[i]:\n",
    "            corr += 1\n",
    "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "    return acc\n",
    "\n",
    "\n",
    "def binary_precision(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的精确率计算\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    pred_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == positive:\n",
    "            pred_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "    return prec\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    true_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if true_y[i] == positive:\n",
    "            true_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    rec = corr / true_corr if true_corr > 0 else 0\n",
    "    return rec\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    \"\"\"\n",
    "    二类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param beta: beta值\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    try:\n",
    "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    except:\n",
    "        f_b = 0\n",
    "    return f_b\n",
    "\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的精确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    \"\"\"\n",
    "    多类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :param beta: beta值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到二分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到多分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param labels:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = multi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm1/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm1/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/hist is illegal; using Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Attention/Variable:0/grad/sparsity is illegal; using Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to E:\\githubProject\\textClassifier\\Bi-LSTM+Attention\\summarys\n",
      "\n",
      "start training model\n",
      "train: step: 1, loss: 0.7088837027549744, acc: 0.546875, recall: 0.140625, precision: 0.75, f_beta: 0.23684210526315788\n",
      "train: step: 2, loss: 1.7695387601852417, acc: 0.4140625, recall: 1.0, precision: 0.4140625, f_beta: 0.585635359116022\n",
      "train: step: 3, loss: 0.7615395188331604, acc: 0.5, recall: 1.0, precision: 0.5, f_beta: 0.6666666666666666\n",
      "train: step: 4, loss: 0.8020325899124146, acc: 0.4453125, recall: 0.06944444444444445, precision: 0.5555555555555556, f_beta: 0.12345679012345681\n",
      "train: step: 5, loss: 0.8011478185653687, acc: 0.5078125, recall: 0.01694915254237288, precision: 0.16666666666666666, f_beta: 0.030769230769230767\n",
      "train: step: 6, loss: 0.7380817532539368, acc: 0.5234375, recall: 0.12698412698412698, precision: 0.5714285714285714, f_beta: 0.20779220779220778\n",
      "train: step: 7, loss: 0.7224935293197632, acc: 0.484375, recall: 0.5076923076923077, precision: 0.4925373134328358, f_beta: 0.5\n",
      "train: step: 8, loss: 0.7092139720916748, acc: 0.4765625, recall: 0.819672131147541, precision: 0.4716981132075472, f_beta: 0.5988023952095808\n",
      "train: step: 9, loss: 0.7310372591018677, acc: 0.5546875, recall: 0.9571428571428572, precision: 0.5537190082644629, f_beta: 0.7015706806282722\n",
      "train: step: 10, loss: 0.8006136417388916, acc: 0.4375, recall: 0.9482758620689655, precision: 0.4435483870967742, f_beta: 0.6043956043956044\n",
      "train: step: 11, loss: 0.7052208185195923, acc: 0.5, recall: 0.9365079365079365, precision: 0.4957983193277311, f_beta: 0.6483516483516484\n",
      "train: step: 12, loss: 0.7138729691505432, acc: 0.4765625, recall: 0.7903225806451613, precision: 0.47572815533980584, f_beta: 0.5939393939393939\n",
      "train: step: 13, loss: 0.6739441156387329, acc: 0.546875, recall: 0.5873015873015873, precision: 0.5362318840579711, f_beta: 0.5606060606060606\n",
      "train: step: 14, loss: 0.7094039916992188, acc: 0.4375, recall: 0.2328767123287671, precision: 0.5151515151515151, f_beta: 0.320754716981132\n",
      "train: step: 15, loss: 0.7143986225128174, acc: 0.515625, recall: 0.203125, precision: 0.5416666666666666, f_beta: 0.29545454545454547\n",
      "train: step: 16, loss: 0.7575368881225586, acc: 0.4375, recall: 0.17333333333333334, precision: 0.5652173913043478, f_beta: 0.2653061224489796\n",
      "train: step: 17, loss: 0.6987805962562561, acc: 0.5234375, recall: 0.15789473684210525, precision: 0.4090909090909091, f_beta: 0.2278481012658228\n",
      "train: step: 18, loss: 0.6939811110496521, acc: 0.4765625, recall: 0.25, precision: 0.45714285714285713, f_beta: 0.32323232323232326\n",
      "train: step: 19, loss: 0.7091342210769653, acc: 0.46875, recall: 0.38095238095238093, precision: 0.4528301886792453, f_beta: 0.4137931034482758\n",
      "train: step: 20, loss: 0.7145029902458191, acc: 0.484375, recall: 0.46875, precision: 0.4838709677419355, f_beta: 0.47619047619047616\n",
      "train: step: 21, loss: 0.7109805941581726, acc: 0.421875, recall: 0.40384615384615385, precision: 0.328125, f_beta: 0.36206896551724144\n",
      "train: step: 22, loss: 0.6991376876831055, acc: 0.5, recall: 0.5416666666666666, precision: 0.5571428571428572, f_beta: 0.5492957746478873\n",
      "train: step: 23, loss: 0.692231297492981, acc: 0.5546875, recall: 0.5846153846153846, precision: 0.5588235294117647, f_beta: 0.5714285714285715\n",
      "train: step: 24, loss: 0.6723755598068237, acc: 0.5703125, recall: 0.625, precision: 0.6164383561643836, f_beta: 0.6206896551724138\n",
      "train: step: 25, loss: 0.7031429409980774, acc: 0.5703125, recall: 0.6764705882352942, precision: 0.5822784810126582, f_beta: 0.6258503401360545\n",
      "train: step: 26, loss: 0.7080327868461609, acc: 0.5078125, recall: 0.6612903225806451, precision: 0.4939759036144578, f_beta: 0.5655172413793104\n",
      "train: step: 27, loss: 0.7106336951255798, acc: 0.5390625, recall: 0.6923076923076923, precision: 0.5357142857142857, f_beta: 0.6040268456375838\n",
      "train: step: 28, loss: 0.7397019863128662, acc: 0.4375, recall: 0.6721311475409836, precision: 0.44086021505376344, f_beta: 0.5324675324675324\n",
      "train: step: 29, loss: 0.6900271773338318, acc: 0.515625, recall: 0.625, precision: 0.5625, f_beta: 0.5921052631578947\n",
      "train: step: 30, loss: 0.7183444499969482, acc: 0.5, recall: 0.5671641791044776, precision: 0.5205479452054794, f_beta: 0.5428571428571428\n",
      "train: step: 31, loss: 0.7052744626998901, acc: 0.484375, recall: 0.576271186440678, precision: 0.4533333333333333, f_beta: 0.5074626865671641\n",
      "train: step: 32, loss: 0.6833520531654358, acc: 0.5703125, recall: 0.5714285714285714, precision: 0.5625, f_beta: 0.5669291338582677\n",
      "train: step: 33, loss: 0.6988614797592163, acc: 0.5546875, recall: 0.44642857142857145, precision: 0.49019607843137253, f_beta: 0.4672897196261682\n",
      "train: step: 34, loss: 0.6953092813491821, acc: 0.515625, recall: 0.42592592592592593, precision: 0.42592592592592593, f_beta: 0.42592592592592593\n",
      "train: step: 35, loss: 0.6880689859390259, acc: 0.4765625, recall: 0.1864406779661017, precision: 0.36666666666666664, f_beta: 0.24719101123595508\n",
      "train: step: 36, loss: 0.7030559182167053, acc: 0.5234375, recall: 0.3492063492063492, precision: 0.5238095238095238, f_beta: 0.41904761904761906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 37, loss: 0.6945070028305054, acc: 0.4921875, recall: 0.22388059701492538, precision: 0.5357142857142857, f_beta: 0.3157894736842105\n",
      "train: step: 38, loss: 0.7284985780715942, acc: 0.46875, recall: 0.2318840579710145, precision: 0.5161290322580645, f_beta: 0.31999999999999995\n",
      "train: step: 39, loss: 0.702323853969574, acc: 0.5625, recall: 0.2903225806451613, precision: 0.6, f_beta: 0.3913043478260869\n",
      "train: step: 40, loss: 0.7271304130554199, acc: 0.4453125, recall: 0.2857142857142857, precision: 0.3404255319148936, f_beta: 0.3106796116504854\n",
      "train: step: 41, loss: 0.6898088455200195, acc: 0.5390625, recall: 0.44776119402985076, precision: 0.5769230769230769, f_beta: 0.5042016806722689\n",
      "train: step: 42, loss: 0.6891903877258301, acc: 0.546875, recall: 0.532258064516129, precision: 0.532258064516129, f_beta: 0.532258064516129\n",
      "train: step: 43, loss: 0.6877189874649048, acc: 0.546875, recall: 0.6, precision: 0.5142857142857142, f_beta: 0.5538461538461538\n",
      "train: step: 44, loss: 0.7009769082069397, acc: 0.5234375, recall: 0.5454545454545454, precision: 0.5373134328358209, f_beta: 0.5413533834586466\n",
      "train: step: 45, loss: 0.7120286822319031, acc: 0.4609375, recall: 0.5625, precision: 0.4675324675324675, f_beta: 0.5106382978723405\n",
      "train: step: 46, loss: 0.6973256468772888, acc: 0.484375, recall: 0.5714285714285714, precision: 0.5263157894736842, f_beta: 0.547945205479452\n",
      "train: step: 47, loss: 0.6842973828315735, acc: 0.5625, recall: 0.6231884057971014, precision: 0.589041095890411, f_beta: 0.6056338028169014\n",
      "train: step: 48, loss: 0.7096275687217712, acc: 0.5, recall: 0.6875, precision: 0.5, f_beta: 0.5789473684210527\n",
      "train: step: 49, loss: 0.7123950719833374, acc: 0.5, recall: 0.78, precision: 0.42391304347826086, f_beta: 0.5492957746478873\n",
      "train: step: 50, loss: 0.730542004108429, acc: 0.421875, recall: 0.5873015873015873, precision: 0.43529411764705883, f_beta: 0.5000000000000001\n",
      "train: step: 51, loss: 0.7190756797790527, acc: 0.4609375, recall: 0.515625, precision: 0.4647887323943662, f_beta: 0.4888888888888889\n",
      "train: step: 52, loss: 0.7229070067405701, acc: 0.4375, recall: 0.35714285714285715, precision: 0.4807692307692308, f_beta: 0.4098360655737705\n",
      "train: step: 53, loss: 0.6774759292602539, acc: 0.5390625, recall: 0.4507042253521127, precision: 0.6153846153846154, f_beta: 0.5203252032520326\n",
      "train: step: 54, loss: 0.6900168061256409, acc: 0.515625, recall: 0.4027777777777778, precision: 0.6041666666666666, f_beta: 0.48333333333333334\n",
      "train: step: 55, loss: 0.7034347057342529, acc: 0.4609375, recall: 0.4142857142857143, precision: 0.5087719298245614, f_beta: 0.4566929133858268\n",
      "train: step: 56, loss: 0.7147462368011475, acc: 0.4609375, recall: 0.5, precision: 0.4057971014492754, f_beta: 0.448\n",
      "train: step: 57, loss: 0.6856096982955933, acc: 0.578125, recall: 0.5882352941176471, precision: 0.47619047619047616, f_beta: 0.5263157894736842\n",
      "train: step: 58, loss: 0.7311606407165527, acc: 0.4375, recall: 0.21739130434782608, precision: 0.45454545454545453, f_beta: 0.29411764705882354\n",
      "train: step: 59, loss: 0.6845932006835938, acc: 0.5625, recall: 0.35294117647058826, precision: 0.43902439024390244, f_beta: 0.391304347826087\n",
      "train: step: 60, loss: 0.6949717402458191, acc: 0.5078125, recall: 0.16981132075471697, precision: 0.32142857142857145, f_beta: 0.2222222222222222\n",
      "train: step: 61, loss: 0.7230887413024902, acc: 0.4375, recall: 0.15151515151515152, precision: 0.38461538461538464, f_beta: 0.21739130434782608\n",
      "train: step: 62, loss: 0.6828291416168213, acc: 0.546875, recall: 0.15, precision: 0.5625, f_beta: 0.23684210526315785\n",
      "train: step: 63, loss: 0.6879544258117676, acc: 0.4921875, recall: 0.11940298507462686, precision: 0.5714285714285714, f_beta: 0.19753086419753085\n",
      "train: step: 64, loss: 0.6932634115219116, acc: 0.5390625, recall: 0.17543859649122806, precision: 0.45454545454545453, f_beta: 0.25316455696202533\n",
      "train: step: 65, loss: 0.7077312469482422, acc: 0.5390625, recall: 0.25757575757575757, precision: 0.6296296296296297, f_beta: 0.3655913978494624\n",
      "train: step: 66, loss: 0.687142014503479, acc: 0.515625, recall: 0.2, precision: 0.7, f_beta: 0.3111111111111111\n",
      "train: step: 67, loss: 0.6978955268859863, acc: 0.5, recall: 0.31343283582089554, precision: 0.5384615384615384, f_beta: 0.3962264150943396\n",
      "train: step: 68, loss: 0.712117075920105, acc: 0.4375, recall: 0.28169014084507044, precision: 0.4878048780487805, f_beta: 0.3571428571428571\n",
      "train: step: 69, loss: 0.6806811094284058, acc: 0.546875, recall: 0.532258064516129, precision: 0.532258064516129, f_beta: 0.532258064516129\n",
      "train: step: 70, loss: 0.6897184252738953, acc: 0.5, recall: 0.704225352112676, precision: 0.5376344086021505, f_beta: 0.6097560975609756\n",
      "train: step: 71, loss: 0.703205943107605, acc: 0.5, recall: 0.85, precision: 0.4811320754716981, f_beta: 0.6144578313253013\n",
      "train: step: 72, loss: 0.7101638317108154, acc: 0.421875, recall: 0.8070175438596491, precision: 0.42201834862385323, f_beta: 0.5542168674698795\n",
      "train: step: 73, loss: 0.7058019638061523, acc: 0.515625, recall: 0.8833333333333333, precision: 0.49074074074074076, f_beta: 0.6309523809523809\n",
      "train: step: 74, loss: 0.6900935769081116, acc: 0.5234375, recall: 0.8571428571428571, precision: 0.5405405405405406, f_beta: 0.6629834254143647\n",
      "train: step: 75, loss: 0.7112457752227783, acc: 0.5, recall: 0.8, precision: 0.5048543689320388, f_beta: 0.6190476190476191\n",
      "train: step: 76, loss: 0.6924203634262085, acc: 0.546875, recall: 0.7971014492753623, precision: 0.5555555555555556, f_beta: 0.6547619047619048\n",
      "train: step: 77, loss: 0.7022581100463867, acc: 0.4765625, recall: 0.647887323943662, precision: 0.5227272727272727, f_beta: 0.5786163522012578\n",
      "train: step: 78, loss: 0.6996952295303345, acc: 0.484375, recall: 0.7307692307692307, precision: 0.4222222222222222, f_beta: 0.5352112676056338\n",
      "train: step: 79, loss: 0.7038499116897583, acc: 0.484375, recall: 0.6190476190476191, precision: 0.48148148148148145, f_beta: 0.5416666666666666\n",
      "train: step: 80, loss: 0.6987332105636597, acc: 0.578125, recall: 0.5694444444444444, precision: 0.640625, f_beta: 0.6029411764705882\n",
      "train: step: 81, loss: 0.6750752925872803, acc: 0.59375, recall: 0.5, precision: 0.6730769230769231, f_beta: 0.5737704918032787\n",
      "train: step: 82, loss: 0.6972882747650146, acc: 0.5625, recall: 0.4626865671641791, precision: 0.6078431372549019, f_beta: 0.5254237288135594\n",
      "train: step: 83, loss: 0.6983337998390198, acc: 0.5234375, recall: 0.46153846153846156, precision: 0.5357142857142857, f_beta: 0.49586776859504134\n",
      "train: step: 84, loss: 0.6671377420425415, acc: 0.59375, recall: 0.6515151515151515, precision: 0.5972222222222222, f_beta: 0.6231884057971014\n",
      "train: step: 85, loss: 0.7016528844833374, acc: 0.515625, recall: 0.6716417910447762, precision: 0.5294117647058824, f_beta: 0.5921052631578948\n",
      "train: step: 86, loss: 0.6991066336631775, acc: 0.515625, recall: 0.6323529411764706, precision: 0.5375, f_beta: 0.581081081081081\n",
      "train: step: 87, loss: 0.7056317329406738, acc: 0.46875, recall: 0.6727272727272727, precision: 0.42528735632183906, f_beta: 0.5211267605633803\n",
      "train: step: 88, loss: 0.7036464810371399, acc: 0.5078125, recall: 0.6875, precision: 0.5057471264367817, f_beta: 0.5827814569536425\n",
      "train: step: 89, loss: 0.6802926063537598, acc: 0.515625, recall: 0.7413793103448276, precision: 0.4777777777777778, f_beta: 0.581081081081081\n",
      "train: step: 90, loss: 0.6979688405990601, acc: 0.5234375, recall: 0.8382352941176471, precision: 0.5327102803738317, f_beta: 0.6514285714285715\n",
      "train: step: 91, loss: 0.6857626438140869, acc: 0.5546875, recall: 0.7222222222222222, precision: 0.5842696629213483, f_beta: 0.6459627329192547\n",
      "train: step: 92, loss: 0.6914296746253967, acc: 0.53125, recall: 0.8405797101449275, precision: 0.5420560747663551, f_beta: 0.659090909090909\n",
      "train: step: 93, loss: 0.6839147210121155, acc: 0.53125, recall: 0.8088235294117647, precision: 0.5392156862745098, f_beta: 0.6470588235294117\n",
      "train: step: 94, loss: 0.7066893577575684, acc: 0.515625, recall: 0.7611940298507462, precision: 0.5257731958762887, f_beta: 0.621951219512195\n",
      "train: step: 95, loss: 0.6809223890304565, acc: 0.5078125, recall: 0.8870967741935484, precision: 0.4954954954954955, f_beta: 0.6358381502890174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 96, loss: 0.705482006072998, acc: 0.484375, recall: 0.864406779661017, precision: 0.46788990825688076, f_beta: 0.6071428571428572\n",
      "train: step: 97, loss: 0.6875587105751038, acc: 0.578125, recall: 0.7808219178082192, precision: 0.6, f_beta: 0.6785714285714285\n",
      "train: step: 98, loss: 0.69725102186203, acc: 0.5078125, recall: 0.5694444444444444, precision: 0.5616438356164384, f_beta: 0.5655172413793104\n",
      "train: step: 99, loss: 0.680282711982727, acc: 0.578125, recall: 0.7014925373134329, precision: 0.5802469135802469, f_beta: 0.6351351351351352\n",
      "train: step: 100, loss: 0.6873658895492554, acc: 0.546875, recall: 0.7333333333333333, precision: 0.5116279069767442, f_beta: 0.6027397260273971\n",
      "\n",
      "Evaluation:\n",
      "2019-07-16T18:54:21.555033, step: 100, loss: 0.6840359339347253, acc: 0.5905448717948718,precision: 0.7863288427838793, recall: 0.568647995590122, f_beta: 0.6588665541894034\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parent directory of ../model/Bi-LSTM-atten/model/my-model doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ../model/Bi-LSTM-atten/model; No such file or directory\n\t [[{{node save/SaveV2}} = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Attention/Variable/_193, Attention/Variable/Adam/_195, Attention/Variable/Adam_1/_197, beta1_power/_199, beta2_power/_201, bi-lstm0/bw/lstm_cell/bias/_203, bi-lstm0/bw/lstm_cell/bias/Adam/_205, bi-lstm0/bw/lstm_cell/bias/Adam_1/_207, bi-lstm0/bw/lstm_cell/kernel/_209, bi-lstm0/bw/lstm_cell/kernel/Adam/_211, bi-lstm0/bw/lstm_cell/kernel/Adam_1/_213, bi-lstm0/fw/lstm_cell/bias/_215, bi-lstm0/fw/lstm_cell/bias/Adam/_217, bi-lstm0/fw/lstm_cell/bias/Adam_1/_219, bi-lstm0/fw/lstm_cell/kernel/_221, bi-lstm0/fw/lstm_cell/kernel/Adam/_223, bi-lstm0/fw/lstm_cell/kernel/Adam_1/_225, bi-lstm1/bw/lstm_cell/bias/_227, bi-lstm1/bw/lstm_cell/bias/Adam/_229, bi-lstm1/bw/lstm_cell/bias/Adam_1/_231, bi-lstm1/bw/lstm_cell/kernel/_233, bi-lstm1/bw/lstm_cell/kernel/Adam/_235, bi-lstm1/bw/lstm_cell/kernel/Adam_1/_237, bi-lstm1/fw/lstm_cell/bias/_239, bi-lstm1/fw/lstm_cell/bias/Adam/_241, bi-lstm1/fw/lstm_cell/bias/Adam_1/_243, bi-lstm1/fw/lstm_cell/kernel/_245, bi-lstm1/fw/lstm_cell/kernel/Adam/_247, bi-lstm1/fw/lstm_cell/kernel/Adam_1/_249, embedding/W/_251, embedding/W/Adam/_253, embedding/W/Adam_1/_255, globalStep, output/outputB/_257, output/outputB/Adam/_259, output/outputB/Adam_1/_261, outputW/_263, outputW/Adam/_265, outputW/Adam_1/_267)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1440\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ../model/Bi-LSTM-atten/model; No such file or directory\n\t [[node save/SaveV2 (defined at <ipython-input-8-dae238248247>:54)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Attention/Variable/_193, Attention/Variable/Adam/_195, Attention/Variable/Adam_1/_197, beta1_power/_199, beta2_power/_201, bi-lstm0/bw/lstm_cell/bias/_203, bi-lstm0/bw/lstm_cell/bias/Adam/_205, bi-lstm0/bw/lstm_cell/bias/Adam_1/_207, bi-lstm0/bw/lstm_cell/kernel/_209, bi-lstm0/bw/lstm_cell/kernel/Adam/_211, bi-lstm0/bw/lstm_cell/kernel/Adam_1/_213, bi-lstm0/fw/lstm_cell/bias/_215, bi-lstm0/fw/lstm_cell/bias/Adam/_217, bi-lstm0/fw/lstm_cell/bias/Adam_1/_219, bi-lstm0/fw/lstm_cell/kernel/_221, bi-lstm0/fw/lstm_cell/kernel/Adam/_223, bi-lstm0/fw/lstm_cell/kernel/Adam_1/_225, bi-lstm1/bw/lstm_cell/bias/_227, bi-lstm1/bw/lstm_cell/bias/Adam/_229, bi-lstm1/bw/lstm_cell/bias/Adam_1/_231, bi-lstm1/bw/lstm_cell/kernel/_233, bi-lstm1/bw/lstm_cell/kernel/Adam/_235, bi-lstm1/bw/lstm_cell/kernel/Adam_1/_237, bi-lstm1/fw/lstm_cell/bias/_239, bi-lstm1/fw/lstm_cell/bias/Adam/_241, bi-lstm1/fw/lstm_cell/bias/Adam_1/_243, bi-lstm1/fw/lstm_cell/kernel/_245, bi-lstm1/fw/lstm_cell/kernel/Adam/_247, bi-lstm1/fw/lstm_cell/kernel/Adam_1/_249, embedding/W/_251, embedding/W/Adam/_253, embedding/W/Adam_1/_255, globalStep, output/outputB/_257, output/outputB/Adam/_259, output/outputB/Adam_1/_261, outputW/_263, outputW/Adam/_265, outputW/Adam_1/_267)]]\n\nCaused by op 'save/SaveV2', defined at:\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tornado\\gen.py\", line 781, in inner\n    self.run()\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tornado\\gen.py\", line 742, in run\n    yielded = self.gen.send(value)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3049, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3214, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3296, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-8-dae238248247>\", line 54, in <module>\n    saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1102, in __init__\n    self.build()\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1114, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1151, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 792, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 284, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 202, in save_op\n    tensors)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1690, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"c:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nNotFoundError (see above for traceback): Failed to create a directory: ../model/Bi-LSTM-atten/model; No such file or directory\n\t [[node save/SaveV2 (defined at <ipython-input-8-dae238248247>:54)  = SaveV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/SaveV2/tensor_names, save/SaveV2/shape_and_slices, Attention/Variable/_193, Attention/Variable/Adam/_195, Attention/Variable/Adam_1/_197, beta1_power/_199, beta2_power/_201, bi-lstm0/bw/lstm_cell/bias/_203, bi-lstm0/bw/lstm_cell/bias/Adam/_205, bi-lstm0/bw/lstm_cell/bias/Adam_1/_207, bi-lstm0/bw/lstm_cell/kernel/_209, bi-lstm0/bw/lstm_cell/kernel/Adam/_211, bi-lstm0/bw/lstm_cell/kernel/Adam_1/_213, bi-lstm0/fw/lstm_cell/bias/_215, bi-lstm0/fw/lstm_cell/bias/Adam/_217, bi-lstm0/fw/lstm_cell/bias/Adam_1/_219, bi-lstm0/fw/lstm_cell/kernel/_221, bi-lstm0/fw/lstm_cell/kernel/Adam/_223, bi-lstm0/fw/lstm_cell/kernel/Adam_1/_225, bi-lstm1/bw/lstm_cell/bias/_227, bi-lstm1/bw/lstm_cell/bias/Adam/_229, bi-lstm1/bw/lstm_cell/bias/Adam_1/_231, bi-lstm1/bw/lstm_cell/kernel/_233, bi-lstm1/bw/lstm_cell/kernel/Adam/_235, bi-lstm1/bw/lstm_cell/kernel/Adam_1/_237, bi-lstm1/fw/lstm_cell/bias/_239, bi-lstm1/fw/lstm_cell/bias/Adam/_241, bi-lstm1/fw/lstm_cell/bias/Adam_1/_243, bi-lstm1/fw/lstm_cell/kernel/_245, bi-lstm1/fw/lstm_cell/kernel/Adam/_247, bi-lstm1/fw/lstm_cell/kernel/Adam_1/_249, embedding/W/_251, embedding/W/Adam/_253, embedding/W/Adam_1/_255, globalStep, output/outputB/_257, output/outputB/Adam/_259, output/outputB/Adam_1/_261, outputW/_263, outputW/Adam/_265, outputW/Adam_1/_267)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-dae238248247>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcurrentStep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckpointEvery\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                     \u001b[1;31m# 保存模型的另一种方法，保存checkpoint文件\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m                     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"../model/Bi-LSTM-atten/model/my-model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurrentStep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Saved model checkpoint to {}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\86130\\anaconda3\\envs\\panlp\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[0;32m   1457\u001b[0m                   save_path))\n\u001b[1;32m-> 1458\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parent directory of ../model/Bi-LSTM-atten/model/my-model doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "labelList = data.labelList\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTMAttention(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        savedModelPath = \"../model/bilstm-atten/savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "            os.rmdir(savedModelPath)\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "\n",
    "                \n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
    "                                                              labels=labelList)\n",
    "                \n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, prec, recall, f_beta\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
    "                feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "            \n",
    "                acc, precision, recall, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, precision, recall, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, precision, recall, f_beta\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "                \n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                    currentStep, loss, acc, recall, prec, f_beta))\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    f_betas = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        f_betas.append(f_beta)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(precisions),\n",
    "                                                                                                       mean(recalls), mean(f_betas)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/Bi-LSTM-atten/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(lstm.predictions)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"this movie is full of references like mad max ii the wild one and many others the ladybug´s face it´s a clear reference or tribute to peter lorre this movie is a masterpiece we´ll talk much more about in the future\"\n",
    "\n",
    "# 注：下面两个词典要保证和当前加载的模型对应的词典是一致的\n",
    "with open(\"../data/wordJson/word2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx = json.load(f)\n",
    "        \n",
    "with open(\"../data/wordJson/label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label2idx = json.load(f)\n",
    "idx2label = {value: key for key, value in label2idx.items()}\n",
    "    \n",
    "xIds = [word2idx.get(item, word2idx[\"UNK\"]) for item in x.split(\" \")]\n",
    "if len(xIds) >= config.sequenceLength:\n",
    "    xIds = xIds[:config.sequenceLength]\n",
    "else:\n",
    "    xIds = xIds + [word2idx[\"PAD\"]] * (config.sequenceLength - len(xIds))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, gpu_options=gpu_options)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        checkpoint_file = tf.train.latest_checkpoint(\"../model/Bi-LSTM-atten/model/\")\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # 获得需要喂给模型的参数，输出的结果依赖的输入值\n",
    "        inputX = graph.get_operation_by_name(\"inputX\").outputs[0]\n",
    "        dropoutKeepProb = graph.get_operation_by_name(\"dropoutKeepProb\").outputs[0]\n",
    "\n",
    "        # 获得输出的结果\n",
    "        predictions = graph.get_tensor_by_name(\"output/predictions:0\")\n",
    "\n",
    "        pred = sess.run(predictions, feed_dict={inputX: [xIds], dropoutKeepProb: 1.0})[0]\n",
    "        \n",
    "pred = [idx2label[item] for item in pred]     \n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
